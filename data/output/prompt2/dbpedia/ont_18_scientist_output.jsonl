{"id": "ont_18_scientist_test_1", "input text": "The Darlington town has an area code of 01325.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Darlington town has an area code of 01325.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"areaCode\", \"01325\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has an area code of\",\n      \"support\": \"The Darlington town has an area code of 01325.\",\n      \"notes\": \"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "areaCode", "01325"], "confidence": 1.0, "cue": "has an area code of", "support": "The Darlington town has an area code of 01325.", "notes": ""}]}}}
{"id": "ont_18_scientist_test_2", "input text": "George Monson was married to Lady Anne Monson, a botanist, who died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"George Monson was married to Lady Anne Monson, a botanist, who died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"George Monson\", \"spouse\", \"Lady Anne Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was married to\",\n      \"support\": \"George Monson was married to Lady Anne Monson\",\n      \"notes\": \"subject and object are explicitly mentioned\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"type\", \"botanist\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a botanist\",\n      \"support\": \"Lady Anne Monson, a botanist\",\n      \"notes\": \"subject is a pronoun (Lady Anne Monson) and the antecedent is explicitly mentioned\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"who died on\",\n      \"support\": \"who died on February 18, 1776\",\n      \"notes\": \"explicit date mentioned in the text\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["George Monson", "spouse", "Lady Anne Monson"], "confidence": 1.0, "cue": "was married to", "support": "George Monson was married to Lady Anne Monson", "notes": "subject and object are explicitly mentioned"}, {"triple": ["Lady Anne Monson", "type", "botanist"], "confidence": 1.0, "cue": "a botanist", "support": "Lady Anne Monson, a botanist", "notes": "subject is a pronoun (Lady Anne Monson) and the antecedent is explicitly mentioned"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18, 1776"], "confidence": 1.0, "cue": "who died on", "support": "who died on February 18, 1776", "notes": "explicit date mentioned in the text"}]}}}
{"id": "ont_18_scientist_test_3", "input text": "Darinka Dentcheva developed a theory influenced by Andrzej Piotr Ruszczyński.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva developed a theory influenced by Andrzej Piotr Ruszczyński.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczyński\"],\n      \"confidence\": 1.0,\n      \"cue\": \"developed a theory influenced by\",\n      \"support\": \"Darinka Dentcheva developed a theory influenced by Andrzej Piotr Ruszczyński.\",\n      \"notes\": \"The subject and object are both Scientists, and the relation 'influencedBy' is correctly expressed by the cue 'developed a theory influenced by'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczyński"], "confidence": 1.0, "cue": "developed a theory influenced by", "support": "Darinka Dentcheva developed a theory influenced by Andrzej Piotr Ruszczyński.", "notes": "The subject and object are both Scientists, and the relation 'influencedBy' is correctly expressed by the cue 'developed a theory influenced by'."}]}}}
{"id": "ont_18_scientist_test_4", "input text": "Lady Anne Monson's spouse was George Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson's spouse was George Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was\",\n      \"support\": \"Lady Anne Monson's spouse was George Monson.\",\n      \"notes\": \"domain/range check: subject and object are both Person\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "was", "support": "Lady Anne Monson's spouse was George Monson.", "notes": "domain/range check: subject and object are both Person"}]}}}
{"id": "ont_18_scientist_test_5", "input text": "Brandon Carter is a professional in the field of general relativity.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter is a professional in the field of general relativity.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"general relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a professional in the field of\",\n      \"support\": \"Brandon Carter is a professional in the field of general relativity.\",\n      \"notes\": \"The subject and object are explicitly mentioned in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "professionalField", "general relativity"], "confidence": 1.0, "cue": "is a professional in the field of", "support": "Brandon Carter is a professional in the field of general relativity.", "notes": "The subject and object are explicitly mentioned in the text."}]}}}
{"id": "ont_18_scientist_test_6", "input text": "Darinka Dentcheva, a Humboldt University graduate, is a mathematician working in the field of mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, a Humboldt University graduate, is a mathematician working in the field of mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"affiliation\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a mathematician working in the field of mathematical optimization.\",\n      \"support\": \"Darinka Dentcheva, a Humboldt University graduate, is a mathematician working in the field of mathematical optimization.\",\n      \"notes\": \"The subject 'Darinka Dentcheva' is a scientist and the object 'Humboldt University' is an organisation. The relation 'affiliation' is inferred from the cue 'working at'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "affiliation", "Humboldt University"], "confidence": 1.0, "cue": "is a mathematician working in the field of mathematical optimization.", "support": "Darinka Dentcheva, a Humboldt University graduate, is a mathematician working in the field of mathematical optimization.", "notes": "The subject 'Darinka Dentcheva' is a scientist and the object 'Humboldt University' is an organisation. The relation 'affiliation' is inferred from the cue 'working at'."}]}}}
{"id": "ont_18_scientist_test_7", "input text": "The zip code areas in Darlington are DL1, DL2, DL3.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The zip code areas in Darlington are DL1, DL2, DL3.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"postalCode\", \"DL1\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"support\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"notes\": \"The city 'Darlington' is the subject, 'DL1' is the object, and the cue is the phrase 'DL1' in the text.\"\n    },\n    {\n      \"triple\": [\"Darlington\", \"postalCode\", \"DL2\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"support\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"notes\": \"The city 'Darlington' is the subject, 'DL2' is the object, and the cue is the phrase 'DL2' in the text.\"\n    },\n    {\n      \"triple\": [\"Darlington\", \"postalCode\", \"DL3\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"support\": \"The zip code areas in Darlington are DL1, DL2, DL3.\",\n      \"notes\": \"The city 'Darlington' is the subject, 'DL3' is the object, and the cue is the phrase 'DL3' in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "postalCode", "DL1"], "confidence": 1.0, "cue": "The zip code areas in Darlington are DL1, DL2, DL3.", "support": "The zip code areas in Darlington are DL1, DL2, DL3.", "notes": "The city 'Darlington' is the subject, 'DL1' is the object, and the cue is the phrase 'DL1' in the text."}, {"triple": ["Darlington", "postalCode", "DL2"], "confidence": 1.0, "cue": "The zip code areas in Darlington are DL1, DL2, DL3.", "support": "The zip code areas in Darlington are DL1, DL2, DL3.", "notes": "The city 'Darlington' is the subject, 'DL2' is the object, and the cue is the phrase 'DL2' in the text."}, {"triple": ["Darlington", "postalCode", "DL3"], "confidence": 1.0, "cue": "The zip code areas in Darlington are DL1, DL2, DL3.", "support": "The zip code areas in Darlington are DL1, DL2, DL3.", "notes": "The city 'Darlington' is the subject, 'DL3' is the object, and the cue is the phrase 'DL3' in the text."}]}}}
{"id": "ont_18_scientist_test_8", "input text": "The Kingdom of England used a unitary state form of government.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Kingdom of England used a unitary state form of government.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Kingdom of England\", \"capital\", \"not specified\"],\n      \"confidence\": 0.8,\n      \"cue\": \"used a unitary state form of government\",\n      \"support\": \"The Kingdom of England used a unitary state form of government.\",\n      \"notes\": \"The text does not specify the capital city of the Kingdom of England.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Kingdom of England", "capital", "not specified"], "confidence": 0.8, "cue": "used a unitary state form of government", "support": "The Kingdom of England used a unitary state form of government.", "notes": "The text does not specify the capital city of the Kingdom of England."}]}}}
{"id": "ont_18_scientist_test_9", "input text": "United States is home to Darinka Dentcheva, where the Vice President is a title of one of its leaders.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"United States is home to Darinka Dentcheva, where the Vice President is a title of one of its leaders.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"United States\", \"leaderTitle\", \"Vice President\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Vice President is a title of one of its leaders.\",\n      \"support\": \"Vice President\",\n      \"notes\": \"The pronoun 'its' refers to 'United States'\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["United States", "leaderTitle", "Vice President"], "confidence": 1.0, "cue": "Vice President is a title of one of its leaders.", "support": "Vice President", "notes": "The pronoun 'its' refers to 'United States'"}]}}}
{"id": "ont_18_scientist_test_10", "input text": "Lady Anne Monson, who was born in Darlington, died on 1776-02-18.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, who was born in Darlington, died on 1776-02-18.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"died on 1776-02-18\",\n      \"notes\": \"subject is a Scientist (based on ontology assumption for historical figures); range is Date (explicitly stated)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on", "support": "died on 1776-02-18", "notes": "subject is a Scientist (based on ontology assumption for historical figures); range is Date (explicitly stated)"}]}}}
{"id": "ont_18_scientist_test_11", "input text": "Lady Anne Monson was born in Darlington and currently resides in India. India was founded on the 26th of January in 1950 and is currently lead by a President.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington and currently resides in India. India was founded on the 26th of January in 1950 and is currently lead by a President.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 0.8,\n      \"cue\": \"currently resides in\",\n      \"support\": \"India\",\n      \"notes\": \"Implicitly inferred that 'Lady Anne Monson' is the subject and 'India' is the object due to the context.\"\n    },\n    {\n      \"triple\": [\"India\", \"leader\", \"President\"],\n      \"confidence\": 0.8,\n      \"cue\": \"is currently lead by\",\n      \"support\": \"President\"\n    },\n    {\n      \"triple\": [\"India\", \"foundingDate\", \"1950-01-26\"],\n      \"confidence\": 1.0,\n      \"cue\": \"founded on the 26th of January in\",\n      \"support\": \"1950-01-26\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 0.8, "cue": "currently resides in", "support": "India", "notes": "Implicitly inferred that 'Lady Anne Monson' is the subject and 'India' is the object due to the context."}, {"triple": ["India", "leader", "President"], "confidence": 0.8, "cue": "is currently lead by", "support": "President"}, {"triple": ["India", "foundingDate", "1950-01-26"], "confidence": 1.0, "cue": "founded on the 26th of January in", "support": "1950-01-26"}]}}}
{"id": "ont_18_scientist_test_12", "input text": "Darinka Dentcheva is resident of new Jersey which has an area of water that is 3.544.04 meters squared and citizen of the United States where the motto is \"In God We Trust\".", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is resident of new Jersey which has an area of water that is 3.544.04 meters squared and citizen of the United States where the motto is \\\"In God We Trust\\\".\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where the motto is \\\"In God We Trust\\\"\",\n      \"support\": \"where the motto is \\\"In God We Trust\\\"\",\n      \"notes\": \"implicitly inferred from the text\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where the motto is \\\"In God We Trust\\\"\",\n      \"support\": \"where the motto is \\\"In God We Trust\\\"\",\n      \"notes\": \"implicitly inferred from the text\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"new Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which has an area of water that is 3.544.04 meters squared and citizen of the United States where\",\n      \"support\": \"which has an area of water that is 3.544.04 meters squared and citizen of the United States where\",\n      \"notes\": \"implicitly inferred from the text\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "where the motto is \"In God We Trust\"", "support": "where the motto is \"In God We Trust\"", "notes": "implicitly inferred from the text"}, {"triple": ["Darinka Dentcheva", "nationality", "United States"], "confidence": 1.0, "cue": "where the motto is \"In God We Trust\"", "support": "where the motto is \"In God We Trust\"", "notes": "implicitly inferred from the text"}, {"triple": ["Darinka Dentcheva", "residence", "new Jersey"], "confidence": 1.0, "cue": "which has an area of water that is 3.544.04 meters squared and citizen of the United States where", "support": "which has an area of water that is 3.544.04 meters squared and citizen of the United States where", "notes": "implicitly inferred from the text"}]}}}
{"id": "ont_18_scientist_test_13", "input text": "Lady Anne Monson, an English national, resided in India with spouse George Monson. With Botany as her professional field, she died on 1776-02-18.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, an English national, resided in India with spouse George Monson. With Botany as her professional field, she died on 1776-02-18.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"an English national\",\n      \"support\": \"Lady Anne Monson, an English national,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resided in India\",\n      \"support\": \"with spouse George Monson. With Botany as her professional field, she died on 1776-02-18.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"professionalField\", \"Botany\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Botany as her professional field\",\n      \"support\": \"With Botany as her professional field,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on 1776-02-18\",\n      \"support\": \". She died on 1776-02-18.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "an English national", "support": "Lady Anne Monson, an English national,"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "resided in India", "support": "with spouse George Monson. With Botany as her professional field, she died on 1776-02-18."}, {"triple": ["Lady Anne Monson", "professionalField", "Botany"], "confidence": 1.0, "cue": "Botany as her professional field", "support": "With Botany as her professional field,"}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on 1776-02-18", "support": ". She died on 1776-02-18."}]}}}
{"id": "ont_18_scientist_test_14", "input text": "Influenced by Andrzej Piotr Ruszczynski, Darinka Dentcheva (born Jan. 1st, 1958) has become widely known for her work in Stochastic Programming, working in the field of Mathematical Optimization; she is a citizen of the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Influenced by Andrzej Piotr Ruszczynski, Darinka Dentcheva (born Jan. 1st, 1958) has become widely known for her work in Stochastic Programming, working in the field of Mathematical Optimization; she is a citizen of the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she is a citizen of the United States\",\n      \"support\": \"she is a citizen of the United States\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Stochastic Programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she has become widely known for her work in Stochastic Programming\",\n      \"support\": \"she has become widely known for her work in Stochastic Programming\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she is a citizen of the United States\",\n      \"support\": \"she is a citizen of the United States\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Influenced by Andrzej Piotr Ruszczynski\",\n      \"support\": \"Influenced by Andrzej Piotr Ruszczynski\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "nationality", "United States"], "confidence": 1.0, "cue": "she is a citizen of the United States", "support": "she is a citizen of the United States"}, {"triple": ["Darinka Dentcheva", "professionalField", "Stochastic Programming"], "confidence": 1.0, "cue": "she has become widely known for her work in Stochastic Programming", "support": "she has become widely known for her work in Stochastic Programming"}, {"triple": ["Darinka Dentcheva", "country", "United States"], "confidence": 1.0, "cue": "she is a citizen of the United States", "support": "she is a citizen of the United States"}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "Influenced by Andrzej Piotr Ruszczynski", "support": "Influenced by Andrzej Piotr Ruszczynski"}]}}}
{"id": "ont_18_scientist_test_15", "input text": "Darinka Dentcheva is a resident of New Jersey, a state in the United States, which uses federalism as its type of governing system.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is a resident of New Jersey, a state in the United States, which uses federalism as its type of governing system.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which uses federalism as its type of governing system.\",\n      \"support\": \"which uses federalism as its type of governing system.\",\n      \"notes\": \"Inferred 'citizenship' relation from 'country' relation and context.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a resident of New Jersey\",\n      \"support\": \"is a resident of New Jersey\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "country", "United States"], "confidence": 1.0, "cue": "which uses federalism as its type of governing system.", "support": "which uses federalism as its type of governing system.", "notes": "Inferred 'citizenship' relation from 'country' relation and context."}, {"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "is a resident of New Jersey", "support": "is a resident of New Jersey"}]}}}
{"id": "ont_18_scientist_test_16", "input text": "Darinka Dentcheva, a United States citizen, developed a theory in Mathematical Optimization, influenced by Andrzej Piotr Ruszczyński . \"In God We Trust\" is the motto of the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, a United States citizen, developed a theory in Mathematical Optimization, influenced by Andrzej Piotr Ruszczyński . \\\"In God We Trust\\\" is the motto of the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Darinka Dentcheva, a United States citizen\",\n      \"support\": \"Darinka Dentcheva, a United States citizen\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical Optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Developed a theory in Mathematical Optimization\",\n      \"support\": \"Developed a theory in Mathematical Optimization\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Darinka Dentcheva, a United States citizen\",\n      \"support\": \"Darinka Dentcheva, a United States citizen\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "Darinka Dentcheva, a United States citizen", "support": "Darinka Dentcheva, a United States citizen"}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical Optimization"], "confidence": 1.0, "cue": "Developed a theory in Mathematical Optimization", "support": "Developed a theory in Mathematical Optimization"}, {"triple": ["Darinka Dentcheva", "nationality", "United States"], "confidence": 1.0, "cue": "Darinka Dentcheva, a United States citizen", "support": "Darinka Dentcheva, a United States citizen"}]}}}
{"id": "ont_18_scientist_test_17", "input text": "Darinka Dentcheva is a citizen of the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is a citizen of the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a citizen of\",\n      \"support\": \"Darinka Dentcheva is a citizen of the United States\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "is a citizen of", "support": "Darinka Dentcheva is a citizen of the United States", "notes": "The subject and object are correctly typed according to the ontology."}]}}}
{"id": "ont_18_scientist_test_18", "input text": "Darinka Dentcheva is known for her work with stochastic programming where she has a residence in the state of New Jersey and her alma mater is Humboldt University.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is known for her work with stochastic programming where she has a residence in the state of New Jersey and her alma mater is Humboldt University.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"her alma mater is Humboldt University\",\n      \"support\": \"her alma mater is Humboldt University\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"the state of New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she has a residence in the state of New Jersey\",\n      \"support\": \"she has a residence in the state of New Jersey\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "her alma mater is Humboldt University", "support": "her alma mater is Humboldt University"}, {"triple": ["Darinka Dentcheva", "residence", "the state of New Jersey"], "confidence": 1.0, "cue": "she has a residence in the state of New Jersey", "support": "she has a residence in the state of New Jersey"}]}}}
{"id": "ont_18_scientist_test_19", "input text": "Darinka Dentcheva (United States citizen) with birthdate as January 1, 1958 studied at Humboldt University. Her theory was influenced by Andrzej Piotr Ruszczyński.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva (United States citizen) with birthdate as January 1, 1958 studied at Humboldt University. Her theory was influenced by Andrzej Piotr Ruszczyński.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Darinka Dentcheva (United States citizen)\",\n      \"support\": \"Darinka Dentcheva (United States citizen)\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied at Humboldt University\",\n      \"support\": \"studied at Humboldt University\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczyński\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Her theory was influenced by Andrzej Piotr Ruszczyński\",\n      \"support\": \"Her theory was influenced by Andrzej Piotr Ruszczyński\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "Darinka Dentcheva (United States citizen)", "support": "Darinka Dentcheva (United States citizen)"}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "studied at Humboldt University", "support": "studied at Humboldt University"}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczyński"], "confidence": 1.0, "cue": "Her theory was influenced by Andrzej Piotr Ruszczyński", "support": "Her theory was influenced by Andrzej Piotr Ruszczyński"}]}}}
{"id": "ont_18_scientist_test_20", "input text": "Lady Anne Monson, born on January 1, 1726 in the Kingdom of England, studied botany.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, born on January 1, 1726 in the Kingdom of England, studied botany.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Kingdom of England\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on\",\n      \"support\": \"1726-01-01\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"professionalField\", \"botany\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied botany\",\n      \"support\": \"botany\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Kingdom of England"], "confidence": 1.0, "cue": "born in", "support": "Kingdom of England", "notes": "domain/range check"}, {"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "born on", "support": "1726-01-01", "notes": "domain/range check"}, {"triple": ["Lady Anne Monson", "professionalField", "botany"], "confidence": 1.0, "cue": "studied botany", "support": "botany", "notes": "domain/range check"}]}}}
{"id": "ont_18_scientist_test_21", "input text": "Brandon Carter (born in England) is known for the Carter constant as well as the No-hair theorem. His doctoral advisor is Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter (born in England) is known for the Carter constant as well as the No-hair theorem. His doctoral advisor is Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"bornIn\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter (born in England)\",\n      \"notes\": \"The subject is a Scientist and the object is a Country.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Carter constant\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"Brandon Carter (born in England) is known for the Carter constant\",\n      \"notes\": \"The subject is a Scientist and the object is a Thing.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"his doctoral advisor\",\n      \"support\": \"His doctoral advisor is Dennis William Sciama.\",\n      \"notes\": \"The subject is a Scientist and the object is also a Scientist.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "bornIn", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter (born in England)", "notes": "The subject is a Scientist and the object is a Country."}, {"triple": ["Brandon Carter", "knownFor", "Carter constant"], "confidence": 1.0, "cue": "known for", "support": "Brandon Carter (born in England) is known for the Carter constant", "notes": "The subject is a Scientist and the object is a Thing."}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "his doctoral advisor", "support": "His doctoral advisor is Dennis William Sciama.", "notes": "The subject is a Scientist and the object is also a Scientist."}]}}}
{"id": "ont_18_scientist_test_22", "input text": "Brandon Carter (born in England) works in the field of general relativity and is best known for the Carter constant and the No-hair theorem.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter (born in England) works in the field of general relativity and is best known for the Carter constant and the No-hair theorem.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"bornIn\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter (born in England)\",\n      \"notes\": \"domain: Scientist, range: Country\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter (born in England)\",\n      \"notes\": \"inferred nationality from birth place\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"general relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"works in the field of\",\n      \"support\": \"works in the field of general relativity\",\n      \"notes\": \"domain: Scientist, range: Field\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", [\"Carter constant\", \"No-hair theorem\"]],\n      \"confidence\": 1.0,\n      \"cue\": \"is best known for\",\n      \"support\": \"is best known for the Carter constant and the No-hair theorem.\",\n      \"notes\": \"domain: Scientist, range: Thing\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "bornIn", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter (born in England)", "notes": "domain: Scientist, range: Country"}, {"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter (born in England)", "notes": "inferred nationality from birth place"}, {"triple": ["Brandon Carter", "professionalField", "general relativity"], "confidence": 1.0, "cue": "works in the field of", "support": "works in the field of general relativity", "notes": "domain: Scientist, range: Field"}, {"triple": ["Brandon Carter", "knownFor", ["Carter constant", "No-hair theorem"]], "confidence": 1.0, "cue": "is best known for", "support": "is best known for the Carter constant and the No-hair theorem.", "notes": "domain: Scientist, range: Thing"}]}}}
{"id": "ont_18_scientist_test_23", "input text": "The Kingdom of England was founded in the year 927.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Kingdom of England was founded in the year 927.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The Kingdom of England\", \"foundedIn\", \"927\"],\n      \"confidence\": 1.0,\n      \"cue\": \"founded in\",\n      \"support\": \"founded in the year 927\",\n      \"notes\": \"domain/range check: Country\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The Kingdom of England", "foundedIn", "927"], "confidence": 1.0, "cue": "founded in", "support": "founded in the year 927", "notes": "domain/range check: Country"}]}}}
{"id": "ont_18_scientist_test_24", "input text": "Lady Anne Monson’s nationality is English but currently resides in India, whose total area is 3,287,590 km^2. One of the leaders of India is Chief Justice.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson’s nationality is English but currently resides in India, whose total area is 3,287,590 km^2. One of the leaders of India is Chief Justice.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Lady Anne Monson’s nationality is English\",\n      \"support\": \"Lady Anne Monson’s nationality is English\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Lady Anne Monson resides in India\",\n      \"support\": \"Lady Anne Monson resides in India\"\n    },\n    {\n      \"triple\": [\"India\", \"leader\", \"Chief Justice\"],\n      \"confidence\": 1.0,\n      \"cue\": \"One of the leaders of India is Chief Justice\",\n      \"support\": \"One of the leaders of India is Chief Justice\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "Lady Anne Monson’s nationality is English", "support": "Lady Anne Monson’s nationality is English"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "Lady Anne Monson resides in India", "support": "Lady Anne Monson resides in India"}, {"triple": ["India", "leader", "Chief Justice"], "confidence": 1.0, "cue": "One of the leaders of India is Chief Justice", "support": "One of the leaders of India is Chief Justice"}]}}}
{"id": "ont_18_scientist_test_25", "input text": "Lady Anne Monson was born on January 1, 1726 in Darlington located in the Kingdom of England. Her residence was in India and she later died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born on January 1, 1726 in Darlington located in the Kingdom of England. Her residence was in India and she later died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Darlington located in the Kingdom of England\",\n      \"notes\": \"Subject is explicitly stated\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Darlington located in the Kingdom of England\",\n      \"notes\": \"Inferred from birthPlace\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"later died on\",\n      \"support\": \"India\",\n      \"notes\": \"Subject is explicitly stated\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"later died on\",\n      \"support\": \"February 18, 1776\",\n      \"notes\": \"Explicit date mentioned\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "born in", "support": "Darlington located in the Kingdom of England", "notes": "Subject is explicitly stated"}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "born in", "support": "Darlington located in the Kingdom of England", "notes": "Inferred from birthPlace"}, {"triple": ["Lady Anne Monson", "deathPlace", "India"], "confidence": 1.0, "cue": "later died on", "support": "India", "notes": "Subject is explicitly stated"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18, 1776"], "confidence": 1.0, "cue": "later died on", "support": "February 18, 1776", "notes": "Explicit date mentioned"}]}}}
{"id": "ont_18_scientist_test_26", "input text": "Darinka Dentcheva's residence is in New Jersey.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva's residence is in New Jersey.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is in\",\n      \"support\": \"Darinka Dentcheva's residence is in New Jersey\",\n      \"notes\": \"domain: Scientist, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "is in", "support": "Darinka Dentcheva's residence is in New Jersey", "notes": "domain: Scientist, range: Place"}]}}}
{"id": "ont_18_scientist_test_27", "input text": "Darinka Dentcheva was born in New Jersey which has a total population of 22,591.38.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva was born in New Jersey which has a total population of 22,591.38.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darinka Dentcheva was born in New Jersey\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "birthPlace", "New Jersey"], "confidence": 1.0, "cue": "was born in", "support": "Darinka Dentcheva was born in New Jersey"}]}}}
{"id": "ont_18_scientist_test_28", "input text": "Darinka Dentcheva, born January 1st of 1958, was influenced by Andrzej Piotr Ruszczynski.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, born January 1st of 1958, was influenced by Andrzej Piotr Ruszczynski.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was influenced by\",\n      \"support\": \"Darinka Dentcheva, born January 1st of 1958, was influenced by Andrzej Piotr Ruszczynski.\",\n      \"notes\": \"The subject and object are both Scientists, but the ontology relation 'influencedBy' does not have a specific range.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "was influenced by", "support": "Darinka Dentcheva, born January 1st of 1958, was influenced by Andrzej Piotr Ruszczynski.", "notes": "The subject and object are both Scientists, but the ontology relation 'influencedBy' does not have a specific range."}]}}}
{"id": "ont_18_scientist_test_29", "input text": "Darinka Dentcheva is a citizen of the United States, the country where John Roberts is one of the leaders.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is a citizen of the United States, the country where John Roberts is one of the leaders.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a citizen of\",\n      \"support\": \"Darinka Dentcheva is a citizen of the United States\",\n      \"notes\": \"Darinka Dentcheva is a Scientist and the domain of citizenship is Country\"\n    },\n    {\n      \"triple\": [\"John Roberts\", \"leader\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"one of the leaders\",\n      \"support\": \"John Roberts is one of the leaders\",\n      \"notes\": \"John Roberts is not a Scientist, but the leader relation's range is Country\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "is a citizen of", "support": "Darinka Dentcheva is a citizen of the United States", "notes": "Darinka Dentcheva is a Scientist and the domain of citizenship is Country"}, {"triple": ["John Roberts", "leader", "United States"], "confidence": 1.0, "cue": "one of the leaders", "support": "John Roberts is one of the leaders", "notes": "John Roberts is not a Scientist, but the leader relation's range is Country"}]}}}
{"id": "ont_18_scientist_test_30", "input text": "India, at one time led by T.S. Thakur, was also where Lady Anne Monson lived most of her life.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"India, at one time led by T.S. Thakur, was also where Lady Anne Monson lived most of her life.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"India\", \"leader\", \"T.S. Thakur\"],\n      \"confidence\": 1.0,\n      \"cue\": \"led by\",\n      \"support\": \"India, at one time led by T.S. Thakur,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived most of her life in\",\n      \"support\": \"Lady Anne Monson lived most of her life in India.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["India", "leader", "T.S. Thakur"], "confidence": 1.0, "cue": "led by", "support": "India, at one time led by T.S. Thakur,"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "lived most of her life in", "support": "Lady Anne Monson lived most of her life in India."}]}}}
{"id": "ont_18_scientist_test_31", "input text": "The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Kingdom of England\", \"capital\", \"not specified in the text\"],\n      \"confidence\": 0.6,\n      \"cue\": \"The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.\",\n      \"notes\": \"The text implies that the Kingdom of England has a capital, but it is not explicitly stated.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.\",\n      \"notes\": \"The text explicitly states that Lady Anne Monson was born in the Kingdom of England.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Kingdom of England", "capital", "not specified in the text"], "confidence": 0.6, "cue": "The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.", "notes": "The text implies that the Kingdom of England has a capital, but it is not explicitly stated."}, {"triple": ["Lady Anne Monson", "birthPlace", "Kingdom of England"], "confidence": 1.0, "cue": "The Kingdom of England, the place where Lady Anne Monson was born, was a unitary state.", "notes": "The text explicitly states that Lady Anne Monson was born in the Kingdom of England."}]}}}
{"id": "ont_18_scientist_test_32", "input text": "The demonym for people from the United States is American.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The demonym for people from the United States is American.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"United States\", \"demonym\", \"American\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The demonym for people from the United States is American.\",\n      \"support\": \"The demonym for people from the United States is American.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["United States", "demonym", "American"], "confidence": 1.0, "cue": "The demonym for people from the United States is American.", "support": "The demonym for people from the United States is American."}]}}}
{"id": "ont_18_scientist_test_33", "input text": "Darinka Dentcheva is a mathematician working in the field of mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is a mathematician working in the field of mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"working in the field of\",\n      \"support\": \"Darinka Dentcheva is a mathematician working in the field of mathematical optimization.\",\n      \"notes\": \"The subject is a Scientist and the object is a Field.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "professionalField", "mathematical optimization"], "confidence": 1.0, "cue": "working in the field of", "support": "Darinka Dentcheva is a mathematician working in the field of mathematical optimization.", "notes": "The subject is a Scientist and the object is a Field."}]}}}
{"id": "ont_18_scientist_test_34", "input text": "Darinka Dentcheva, known for her work with stochastic programming, was influenced by the work of Andrzej Piotr Ruszczynski. She attended Humboldt University and her professional field is Mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, known for her work with stochastic programming, was influenced by the work of Andrzej Piotr Ruszczynski. She attended Humboldt University and her professional field is Mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for her work with stochastic programming\",\n      \"support\": \"Darinka Dentcheva, known for her work with stochastic programming\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was influenced by the work of Andrzej Piotr Ruszczynski\",\n      \"support\": \"Darinka Dentcheva, known for her work with stochastic programming, was influenced by the work of Andrzej Piotr Ruszczynski\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"attended Humboldt University\",\n      \"support\": \"her professional field is Mathematical optimization\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"her professional field is Mathematical optimization\",\n      \"support\": \"her professional field is Mathematical optimization\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "stochastic programming"], "confidence": 1.0, "cue": "known for her work with stochastic programming", "support": "Darinka Dentcheva, known for her work with stochastic programming"}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "was influenced by the work of Andrzej Piotr Ruszczynski", "support": "Darinka Dentcheva, known for her work with stochastic programming, was influenced by the work of Andrzej Piotr Ruszczynski"}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "attended Humboldt University", "support": "her professional field is Mathematical optimization"}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "her professional field is Mathematical optimization", "support": "her professional field is Mathematical optimization"}]}}}
{"id": "ont_18_scientist_test_35", "input text": "Lady Anne Monson lived from January 1, 1726 to February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson lived from January 1, 1726 to February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"January 1, 1726\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived from January 1, 1726 to February 1, 1776\",\n      \"support\": \"from January 1, 1726 to February 1, 1776\",\n      \"notes\": \"The date is explicitly mentioned in the text.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived from January 1, 1726 to February 1, 1776\",\n      \"support\": \"to February 1, 1776\",\n      \"notes\": \"The date is explicitly mentioned in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthDate", "January 1, 1726"], "confidence": 1.0, "cue": "lived from January 1, 1726 to February 1, 1776", "support": "from January 1, 1726 to February 1, 1776", "notes": "The date is explicitly mentioned in the text."}, {"triple": ["Lady Anne Monson", "deathDate", "February 18, 1776"], "confidence": 1.0, "cue": "lived from January 1, 1726 to February 1, 1776", "support": "to February 1, 1776", "notes": "The date is explicitly mentioned in the text."}]}}}
{"id": "ont_18_scientist_test_36", "input text": "Lady Anne Monson married George Monson was a British national but resided in India for most of her life. She died on February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson married George Monson was a British national but resided in India for most of her life. She died on February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"British\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was a British national\",\n      \"support\": \"Lady Anne Monson was a British national\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resided in India for most of her life\",\n      \"support\": \"resided in India for most of her life\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"18th, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on February 18th, 1776\",\n      \"support\": \"died on February 18th, 1776\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "British"], "confidence": 1.0, "cue": "was a British national", "support": "Lady Anne Monson was a British national"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "resided in India for most of her life", "support": "resided in India for most of her life"}, {"triple": ["Lady Anne Monson", "deathDate", "18th, 1776"], "confidence": 1.0, "cue": "died on February 18th, 1776", "support": "died on February 18th, 1776"}]}}}
{"id": "ont_18_scientist_test_37", "input text": "Brandon Carter is well known for Anthropic principle theory.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter is well known for Anthropic principle theory.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Anthropic principle theory\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is well known for\",\n      \"support\": \"Brandon Carter is well known for Anthropic principle theory.\",\n      \"notes\": \"The subject and object are correctly typed as Scientist and Thing respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "Anthropic principle theory"], "confidence": 1.0, "cue": "is well known for", "support": "Brandon Carter is well known for Anthropic principle theory.", "notes": "The subject and object are correctly typed as Scientist and Thing respectively."}]}}}
{"id": "ont_18_scientist_test_38", "input text": "Darinka Dentcheva, who resides in New Jersey, is a graduate of Humboldt University.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, who resides in New Jersey, is a graduate of Humboldt University.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"who resides in\",\n      \"support\": \"Darinka Dentcheva, who resides in New Jersey,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a graduate of\",\n      \"support\": \"Darinka Dentcheva, who resides in New Jersey, is a graduate of Humboldt University.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "who resides in", "support": "Darinka Dentcheva, who resides in New Jersey,"}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "is a graduate of", "support": "Darinka Dentcheva, who resides in New Jersey, is a graduate of Humboldt University."}]}}}
{"id": "ont_18_scientist_test_39", "input text": "English is commonly spoken in New Jersey.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"English is commonly spoken in New Jersey.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Language\", \"language\", \"English\"],\n      \"confidence\": 1.0,\n      \"cue\": \"spoken in\",\n      \"support\": \"English is commonly spoken in New Jersey.\",\n      \"notes\": \"The subject 'Language' is inferred from the object 'English' as it is the entity being spoken.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Language", "language", "English"], "confidence": 1.0, "cue": "spoken in", "support": "English is commonly spoken in New Jersey.", "notes": "The subject 'Language' is inferred from the object 'English' as it is the entity being spoken."}]}}}
{"id": "ont_18_scientist_test_40", "input text": "Lady Anne Monson was born on January 1, 1726 in the kingdom of England, where she lived and held British nationality. Lady Anne Monson’s spouse is George Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born on January 1, 1726 in the kingdom of England, where she lived and held British nationality. Lady Anne Monson’s spouse is George Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where she lived and held British nationality\",\n      \"support\": \"where she lived and held British nationality\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in the kingdom of England\",\n      \"support\": \"born in the kingdom of England\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Lady Anne Monson’s spouse is George Monson\",\n      \"support\": \"Lady Anne Monson’s spouse is George Monson\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "where she lived and held British nationality", "support": "where she lived and held British nationality"}, {"triple": ["Lady Anne Monson", "birthPlace", "England"], "confidence": 1.0, "cue": "born in the kingdom of England", "support": "born in the kingdom of England"}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "Lady Anne Monson’s spouse is George Monson", "support": "Lady Anne Monson’s spouse is George Monson"}]}}}
{"id": "ont_18_scientist_test_41", "input text": "The total population of the United States is over three hundred and twenty four million.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The total population of the United States is over three hundred and twenty four million.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"United States\", \"populationTotal\", \"324000000\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The total population of the United States is over three hundred and twenty four million.\",\n      \"notes\": \"The pronoun 'it' is resolved to 'The total population of the United States'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["United States", "populationTotal", "324000000"], "confidence": 1.0, "cue": "The total population of the United States is over three hundred and twenty four million.", "notes": "The pronoun 'it' is resolved to 'The total population of the United States'."}]}}}
{"id": "ont_18_scientist_test_42", "input text": "Lady Anne Monson was born in Darlington on January 1, 1726, and died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington on January 1, 1726, and died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington on January 1, 1726,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"February 18, 1776.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on\",\n      \"support\": \"Darlington on January 1, 1726,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"February 18, 1776.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington on January 1, 1726,"}, {"triple": ["Lady Anne Monson", "deathPlace", "Darlington"], "confidence": 1.0, "cue": "died on", "support": "February 18, 1776."}, {"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "was born on", "support": "Darlington on January 1, 1726,"}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on", "support": "February 18, 1776."}]}}}
{"id": "ont_18_scientist_test_43", "input text": "John Roberts is a leader in the United States, whereas Darinka Dentcheva only resides in the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"John Roberts is a leader in the United States, whereas Darinka Dentcheva only resides in the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"John Roberts\", \"nationality\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a leader in the United States\",\n      \"support\": \"John Roberts\",\n      \"notes\": \"Inferred nationality from 'United States' being the country where the leader resides.\"\n    },\n    {\n      \"triple\": [\"John Roberts\", \"residence\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a leader in the United States\",\n      \"support\": \"United States\",\n      \"notes\": \"Inferred residence from 'United States' being the country where the leader resides.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"only resides in the United States\",\n      \"support\": \"Darinka Dentcheva\",\n      \"notes\": \"Explicitly stated in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["John Roberts", "nationality", "United States"], "confidence": 1.0, "cue": "is a leader in the United States", "support": "John Roberts", "notes": "Inferred nationality from 'United States' being the country where the leader resides."}, {"triple": ["John Roberts", "residence", "United States"], "confidence": 1.0, "cue": "is a leader in the United States", "support": "United States", "notes": "Inferred residence from 'United States' being the country where the leader resides."}, {"triple": ["Darinka Dentcheva", "residence", "United States"], "confidence": 1.0, "cue": "only resides in the United States", "support": "Darinka Dentcheva", "notes": "Explicitly stated in the text."}]}}}
{"id": "ont_18_scientist_test_44", "input text": "Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.\",\n      \"notes\": \"subject: Lady Anne Monson (Scientist, Person); object: Darlington (Place)\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"UK\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.\",\n      \"notes\": \"subject: Lady Anne Monson (Scientist, Person); object: UK (Country)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.", "notes": "subject: Lady Anne Monson (Scientist, Person); object: Darlington (Place)"}, {"triple": ["Lady Anne Monson", "country", "UK"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in the Kingdom of England in a town called Darlington which is in the UK.", "notes": "subject: Lady Anne Monson (Scientist, Person); object: UK (Country)"}]}}}
{"id": "ont_18_scientist_test_45", "input text": "Lady Anne Monson was born in Darlington but resided in India for most of her life. She was born on January 1, 1726 and died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington but resided in India for most of her life. She was born on January 1, 1726 and died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington but resided in India for most of her life.\",\n      \"notes\": \"Subject is explicitly mentioned\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"January 1, 1726\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on\",\n      \"support\": \"January 1, 1726 and died on February 18, 1776.\",\n      \"notes\": \"Date is explicitly mentioned\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"February 18, 1776.\",\n      \"notes\": \"Place is inferred as the context mentions death in India\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"February 18, 1776.\",\n      \"notes\": \"Date is explicitly mentioned\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington but resided in India for most of her life.", "notes": "Subject is explicitly mentioned"}, {"triple": ["Lady Anne Monson", "birthDate", "January 1, 1726"], "confidence": 1.0, "cue": "was born on", "support": "January 1, 1726 and died on February 18, 1776.", "notes": "Date is explicitly mentioned"}, {"triple": ["Lady Anne Monson", "deathPlace", "India"], "confidence": 1.0, "cue": "died on", "support": "February 18, 1776.", "notes": "Place is inferred as the context mentions death in India"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18, 1776"], "confidence": 1.0, "cue": "died on", "support": "February 18, 1776.", "notes": "Date is explicitly mentioned"}]}}}
{"id": "ont_18_scientist_test_46", "input text": "Lady Anne Monson was born in Darlington, England. Darlington is in the North East of England and has a grid reference of NZ289147.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington, England. Darlington is in the North East of England and has a grid reference of NZ289147.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in Darlington\",\n      \"notes\": \"Subject is a person (Scientist domain is not applicable in this context)\"\n    },\n    {\n      \"triple\": [\"Darlington\", \"isPartOf\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is in\",\n      \"support\": \"Darlington is in the North East of England\",\n      \"notes\": \"Domain and range are both Place\"\n    },\n    {\n      \"triple\": [\"Darlington\", \"gridReference\", \"NZ289147\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a grid reference of\",\n      \"support\": \"Darlington has a grid reference of NZ289147\",\n      \"notes\": \"Domain and range are both Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in Darlington", "notes": "Subject is a person (Scientist domain is not applicable in this context)"}, {"triple": ["Darlington", "isPartOf", "England"], "confidence": 1.0, "cue": "is in", "support": "Darlington is in the North East of England", "notes": "Domain and range are both Place"}, {"triple": ["Darlington", "gridReference", "NZ289147"], "confidence": 1.0, "cue": "has a grid reference of", "support": "Darlington has a grid reference of NZ289147", "notes": "Domain and range are both Place"}]}}}
{"id": "ont_18_scientist_test_47", "input text": "Brandon Carter, best known for the Carter constant and the No-hair theorem, is a professional in the field of general relativity. He was born in England and studied at the University of Cambridge, where he was advised by Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, best known for the Carter constant and the No-hair theorem, is a professional in the field of general relativity. He was born in England and studied at the University of Cambridge, where he was advised by Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter, best known for the Carter constant and the No-hair theorem, is a professional in the field of general relativity. He was born in England\",\n      \"notes\": \"Inferred nationality from birth place\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied at\",\n      \"support\": \"He was born in England and studied at the University of Cambridge\",\n      \"notes\": \"Inferred alma mater from mentioned university\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"affiliation\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied at\",\n      \"support\": \"He was born in England and studied at the University of Cambridge\",\n      \"notes\": \"Inferred affiliation from alma mater\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", [\"Carter constant\", \"No-hair theorem\"]],\n      \"confidence\": 1.0,\n      \"cue\": \"best known for\",\n      \"support\": \"Brandon Carter, best known for the Carter constant and the No-hair theorem\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"He was born in England\",\n      \"notes\": \"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter, best known for the Carter constant and the No-hair theorem, is a professional in the field of general relativity. He was born in England", "notes": "Inferred nationality from birth place"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "studied at", "support": "He was born in England and studied at the University of Cambridge", "notes": "Inferred alma mater from mentioned university"}, {"triple": ["Brandon Carter", "affiliation", "University of Cambridge"], "confidence": 1.0, "cue": "studied at", "support": "He was born in England and studied at the University of Cambridge", "notes": "Inferred affiliation from alma mater"}, {"triple": ["Brandon Carter", "knownFor", ["Carter constant", "No-hair theorem"]], "confidence": 1.0, "cue": "best known for", "support": "Brandon Carter, best known for the Carter constant and the No-hair theorem", "notes": ""}, {"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "He was born in England", "notes": ""}]}}}
{"id": "ont_18_scientist_test_48", "input text": "Darinka Dentcheva, an American citizen known for her work with stochastic programming, developed a theory influenced by Andrzej Piotr Ruszczyński.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, an American citizen known for her work with stochastic programming, developed a theory influenced by Andrzej Piotr Ruszczyński.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for her work with stochastic programming\",\n      \"support\": \"Darinka Dentcheva, an American citizen known for her work with stochastic programming,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"American\"],\n      \"confidence\": 1.0,\n      \"cue\": \"American citizen\",\n      \"support\": \"Darinka Dentcheva, an American citizen known for her work with stochastic programming,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczyński\"],\n      \"confidence\": 1.0,\n      \"cue\": \"influenced by Andrzej Piotr Ruszczyński\",\n      \"support\": \"Darinka Dentcheva, an American citizen known for her work with stochastic programming, developed a theory influenced by Andrzej Piotr Ruszczyński.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "stochastic programming"], "confidence": 1.0, "cue": "known for her work with stochastic programming", "support": "Darinka Dentcheva, an American citizen known for her work with stochastic programming,"}, {"triple": ["Darinka Dentcheva", "nationality", "American"], "confidence": 1.0, "cue": "American citizen", "support": "Darinka Dentcheva, an American citizen known for her work with stochastic programming,"}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczyński"], "confidence": 1.0, "cue": "influenced by Andrzej Piotr Ruszczyński", "support": "Darinka Dentcheva, an American citizen known for her work with stochastic programming, developed a theory influenced by Andrzej Piotr Ruszczyński."}]}}}
{"id": "ont_18_scientist_test_49", "input text": "Brandon Carter, a specialist in General Relativity, is known for the Carter Constant.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, a specialist in General Relativity, is known for the Carter Constant.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Carter Constant\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Brandon Carter, a specialist in General Relativity, is known for the Carter Constant.\",\n      \"notes\": \"The subject and object are correctly typed as Scientist and Thing, respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "Carter Constant"], "confidence": 1.0, "cue": "is known for", "support": "Brandon Carter, a specialist in General Relativity, is known for the Carter Constant.", "notes": "The subject and object are correctly typed as Scientist and Thing, respectively."}]}}}
{"id": "ont_18_scientist_test_50", "input text": "Lady Anne Monson was born in the Kingdom of England in Darlington, whose OS grid reference is NZ289147. Lady Anne Monson lived in the Kingdom of England, whose dissolution took place in 1707, and held British nationality.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in the Kingdom of England in Darlington, whose OS grid reference is NZ289147. Lady Anne Monson lived in the Kingdom of England, whose dissolution took place in 1707, and held British nationality.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"held British nationality.\",\n      \"support\": \"held British nationality.\",\n      \"notes\": \"The subject 'Lady Anne Monson' is a Scientist (implied from the text being about a person), and the country 'Kingdom of England' is the range for the 'nationality' relation.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in Darlington,\".replace(\"was born \", \"\"),\n      \"support\": \"Darlington,\",\n      \"notes\": \"The subject 'Lady Anne Monson' is a Scientist (implied from the text being about a person), and 'Darlington' is a Place, which is the range for the 'birthPlace' relation.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived in the Kingdom of England,\".replace(\"lived in \", \"\"),\n      \"support\": \"Kingdom of England,\".replace(\"Kingdom of England,\", \"\"),\n      \"notes\": \"The subject 'Lady Anne Monson' is a Scientist (implied from the text being about a person), and 'Kingdom of England' is a Country, which is the range for the'residence' relation.\"\n    }\n  ]\n}", "json": null}}
{"id": "ont_18_scientist_test_51", "input text": "Brandon Carter was born on January 1, 1942 and worked professionally in the field of General Relativity. Brandon Carter was known for the Carter Constant and his contributions to the no-hair theorem. Dennis William Sciama was his doctoral advisor.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born on January 1, 1942 and worked professionally in the field of General Relativity. Brandon Carter was known for the Carter Constant and his contributions to the no-hair theorem. Dennis William Sciama was his doctoral advisor.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"General Relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"worked professionally in the field of General Relativity\",\n      \"support\": \"worked professionally in the field of General Relativity\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", [\"Carter Constant\", \"no-hair theorem\"]],\n      \"confidence\": 1.0,\n      \"cue\": \"was known for the Carter Constant and his contributions to the no-hair theorem\",\n      \"support\": \"was known for the Carter Constant and his contributions to the no-hair theorem\"\n    },\n    {\n      \"triple\": [\"Dennis William Sciama\", \"doctoralAdvisor\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was his doctoral advisor\",\n      \"support\": \"was his doctoral advisor\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "professionalField", "General Relativity"], "confidence": 1.0, "cue": "worked professionally in the field of General Relativity", "support": "worked professionally in the field of General Relativity"}, {"triple": ["Brandon Carter", "knownFor", ["Carter Constant", "no-hair theorem"]], "confidence": 1.0, "cue": "was known for the Carter Constant and his contributions to the no-hair theorem", "support": "was known for the Carter Constant and his contributions to the no-hair theorem"}, {"triple": ["Dennis William Sciama", "doctoralAdvisor", "Brandon Carter"], "confidence": 1.0, "cue": "was his doctoral advisor", "support": "was his doctoral advisor"}]}}}
{"id": "ont_18_scientist_test_52", "input text": "Lady Anne Monson was born in Darlington.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"was born in Darlington\",\n      \"notes\": \"subject is a person (Scientist domain is not applicable in this text)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "was born in Darlington", "notes": "subject is a person (Scientist domain is not applicable in this text)"}]}}}
{"id": "ont_18_scientist_test_53", "input text": "Brandon Carter, who is well-known for the Anthropic principle theory, calls England his birthplace.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, who is well-known for the Anthropic principle theory, calls England his birthplace.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"calls England his birthplace\",\n      \"support\": \"calls England his birthplace\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"calls England his birthplace\",\n      \"support\": \"calls England his birthplace\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Anthropic principle theory\"],\n      \"confidence\": 1.0,\n      \"cue\": \"well-known for the Anthropic principle theory\",\n      \"support\": \"well-known for the Anthropic principle theory\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "calls England his birthplace", "support": "calls England his birthplace"}, {"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "calls England his birthplace", "support": "calls England his birthplace"}, {"triple": ["Brandon Carter", "knownFor", "Anthropic principle theory"], "confidence": 1.0, "cue": "well-known for the Anthropic principle theory", "support": "well-known for the Anthropic principle theory"}]}}}
{"id": "ont_18_scientist_test_54", "input text": "Indian is the demonym for people from India.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Indian is the demonym for people from India.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"India\", \"demonym\", \"Indian\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Indian is the demonym for people from India.\",\n      \"support\": \"Indian, Indian\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["India", "demonym", "Indian"], "confidence": 1.0, "cue": "Indian is the demonym for people from India.", "support": "Indian, Indian"}]}}}
{"id": "ont_18_scientist_test_55", "input text": "Darinka Dentcheva, who is known for Stochastic programming, was born January 1, 1958.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, who is known for Stochastic programming, was born January 1, 1958.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"Stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"Darinka Dentcheva, who is known for Stochastic programming,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthDate\", \"January 1, 1958\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born\",\n      \"support\": \"Darinka Dentcheva, who is known for Stochastic programming, was born January 1, 1958.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "Stochastic programming"], "confidence": 1.0, "cue": "known for", "support": "Darinka Dentcheva, who is known for Stochastic programming,"}, {"triple": ["Darinka Dentcheva", "birthDate", "January 1, 1958"], "confidence": 1.0, "cue": "was born", "support": "Darinka Dentcheva, who is known for Stochastic programming, was born January 1, 1958."}]}}}
{"id": "ont_18_scientist_test_56", "input text": "Darlington is in the North East England region.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darlington is in the North East England region.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"region\", \"North East England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is in\",\n      \"support\": \"Darlington is in the North East England region.\",\n      \"notes\": \"The subject 'Darlington' is a City, and the object 'North East England' is a Place.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "region", "North East England"], "confidence": 1.0, "cue": "is in", "support": "Darlington is in the North East England region.", "notes": "The subject 'Darlington' is a City, and the object 'North East England' is a Place."}]}}}
{"id": "ont_18_scientist_test_57", "input text": "The leader's title in India is \"President.\"", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The leader's title in India is \\\"President.\\\"\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"leader\", \"leaderTitle\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The leader's title in India is \\\"President.\\\"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["leader", "leaderTitle", "India"], "confidence": 1.0, "cue": "The leader's title in India is \"President.\""}]}}}
{"id": "ont_18_scientist_test_58", "input text": "Darlington, population 106,000, was the birthplace of Lady Anne Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darlington, population 106,000, was the birthplace of Lady Anne Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"birthPlace\", \"Place\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was the birthplace of\",\n      \"support\": \"Darlington was the birthplace of Lady Anne Monson.\",\n      \"notes\": \"The subject 'Darlington' is a City and the object 'Place' is the inferred domain for the birthPlace relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "birthPlace", "Place"], "confidence": 1.0, "cue": "was the birthplace of", "support": "Darlington was the birthplace of Lady Anne Monson.", "notes": "The subject 'Darlington' is a City and the object 'Place' is the inferred domain for the birthPlace relation."}]}}}
{"id": "ont_18_scientist_test_59", "input text": "A Chief Justice is a leadership title in India.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"A Chief Justice is a leadership title in India.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"A Chief Justice\", \"leaderTitle\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"leadership title in\",\n      \"support\": \"A Chief Justice is a leadership title in India.\",\n      \"notes\": \"The subject is a type of Person, but the relation 'leaderTitle' is not typically associated with a Person's type in the ontology. However, in this context, it is used to denote the country's leader, which is a valid application of the relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["A Chief Justice", "leaderTitle", "India"], "confidence": 1.0, "cue": "leadership title in", "support": "A Chief Justice is a leadership title in India.", "notes": "The subject is a type of Person, but the relation 'leaderTitle' is not typically associated with a Person's type in the ontology. However, in this context, it is used to denote the country's leader, which is a valid application of the relation."}]}}}
{"id": "ont_18_scientist_test_60", "input text": "Brandon Carter was born in England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born in England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"was born in England\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "was born in", "support": "was born in England", "notes": "domain/range check"}]}}}
{"id": "ont_18_scientist_test_61", "input text": "Darinka Dentcheva was born on January 1st, 1958. She is a mathematician working in the field of mathematical optimization, and is known for her work with stochastic programming. One of her theories was influenced by Andrzej Piotr Ruszczyński. She is a citizen of the United States and resides in New Jersey.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva was born on January 1st, 1958. She is a mathematician working in the field of mathematical optimization, and is known for her work with stochastic programming. One of her theories was influenced by Andrzej Piotr Ruszczyński. She is a citizen of the United States and resides in New Jersey.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She is a citizen of the United States\",\n      \"support\": \"She is a citizen of the United States\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She is a mathematician working in the field of mathematical optimization\",\n      \"support\": \"She is a mathematician working in the field of mathematical optimization\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"type\", \"Scientist\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She is a mathematician\",\n      \"support\": \"She is a mathematician\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"January 1st, 1958\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She was born on January 1st, 1958\",\n      \"support\": \"She was born on January 1st, 1958\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She is a citizen of the United States\",\n      \"support\": \"She is a citizen of the United States\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She resides in New Jersey\",\n      \"support\": \"She resides in New Jersey\"\n    },\n    {\n      \"triple\": [\"One of her theories\", \"influencedBy\", \"Andrzej Piotr Ruszczyński\"],\n      \"confidence\": 1.0,\n      \"cue\": \"One of her theories was influenced by Andrzej Piotr Ruszczyński\",\n      \"support\": \"One of her theories was influenced by Andrzej Piotr Ruszczyński\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "She is a citizen of the United States", "support": "She is a citizen of the United States"}, {"triple": ["Darinka Dentcheva", "professionalField", "mathematical optimization"], "confidence": 1.0, "cue": "She is a mathematician working in the field of mathematical optimization", "support": "She is a mathematician working in the field of mathematical optimization"}, {"triple": ["Darinka Dentcheva", "type", "Scientist"], "confidence": 1.0, "cue": "She is a mathematician", "support": "She is a mathematician"}, {"triple": ["Darinka Dentcheva", "birthPlace", "January 1st, 1958"], "confidence": 1.0, "cue": "She was born on January 1st, 1958", "support": "She was born on January 1st, 1958"}, {"triple": ["Darinka Dentcheva", "nationality", "United States"], "confidence": 1.0, "cue": "She is a citizen of the United States", "support": "She is a citizen of the United States"}, {"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "She resides in New Jersey", "support": "She resides in New Jersey"}, {"triple": ["One of her theories", "influencedBy", "Andrzej Piotr Ruszczyński"], "confidence": 1.0, "cue": "One of her theories was influenced by Andrzej Piotr Ruszczyński", "support": "One of her theories was influenced by Andrzej Piotr Ruszczyński"}]}}}
{"id": "ont_18_scientist_test_62", "input text": "Lady Anne Monson, a British national was born in Darlington, Kingdom of England, on January 1, 1726. She resided in India for most of her life with spouse George Monson and died on February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, a British national was born in Darlington, Kingdom of England, on January 1, 1726. She resided in India for most of her life with spouse George Monson and died on February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"citizenship\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in Darlington, Kingdom of England\",\n      \"support\": \"Lady Anne Monson, a British national was born in Darlington, Kingdom of England,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"British\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a British national\",\n      \"support\": \"Lady Anne Monson, a British national was born in Darlington, Kingdom of England,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in Darlington\",\n      \"support\": \"Lady Anne Monson, a British national was born in Darlington,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18th, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on February 18th, 1776\",\n      \"support\": \"She resided in India for most of her life with spouse George Monson and died on February 18th, 1776.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died in India\",\n      \"support\": \"She resided in India for most of her life with spouse George Monson and died on February 18th, 1776.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"with spouse George Monson\",\n      \"support\": \"She resided in India for most of her life with spouse George Monson and died on February 18th, 1776.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "citizenship", "Kingdom of England"], "confidence": 1.0, "cue": "born in Darlington, Kingdom of England", "support": "Lady Anne Monson, a British national was born in Darlington, Kingdom of England,"}, {"triple": ["Lady Anne Monson", "nationality", "British"], "confidence": 1.0, "cue": "a British national", "support": "Lady Anne Monson, a British national was born in Darlington, Kingdom of England,"}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "born in Darlington", "support": "Lady Anne Monson, a British national was born in Darlington,"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18th, 1776"], "confidence": 1.0, "cue": "died on February 18th, 1776", "support": "She resided in India for most of her life with spouse George Monson and died on February 18th, 1776."}, {"triple": ["Lady Anne Monson", "deathPlace", "India"], "confidence": 1.0, "cue": "died in India", "support": "She resided in India for most of her life with spouse George Monson and died on February 18th, 1776."}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "with spouse George Monson", "support": "She resided in India for most of her life with spouse George Monson and died on February 18th, 1776."}]}}}
{"id": "ont_18_scientist_test_63", "input text": "Lady Anne Monson was born in Darlington, in the Kingdom of England, on January 1st, 1726. Her nationality was with the Kingdom of England, but her residence was in India. She passed away on February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington, in the Kingdom of England, on January 1st, 1726. Her nationality was with the Kingdom of England, but her residence was in India. She passed away on February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Her nationality was with the Kingdom of England\",\n      \"support\": \"Her nationality was with the Kingdom of England\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in Darlington\",\n      \"support\": \"born in Darlington\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"residence was in India\",\n      \"support\": \"residence was in India\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18th, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"She passed away on February 18th, 1776\",\n      \"support\": \"She passed away on February 18th, 1776\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "Her nationality was with the Kingdom of England", "support": "Her nationality was with the Kingdom of England"}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "born in Darlington", "support": "born in Darlington"}, {"triple": ["Lady Anne Monson", "deathPlace", "India"], "confidence": 1.0, "cue": "residence was in India", "support": "residence was in India"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18th, 1776"], "confidence": 1.0, "cue": "She passed away on February 18th, 1776", "support": "She passed away on February 18th, 1776"}]}}}
{"id": "ont_18_scientist_test_64", "input text": "India's total population is 1,293,057,000 and the country’s demonym is Indian. Lady Anne Monson, born January 1, 1726, known for her study of botany, resided here most of her life.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"India's total population is 1,293,057,000 and the country’s demonym is Indian. Lady Anne Monson, born January 1, 1726, known for her study of botany, resided here most of her life.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"here\"],\n      \"confidence\": 0.8,\n      \"cue\": \"resided here\",\n      \"support\": \"Lady Anne Monson, born January 1, 1726, known for her study of botany, resided here most of her life.\",\n      \"notes\": \"The pronoun 'her' is resolved to 'Lady Anne Monson'.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Indian\"],\n      \"confidence\": 0.2,\n      \"cue\": \"India's\",\n      \"support\": \"India's total population is 1,293,057,000 and the country’s demonym is Indian.\",\n      \"notes\": \"The pronoun 'India's' is resolved to 'Indian' based on the context.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"January 1, 1726\"],\n      \"confidence\": 0.8,\n      \"cue\": \"born\",\n      \"support\": \"Lady Anne Monson, born January 1, 1726,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"professionalField\", \"botany\"],\n      \"confidence\": 0.6,\n      \"cue\": \"known for her study of botany\",\n      \"support\": \"Lady Anne Monson, born January 1, 1726, known for her study of botany,\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "residence", "here"], "confidence": 0.8, "cue": "resided here", "support": "Lady Anne Monson, born January 1, 1726, known for her study of botany, resided here most of her life.", "notes": "The pronoun 'her' is resolved to 'Lady Anne Monson'."}, {"triple": ["Lady Anne Monson", "nationality", "Indian"], "confidence": 0.2, "cue": "India's", "support": "India's total population is 1,293,057,000 and the country’s demonym is Indian.", "notes": "The pronoun 'India's' is resolved to 'Indian' based on the context."}, {"triple": ["Lady Anne Monson", "birthPlace", "January 1, 1726"], "confidence": 0.8, "cue": "born", "support": "Lady Anne Monson, born January 1, 1726,"}, {"triple": ["Lady Anne Monson", "professionalField", "botany"], "confidence": 0.6, "cue": "known for her study of botany", "support": "Lady Anne Monson, born January 1, 1726, known for her study of botany,"}]}}}
{"id": "ont_18_scientist_test_65", "input text": "Brandon Carter was born in England on January 1st, 1942. His contributions to the field of general relativity were conducted under Dennis William Sciama, his doctoral advisor.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born in England on January 1st, 1942. His contributions to the field of general relativity were conducted under Dennis William Sciama, his doctoral advisor.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter was born in England\",\n      \"notes\": \"subject is a Scientist, place is a Country\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on January 1st, 1942\",\n      \"support\": \"Brandon Carter was born in England on January 1st, 1942\",\n      \"notes\": \"subject is a Scientist, date is a Date\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"conducted under Dennis William Sciama\",\n      \"support\": \"His contributions to the field of general relativity were conducted under Dennis William Sciama\",\n      \"notes\": \"subject is a Scientist, object is a Scientist\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter was born in England", "notes": "subject is a Scientist, place is a Country"}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "born on January 1st, 1942", "support": "Brandon Carter was born in England on January 1st, 1942", "notes": "subject is a Scientist, date is a Date"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "conducted under Dennis William Sciama", "support": "His contributions to the field of general relativity were conducted under Dennis William Sciama", "notes": "subject is a Scientist, object is a Scientist"}]}}}
{"id": "ont_18_scientist_test_66", "input text": "Brandon Carter, born in England, had Dennis William Sciama as his doctoral advisor.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, born in England, had Dennis William Sciama as his doctoral advisor.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter, born in England,\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter, born in England,\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"had Dennis William Sciama as his doctoral advisor.\",\n      \"support\": \"had Dennis William Sciama as his doctoral advisor.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter, born in England,"}, {"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter, born in England,"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "had Dennis William Sciama as his doctoral advisor.", "support": "had Dennis William Sciama as his doctoral advisor."}]}}}
{"id": "ont_18_scientist_test_67", "input text": "India has reached the total population of 1,293,057,000.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"India has reached the total population of 1,293,057,000.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"India\", \"populationTotal\", \"1,293,057,000\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has reached the total population of\",\n      \"support\": \"India has reached the total population of 1,293,057,000\",\n      \"notes\": \"The population is a number, which matches the range for the 'populationTotal' relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["India", "populationTotal", "1,293,057,000"], "confidence": 1.0, "cue": "has reached the total population of", "support": "India has reached the total population of 1,293,057,000", "notes": "The population is a number, which matches the range for the 'populationTotal' relation."}]}}}
{"id": "ont_18_scientist_test_68", "input text": "Brandon Carter, who was born on 01-01-1942, goes to Dennis William Sciama for doctoral advice.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, who was born on 01-01-1942, goes to Dennis William Sciama for doctoral advice.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"goes to Dennis William Sciama for doctoral advice.\",\n      \"support\": \"Dennis William Sciama\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "goes to Dennis William Sciama for doctoral advice.", "support": "Dennis William Sciama"}]}}}
{"id": "ont_18_scientist_test_69", "input text": "Contributor to the no-hair theorem and developer of the Carter Constant, Brandon Carter, works in the field of General Relativity. He was born in England on January, 1 1942. He graduated from the University of Cambridge where he was under the doctoral advisement of Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Contributor to the no-hair theorem and developer of the Carter Constant, Brandon Carter, works in the field of General Relativity. He was born in England on January, 1 1942. He graduated from the University of Cambridge where he was under the doctoral advisement of Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"He was born in England\",\n      \"support\": \"He was born in England\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where he was under the doctoral advisement of Dennis William Sciama.\",\n      \"support\": \"where he was under the doctoral advisement of Dennis William Sciama.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"affiliation\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where he was under the doctoral advisement of Dennis William Sciama.\",\n      \"support\": \"where he was under the doctoral advisement of Dennis William Sciama.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"country\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"He was born in England\",\n      \"support\": \"He was born in England\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"He was born in England on January, 1 1942.\",\n      \"support\": \"He was born in England on January, 1 1942.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "He was born in England", "support": "He was born in England"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "where he was under the doctoral advisement of Dennis William Sciama.", "support": "where he was under the doctoral advisement of Dennis William Sciama."}, {"triple": ["Brandon Carter", "affiliation", "University of Cambridge"], "confidence": 1.0, "cue": "where he was under the doctoral advisement of Dennis William Sciama.", "support": "where he was under the doctoral advisement of Dennis William Sciama."}, {"triple": ["Brandon Carter", "country", "England"], "confidence": 1.0, "cue": "He was born in England", "support": "He was born in England"}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "He was born in England on January, 1 1942.", "support": "He was born in England on January, 1 1942."}]}}}
{"id": "ont_18_scientist_test_70", "input text": "Lady Anne Monson works in the professional field of Botany and is married to George Monson. She is of the English nationality but currently resides in India.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson works in the professional field of Botany and is married to George Monson. She is of the English nationality but currently resides in India.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"but currently resides in India\",\n      \"support\": \"but currently resides in India\",\n      \"notes\": \"Inferred nationality from current residence and common knowledge\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"but currently resides in India\",\n      \"support\": \"but currently resides in India\",\n      \"notes\": \"Inferred current residence from text\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"professionalField\", \"Botany\"],\n      \"confidence\": 1.0,\n      \"cue\": \"works in the professional field of Botany\",\n      \"support\": \"works in the professional field of Botany\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"but currently resides in India\",\n      \"support\": \"but currently resides in India\",\n      \"notes\": \"Inferred nationality from current residence and common knowledge\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"married to George Monson\",\n      \"support\": \"married to George Monson\",\n      \"notes\": \"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "but currently resides in India", "support": "but currently resides in India", "notes": "Inferred nationality from current residence and common knowledge"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "but currently resides in India", "support": "but currently resides in India", "notes": "Inferred current residence from text"}, {"triple": ["Lady Anne Monson", "professionalField", "Botany"], "confidence": 1.0, "cue": "works in the professional field of Botany", "support": "works in the professional field of Botany", "notes": ""}, {"triple": ["Lady Anne Monson", "country", "England"], "confidence": 1.0, "cue": "but currently resides in India", "support": "but currently resides in India", "notes": "Inferred nationality from current residence and common knowledge"}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "married to George Monson", "support": "married to George Monson", "notes": ""}]}}}
{"id": "ont_18_scientist_test_71", "input text": "Lady Anne Monson, who was born in the Kingdom of England, lived there, and held British nationality, died on February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, who was born in the Kingdom of England, lived there, and held British nationality, died on February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived there\",\n      \"support\": \"Lady Anne Monson, who was born in the Kingdom of England, lived there,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Lady Anne Monson, who was born in the Kingdom of England,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"18th, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"died on February 18th, 1776.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "lived there", "support": "Lady Anne Monson, who was born in the Kingdom of England, lived there,"}, {"triple": ["Lady Anne Monson", "birthPlace", "Kingdom of England"], "confidence": 1.0, "cue": "born in", "support": "Lady Anne Monson, who was born in the Kingdom of England,"}, {"triple": ["Lady Anne Monson", "deathDate", "18th, 1776"], "confidence": 1.0, "cue": "died on", "support": "died on February 18th, 1776."}]}}}
{"id": "ont_18_scientist_test_72", "input text": "Darinka Dentcheva was born January 1, 1958. She attended Humboldt University and is a US citizen. Darinka Dentcheva is a mathematician who developed a theory influenced by Andrzej Piotr Ruszczynski in the field of mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva was born January 1, 1958. She attended Humboldt University and is a US citizen. Darinka Dentcheva is a mathematician who developed a theory influenced by Andrzej Piotr Ruszczynski in the field of mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"US\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a US citizen.\",\n      \"support\": \"is a US citizen.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical Optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a mathematician who developed a theory in the field of mathematical optimization.\",\n      \"support\": \"is a mathematician who developed a theory in the field of mathematical optimization.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"January 1, 1958\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born January 1, 1958.\",\n      \"support\": \"was born January 1, 1958.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"developed a theory influenced by Andrzej Piotr Ruszczynski.\",\n      \"support\": \"developed a theory influenced by Andrzej Piotr Ruszczynski.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "US"], "confidence": 1.0, "cue": "is a US citizen.", "support": "is a US citizen."}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical Optimization"], "confidence": 1.0, "cue": "is a mathematician who developed a theory in the field of mathematical optimization.", "support": "is a mathematician who developed a theory in the field of mathematical optimization."}, {"triple": ["Darinka Dentcheva", "birthPlace", "January 1, 1958"], "confidence": 1.0, "cue": "was born January 1, 1958.", "support": "was born January 1, 1958."}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "developed a theory influenced by Andrzej Piotr Ruszczynski.", "support": "developed a theory influenced by Andrzej Piotr Ruszczynski."}]}}}
{"id": "ont_18_scientist_test_73", "input text": "Brandon Carter, a University of Cambridge alum in the field of General Relativity, was born on January 1, 1942.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, a University of Cambridge alum in the field of General Relativity, was born on January 1, 1942.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"alum\",\n      \"support\": \"Brandon Carter, a University of Cambridge alum\",\n      \"notes\": \"The pronoun 'he' is not explicitly stated but can be inferred as the subject of the sentence\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"General Relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in the field of\",\n      \"support\": \"in the field of General Relativity\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"University of Cambridge\"],\n      \"confidence\": 0.0,\n      \"cue\": \"\",\n      \"support\": \"\",\n      \"notes\": \"The text does not provide information about Brandon Carter's birthplace. The almaMater relation is used instead as a proxy.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"\",\n      \"support\": \"\",\n      \"notes\": \"The birth date is explicitly stated in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "alum", "support": "Brandon Carter, a University of Cambridge alum", "notes": "The pronoun 'he' is not explicitly stated but can be inferred as the subject of the sentence"}, {"triple": ["Brandon Carter", "professionalField", "General Relativity"], "confidence": 1.0, "cue": "in the field of", "support": "in the field of General Relativity"}, {"triple": ["Brandon Carter", "birthPlace", "University of Cambridge"], "confidence": 0.0, "cue": "", "support": "", "notes": "The text does not provide information about Brandon Carter's birthplace. The almaMater relation is used instead as a proxy."}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "", "support": "", "notes": "The birth date is explicitly stated in the text."}]}}}
{"id": "ont_18_scientist_test_74", "input text": "Born in Darlington on January 1st, 1726, Lady Anne Monson lived in the Kingdom of England holding British nationality.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Born in Darlington on January 1st, 1726, Lady Anne Monson lived in the Kingdom of England holding British nationality.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"holding British nationality.\",\n      \"support\": \"holding British nationality.\",\n      \"notes\": \"The subject is explicitly stated as 'Lady Anne Monson' and the nationality is inferred from the text.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Born in Darlington\",\n      \"support\": \"Born in Darlington\",\n      \"notes\": \"The subject is explicitly stated as 'Lady Anne Monson' and the birthplace is inferred from the text.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"living in the Kingdom of England\",\n      \"support\": \"living in the Kingdom of England\",\n      \"notes\": \"The subject is explicitly stated as 'Lady Anne Monson' and the country is inferred from the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "holding British nationality.", "support": "holding British nationality.", "notes": "The subject is explicitly stated as 'Lady Anne Monson' and the nationality is inferred from the text."}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "Born in Darlington", "support": "Born in Darlington", "notes": "The subject is explicitly stated as 'Lady Anne Monson' and the birthplace is inferred from the text."}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "living in the Kingdom of England", "support": "living in the Kingdom of England", "notes": "The subject is explicitly stated as 'Lady Anne Monson' and the country is inferred from the text."}]}}}
{"id": "ont_18_scientist_test_75", "input text": "Lady Anne Monson was born in the Kingdom of England whose capital was previously Winchester.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in the Kingdom of England whose capital was previously Winchester.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in the Kingdom of England\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in the Kingdom of England\"\n    },\n    {\n      \"triple\": [\"Kingdom of England\", \"capital\", \"Winchester\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose capital was previously\",\n      \"support\": \"whose capital was previously Winchester\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in the Kingdom of England"}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in the Kingdom of England"}, {"triple": ["Kingdom of England", "capital", "Winchester"], "confidence": 1.0, "cue": "whose capital was previously", "support": "whose capital was previously Winchester"}]}}}
{"id": "ont_18_scientist_test_76", "input text": "Darinka Dentcheva, born January 1, 1958, is a professional in the field of mathematical optimization known for Stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, born January 1, 1958, is a professional in the field of mathematical optimization known for Stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"Stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for Stochastic programming.\",\n      \"support\": \"Stochastic programming.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"country\", \"unknown\"],\n      \"confidence\": 0.5,\n      \"cue\": \"no explicit cue for citizenship or nationality\",\n      \"support\": \"Darinka Dentcheva\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"unknown\"],\n      \"confidence\": 0.5,\n      \"cue\": \"born January 1, 1958\",\n      \"support\": \"January 1, 1958\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "Stochastic programming"], "confidence": 1.0, "cue": "known for Stochastic programming.", "support": "Stochastic programming."}, {"triple": ["Darinka Dentcheva", "country", "unknown"], "confidence": 0.5, "cue": "no explicit cue for citizenship or nationality", "support": "Darinka Dentcheva"}, {"triple": ["Darinka Dentcheva", "birthPlace", "unknown"], "confidence": 0.5, "cue": "born January 1, 1958", "support": "January 1, 1958"}]}}}
{"id": "ont_18_scientist_test_77", "input text": "Lady Anne Monson is from the Kingdom of England. She resided in India and was married to George Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson is from the Kingdom of England. She resided in India and was married to George Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is from\",\n      \"support\": \"Lady Anne Monson is from the Kingdom of England.\",\n      \"notes\": \"directly stated\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is from\",\n      \"support\": \"Lady Anne Monson is from the Kingdom of England.\",\n      \"notes\": \"directly stated\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resided in\",\n      \"support\": \"She resided in India.\",\n      \"notes\": \"directly stated\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was married to\",\n      \"support\": \"She was married to George Monson.\",\n      \"notes\": \"directly stated\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "is from", "support": "Lady Anne Monson is from the Kingdom of England.", "notes": "directly stated"}, {"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "is from", "support": "Lady Anne Monson is from the Kingdom of England.", "notes": "directly stated"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "resided in", "support": "She resided in India.", "notes": "directly stated"}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "was married to", "support": "She was married to George Monson.", "notes": "directly stated"}]}}}
{"id": "ont_18_scientist_test_78", "input text": "Brandon Carter was born in 1942 on January, 1 and graduated from the University of Cambridge. He is best known for creating the Penrose diagram.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born in 1942 on January, 1 and graduated from the University of Cambridge. He is best known for creating the Penrose diagram.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born\",\n      \"support\": \"Brandon Carter was born in 1942 on January, 1\",\n      \"notes\": \"The date is extracted as a factual triple since it is explicitly mentioned in the text.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"he graduated from the University of Cambridge\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Penrose diagram\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is best known for\",\n      \"support\": \"He is best known for creating the Penrose diagram\",\n      \"notes\": \"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "1942-01-01"], "confidence": 1.0, "cue": "was born", "support": "Brandon Carter was born in 1942 on January, 1", "notes": "The date is extracted as a factual triple since it is explicitly mentioned in the text."}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "he graduated from the University of Cambridge", "notes": ""}, {"triple": ["Brandon Carter", "knownFor", "Penrose diagram"], "confidence": 1.0, "cue": "is best known for", "support": "He is best known for creating the Penrose diagram", "notes": ""}]}}}
{"id": "ont_18_scientist_test_79", "input text": "Brandon Carter, born in England on January 1, 1942, had Dennis William Sciama as his doctoral advisor.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, born in England on January 1, 1942, had Dennis William Sciama as his doctoral advisor.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"England on January 1, 1942\",\n      \"notes\": \"The subject 'Brandon Carter' is a Scientist and 'England' is a Country.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"England on January 1, 1942\",\n      \"notes\": \"The subject 'Brandon Carter' is a Scientist and 'England' is a Country.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on\",\n      \"support\": \"January 1, 1942\",\n      \"notes\": \"The subject 'Brandon Carter' is a Scientist and '1942-01-01' is a Date.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"had\",\n      \"support\": \"Dennis William Sciama as his doctoral advisor.\",\n      \"notes\": \"The subject 'Brandon Carter' is a Scientist and 'Dennis William Sciama' is also a Scientist.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "England on January 1, 1942", "notes": "The subject 'Brandon Carter' is a Scientist and 'England' is a Country."}, {"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "born in", "support": "England on January 1, 1942", "notes": "The subject 'Brandon Carter' is a Scientist and 'England' is a Country."}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "born on", "support": "January 1, 1942", "notes": "The subject 'Brandon Carter' is a Scientist and '1942-01-01' is a Date."}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "had", "support": "Dennis William Sciama as his doctoral advisor.", "notes": "The subject 'Brandon Carter' is a Scientist and 'Dennis William Sciama' is also a Scientist."}]}}}
{"id": "ont_18_scientist_test_80", "input text": "Brandon Carter was born in England on January 1, 1942. He graduated from the University of Cambridge and the chancellor of the University was known as David Sainsbury. The doctoral advisor of Brandon Carter while he was at the University was Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born in England on January 1, 1942. He graduated from the University of Cambridge and the chancellor of the University was known as David Sainsbury. The doctoral advisor of Brandon Carter while he was at the University was Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Brandon Carter was born in England\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"Brandon Carter graduated from the University of Cambridge\"\n    },\n    {\n      \"triple\": [\"David Sainsbury\", \"chancellor\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was known as\",\n      \"support\": \"The chancellor of the University was known as David Sainsbury\"\n    },\n    {\n      \"triple\": [\"Dennis William Sciama\", \"doctoralAdvisor\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was the doctoral advisor of\",\n      \"support\": \"The doctoral advisor of Brandon Carter while he was at the University was Dennis William Sciama\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "was born in", "support": "Brandon Carter was born in England"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "Brandon Carter graduated from the University of Cambridge"}, {"triple": ["David Sainsbury", "chancellor", "University of Cambridge"], "confidence": 1.0, "cue": "was known as", "support": "The chancellor of the University was known as David Sainsbury"}, {"triple": ["Dennis William Sciama", "doctoralAdvisor", "Brandon Carter"], "confidence": 1.0, "cue": "was the doctoral advisor of", "support": "The doctoral advisor of Brandon Carter while he was at the University was Dennis William Sciama"}]}}}
{"id": "ont_18_scientist_test_81", "input text": "Lady Anne Monson was born in Darlington, England making her nationality English.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington, England making her nationality English.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"making her nationality English.\",\n      \"support\": \"making her nationality English.\",\n      \"notes\": \"The pronoun 'her' is resolved to the nearest valid antecedent, Lady Anne Monson.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in Darlington\",\n      \"support\": \"born in Darlington,\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "making her nationality English.", "support": "making her nationality English.", "notes": "The pronoun 'her' is resolved to the nearest valid antecedent, Lady Anne Monson."}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "born in Darlington", "support": "born in Darlington,"}]}}}
{"id": "ont_18_scientist_test_82", "input text": "University of Cambridge is located in England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"University of Cambridge is located in England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"University of Cambridge\", \"isPartOf\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"University of Cambridge is located in England.\",\n      \"notes\": \"The subject is a University, and the object is a Country.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["University of Cambridge", "isPartOf", "England"], "confidence": 1.0, "cue": "located in", "support": "University of Cambridge is located in England.", "notes": "The subject is a University, and the object is a Country."}]}}}
{"id": "ont_18_scientist_test_83", "input text": "Mathematical optimization is the professional field of Andrzej Piotr Ruszczynski.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Mathematical optimization is the professional field of Andrzej Piotr Ruszczynski.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Andrzej Piotr Ruszczynski\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the professional field of\",\n      \"support\": \"Mathematical optimization is the professional field of Andrzej Piotr Ruszczynski.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Andrzej Piotr Ruszczynski", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "is the professional field of", "support": "Mathematical optimization is the professional field of Andrzej Piotr Ruszczynski."}]}}}
{"id": "ont_18_scientist_test_84", "input text": "Lady Anne Monson lived in the Kingdom of England and held British nationality but resided in India for most of her life. Pranab Mukherjee is the leader of India, which has a total area of 32875900000000.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson lived in the Kingdom of England and held British nationality but resided in India for most of her life. Pranab Mukherjee is the leader of India, which has a total area of 32875900000000.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"held British nationality\",\n      \"support\": \"Lady Anne Monson held British nationality\",\n      \"notes\": \"Implicitly, the subject is a Scientist as per the given context.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resided in India for most of her life\",\n      \"support\": \"Lady Anne Monson resided in India for most of her life\",\n      \"notes\": \"Implicitly, the subject is a Person as per the given context.\"\n    },\n    {\n      \"triple\": [\"Pranab Mukherjee\", \"leader\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the leader of India\",\n      \"support\": \"Pranab Mukherjee is the leader of India\",\n      \"notes\": \"Implicitly, the subject is a Person as per the given context.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "England"], "confidence": 1.0, "cue": "held British nationality", "support": "Lady Anne Monson held British nationality", "notes": "Implicitly, the subject is a Scientist as per the given context."}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "resided in India for most of her life", "support": "Lady Anne Monson resided in India for most of her life", "notes": "Implicitly, the subject is a Person as per the given context."}, {"triple": ["Pranab Mukherjee", "leader", "India"], "confidence": 1.0, "cue": "is the leader of India", "support": "Pranab Mukherjee is the leader of India", "notes": "Implicitly, the subject is a Person as per the given context."}]}}}
{"id": "ont_18_scientist_test_85", "input text": "Darinka Dentcheva, a citizen of the United States of America, attended Humboldt University and is known for her work with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, a citizen of the United States of America, attended Humboldt University and is known for her work with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States of America\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a citizen of the United States of America\",\n      \"support\": \"Darinka Dentcheva, a citizen of the United States of America,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"nationality\", \"United States of America\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a citizen of the United States of America\",\n      \"support\": \"Darinka Dentcheva, a citizen of the United States of America,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"attended Humboldt University\",\n      \"support\": \"attended Humboldt University\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"her work with stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for her work with stochastic programming\",\n      \"support\": \"is known for her work with stochastic programming.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States of America"], "confidence": 1.0, "cue": "a citizen of the United States of America", "support": "Darinka Dentcheva, a citizen of the United States of America,"}, {"triple": ["Darinka Dentcheva", "nationality", "United States of America"], "confidence": 1.0, "cue": "a citizen of the United States of America", "support": "Darinka Dentcheva, a citizen of the United States of America,"}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "attended Humboldt University", "support": "attended Humboldt University"}, {"triple": ["Darinka Dentcheva", "knownFor", "her work with stochastic programming"], "confidence": 1.0, "cue": "known for her work with stochastic programming", "support": "is known for her work with stochastic programming."}]}}}
{"id": "ont_18_scientist_test_86", "input text": "University of Cambridge is affiliated with the League of European Research Universities and is the alma mater of Brandon Carter.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"University of Cambridge is affiliated with the League of European Research Universities and is the alma mater of Brandon Carter.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"University of Cambridge\", \"almaMater\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the alma mater of Brandon Carter\",\n      \"support\": \"University of Cambridge is the alma mater of Brandon Carter\"\n    },\n    {\n      \"triple\": [\"University of Cambridge\", \"affiliation\", \"League of European Research Universities\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is affiliated with the League of European Research Universities\",\n      \"support\": \"University of Cambridge is affiliated with the League of European Research Universities\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["University of Cambridge", "almaMater", "Brandon Carter"], "confidence": 1.0, "cue": "is the alma mater of Brandon Carter", "support": "University of Cambridge is the alma mater of Brandon Carter"}, {"triple": ["University of Cambridge", "affiliation", "League of European Research Universities"], "confidence": 1.0, "cue": "is affiliated with the League of European Research Universities", "support": "University of Cambridge is affiliated with the League of European Research Universities"}]}}}
{"id": "ont_18_scientist_test_87", "input text": "Brandon Carter, known for the Carter constant and No-hair theorem, is a professional in the field of general relativity.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, known for the Carter constant and No-hair theorem, is a professional in the field of general relativity.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"No-hair theorem, Carter constant\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"Brandon Carter, known for the Carter constant and No-hair theorem,\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"general relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in the field of\",\n      \"support\": \"is a professional in the field of general relativity.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "No-hair theorem, Carter constant"], "confidence": 1.0, "cue": "known for", "support": "Brandon Carter, known for the Carter constant and No-hair theorem,"}, {"triple": ["Brandon Carter", "professionalField", "general relativity"], "confidence": 1.0, "cue": "in the field of", "support": "is a professional in the field of general relativity."}]}}}
{"id": "ont_18_scientist_test_88", "input text": "Lady Anne Monson's nationality was with the Kingdom of England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson's nationality was with the Kingdom of England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was with\",\n      \"support\": \"Lady Anne Monson's nationality was with the Kingdom of England\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "was with", "support": "Lady Anne Monson's nationality was with the Kingdom of England", "notes": "The subject and object are correctly typed according to the ontology."}]}}}
{"id": "ont_18_scientist_test_89", "input text": "India was founded on January 26, 1950.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"India was founded on January 26, 1950.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"India\", \"foundingDate\", \"1950-01-26\"],\n      \"confidence\": 1.0,\n      \"cue\": \"founded on January 26, 1950\",\n      \"support\": \"founded on January 26, 1950\",\n      \"notes\": \"The date is explicitly mentioned in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["India", "foundingDate", "1950-01-26"], "confidence": 1.0, "cue": "founded on January 26, 1950", "support": "founded on January 26, 1950", "notes": "The date is explicitly mentioned in the text."}]}}}
{"id": "ont_18_scientist_test_90", "input text": "Brandon Carter is known for the Carter constant and the No-hair theorem.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter is known for the Carter constant and the No-hair theorem.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Carter constant and the No-hair theorem\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Brandon Carter is known for the Carter constant and the No-hair theorem.\",\n      \"notes\": \"The relation 'knownFor' has a domain of Scientist and a range of Thing. The subject and object are correctly typed.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "Carter constant and the No-hair theorem"], "confidence": 1.0, "cue": "is known for", "support": "Brandon Carter is known for the Carter constant and the No-hair theorem.", "notes": "The relation 'knownFor' has a domain of Scientist and a range of Thing. The subject and object are correctly typed."}]}}}
{"id": "ont_18_scientist_test_91", "input text": "Brandon Carter is known for the Carter constant.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter is known for the Carter constant.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Carter constant\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Brandon Carter is known for the Carter constant.\",\n      \"notes\": \"The relation 'knownFor' is between 'Scientist' (domain) and 'Thing' (range), and Brandon Carter is a Scientist, and 'Carter constant' is a Thing, so this triple is valid.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "Carter constant"], "confidence": 1.0, "cue": "is known for", "support": "Brandon Carter is known for the Carter constant.", "notes": "The relation 'knownFor' is between 'Scientist' (domain) and 'Thing' (range), and Brandon Carter is a Scientist, and 'Carter constant' is a Thing, so this triple is valid."}]}}}
{"id": "ont_18_scientist_test_92", "input text": "Lady Anne Monson was born in Darlington on January 1st, 1726.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington on January 1st, 1726.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington on January 1st, 1726.\",\n      \"notes\": \"The subject is a Scientist (given the context of the text), and the range is Place.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington on January 1st, 1726.", "notes": "The subject is a Scientist (given the context of the text), and the range is Place."}]}}}
{"id": "ont_18_scientist_test_93", "input text": "Brandon Carter is known for his work with the Doomsday argument.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter is known for his work with the Doomsday argument.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Doomsday argument\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Brandon Carter is known for his work with the Doomsday argument.\",\n      \"notes\": \"The subject and object are correctly typed as Scientist and Thing respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "knownFor", "Doomsday argument"], "confidence": 1.0, "cue": "is known for", "support": "Brandon Carter is known for his work with the Doomsday argument.", "notes": "The subject and object are correctly typed as Scientist and Thing respectively."}]}}}
{"id": "ont_18_scientist_test_94", "input text": "Darinka Dentcheva currently resides in the US state of New Jersey, where one of the leaders of the country is the Vice President.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva currently resides in the US state of New Jersey, where one of the leaders of the country is the Vice President.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resides in\",\n      \"support\": \"Darinka Dentcheva currently resides in the US state of New Jersey,\"\n    },\n    {\n      \"triple\": [\"the country\", \"leader\", \"Vice President\"],\n      \"confidence\": 1.0,\n      \"cue\": \"one of the leaders of the country is\",\n      \"support\": \"where one of the leaders of the country is the Vice President\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "resides in", "support": "Darinka Dentcheva currently resides in the US state of New Jersey,"}, {"triple": ["the country", "leader", "Vice President"], "confidence": 1.0, "cue": "one of the leaders of the country is", "support": "where one of the leaders of the country is the Vice President"}]}}}
{"id": "ont_18_scientist_test_95", "input text": "Botanist Lady Anne Monson was born in Darlington in the Kingdom of England. She lived from January 1st, 1726 to February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Botanist Lady Anne Monson was born in Darlington in the Kingdom of England. She lived from January 1st, 1726 to February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington in the Kingdom of England\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born\",\n      \"support\": \"January 1st, 1726\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died\",\n      \"support\": \"February 18th, 1776\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington in the Kingdom of England\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington in the Kingdom of England"}, {"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "was born", "support": "January 1st, 1726"}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died", "support": "February 18th, 1776"}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Darlington in the Kingdom of England"}]}}}
{"id": "ont_18_scientist_test_96", "input text": "The OS grid reference for Darlington is NZ289147.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The OS grid reference for Darlington is NZ289147.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"gridReference\", \"NZ289147\"],\n      \"confidence\": 1.0,\n      \"cue\": \"OS grid reference for Darlington is NZ289147\",\n      \"support\": \"The OS grid reference for Darlington is NZ289147\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "gridReference", "NZ289147"], "confidence": 1.0, "cue": "OS grid reference for Darlington is NZ289147", "support": "The OS grid reference for Darlington is NZ289147"}]}}}
{"id": "ont_18_scientist_test_97", "input text": "Darinka Dentcheva is known for her work with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is known for her work with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"her work with stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"Darinka Dentcheva is known for her work with stochastic programming\",\n      \"notes\": \"subject and object are explicitly mentioned\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "her work with stochastic programming"], "confidence": 1.0, "cue": "known for", "support": "Darinka Dentcheva is known for her work with stochastic programming", "notes": "subject and object are explicitly mentioned"}]}}}
{"id": "ont_18_scientist_test_98", "input text": "Brandon Carter who was born in England, graduated from the University of Cambridge and developed General Relativity as his profession. His doctoral advisor was Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter who was born in England, graduated from the University of Cambridge and developed General Relativity as his profession. His doctoral advisor was Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter who was born in England,\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"University of Cambridge\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"General Relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"developed General Relativity as his profession.\",\n      \"support\": \"General Relativity as his profession.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose doctoral advisor was\",\n      \"support\": \"Dennis William Sciama.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter who was born in England,"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "University of Cambridge"}, {"triple": ["Brandon Carter", "professionalField", "General Relativity"], "confidence": 1.0, "cue": "developed General Relativity as his profession.", "support": "General Relativity as his profession."}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "whose doctoral advisor was", "support": "Dennis William Sciama."}]}}}
{"id": "ont_18_scientist_test_99", "input text": "Lady Anne Monson was born in Darlington in the Kingdom of England on January 1, 1726. She was the spouse of George Monson. Lady Anne Monson died on 1776-02-18.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington in the Kingdom of England on January 1, 1726. She was the spouse of George Monson. Lady Anne Monson died on 1776-02-18.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Darlington in the Kingdom of England\",\n      \"notes\": \"subject is a person, Kingdom of England is a Country\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Darlington in the Kingdom of England\",\n      \"notes\": \"subject is a person, Darlington is a Place\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"spouse of\",\n      \"support\": \"George Monson\",\n      \"notes\": \"subject and object are both Person\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"1776-02-18\",\n      \"notes\": \"subject is a Person, deathDate is a Date\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"not specified\"],\n      \"confidence\": 0.5,\n      \"cue\": \"died on\",\n      \"support\": \"1776-02-18\",\n      \"notes\": \"deathPlace is not explicitly mentioned, inferred from the text\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "born in", "support": "Darlington in the Kingdom of England", "notes": "subject is a person, Kingdom of England is a Country"}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "born in", "support": "Darlington in the Kingdom of England", "notes": "subject is a person, Darlington is a Place"}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "spouse of", "support": "George Monson", "notes": "subject and object are both Person"}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on", "support": "1776-02-18", "notes": "subject is a Person, deathDate is a Date"}, {"triple": ["Lady Anne Monson", "deathPlace", "not specified"], "confidence": 0.5, "cue": "died on", "support": "1776-02-18", "notes": "deathPlace is not explicitly mentioned, inferred from the text"}]}}}
{"id": "ont_18_scientist_test_100", "input text": "Lady Anne Monson whose nationality was with the Kingdom of England, had a residence in India and died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson whose nationality was with the Kingdom of England, had a residence in India and died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose nationality was with\",\n      \"support\": \"Lady Anne Monson whose nationality was with the Kingdom of England,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"had a residence in\",\n      \"support\": \"Lady Anne Monson had a residence in India\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"died on February 18, 1776.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "nationality", "Kingdom of England"], "confidence": 1.0, "cue": "whose nationality was with", "support": "Lady Anne Monson whose nationality was with the Kingdom of England,"}, {"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 1.0, "cue": "had a residence in", "support": "Lady Anne Monson had a residence in India"}, {"triple": ["Lady Anne Monson", "deathDate", "February 18, 1776"], "confidence": 1.0, "cue": "died on", "support": "died on February 18, 1776."}]}}}
{"id": "ont_18_scientist_test_101", "input text": "Dennis William Sciama was the doctoral advisor for Brandon Carter, known for the Carter Constant.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Dennis William Sciama was the doctoral advisor for Brandon Carter, known for the Carter Constant.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Dennis William Sciama\", \"doctoralAdvisor\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was the doctoral advisor for\",\n      \"support\": \"Dennis William Sciama was the doctoral advisor for Brandon Carter,\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", \"Carter Constant\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"known for the Carter Constant.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Dennis William Sciama", "doctoralAdvisor", "Brandon Carter"], "confidence": 1.0, "cue": "was the doctoral advisor for", "support": "Dennis William Sciama was the doctoral advisor for Brandon Carter,"}, {"triple": ["Brandon Carter", "knownFor", "Carter Constant"], "confidence": 1.0, "cue": "known for", "support": "known for the Carter Constant."}]}}}
{"id": "ont_18_scientist_test_102", "input text": "Darinka Dentcheva (United States citizen) was born on January 1st, 1958. Mathematical optimization is her professional field.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva (United States citizen) was born on January 1st, 1958. Mathematical optimization is her professional field.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"implied by the fact that she is a United States citizen\",\n      \"support\": \"Darinka Dentcheva (United States citizen)\",\n      \"notes\": \"implied from the demonym 'United States citizen'\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Mathematical optimization is her professional field\",\n      \"support\": \"Mathematical optimization is her professional field.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthDate\", \"1958-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on January 1st, 1958\",\n      \"support\": \"was born on January 1st, 1958.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "country", "United States"], "confidence": 1.0, "cue": "implied by the fact that she is a United States citizen", "support": "Darinka Dentcheva (United States citizen)", "notes": "implied from the demonym 'United States citizen'"}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "Mathematical optimization is her professional field", "support": "Mathematical optimization is her professional field."}, {"triple": ["Darinka Dentcheva", "birthDate", "1958-01-01"], "confidence": 1.0, "cue": "was born on January 1st, 1958", "support": "was born on January 1st, 1958."}]}}}
{"id": "ont_18_scientist_test_103", "input text": "Darinka Dentcheva worked in the field of mathematical optimization and was influenced by Andrzej Piotr Ruszczynski. She was born on January 1st 1989 and is a citizen of the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva worked in the field of mathematical optimization and was influenced by Andrzej Piotr Ruszczynski. She was born on January 1st 1989 and is a citizen of the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"She\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a citizen of the United States\",\n      \"support\": \"She is a citizen of the United States.\"\n    },\n    {\n      \"triple\": [\"She\", \"professionalField\", \"mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"worked in the field of mathematical optimization\",\n      \"support\": \"Darinka Dentcheva worked in the field of mathematical optimization\"\n    },\n    {\n      \"triple\": [\"She\", \"birthPlace\", \"January 1st 1989\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on January 1st 1989\",\n      \"support\": \"She was born on January 1st 1989\"\n    },\n    {\n      \"triple\": [\"She\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a citizen of the United States\",\n      \"support\": \"She is a citizen of the United States.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["She", "citizenship", "United States"], "confidence": 1.0, "cue": "is a citizen of the United States", "support": "She is a citizen of the United States."}, {"triple": ["She", "professionalField", "mathematical optimization"], "confidence": 1.0, "cue": "worked in the field of mathematical optimization", "support": "Darinka Dentcheva worked in the field of mathematical optimization"}, {"triple": ["She", "birthPlace", "January 1st 1989"], "confidence": 1.0, "cue": "born on January 1st 1989", "support": "She was born on January 1st 1989"}, {"triple": ["She", "country", "United States"], "confidence": 1.0, "cue": "is a citizen of the United States", "support": "She is a citizen of the United States."}]}}}
{"id": "ont_18_scientist_test_104", "input text": "The University of Cambridge is the alma mater of Brandon Carter, who was born in England. David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The University of Cambridge is the alma mater of Brandon Carter, who was born in England. David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"University of Cambridge\", \"almaMater\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"alma mater of Brandon Carter\",\n      \"support\": \"The University of Cambridge is the alma mater of Brandon Carter,\"\n    },\n    {\n      \"triple\": [\"University of Cambridge\", \"chancellor\", \"David Sainsbury\"],\n      \"confidence\": 1.0,\n      \"cue\": \"David Sainsbury,... the chancellor\",\n      \"support\": \"David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge.\"\n    },\n    {\n      \"triple\": [\"University of Cambridge\", \"viceChancellor\", \"Leszek Borysiewicz\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Leszek Borysiewicz,... the vice chancellor\",\n      \"support\": \"David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["University of Cambridge", "almaMater", "Brandon Carter"], "confidence": 1.0, "cue": "alma mater of Brandon Carter", "support": "The University of Cambridge is the alma mater of Brandon Carter,"}, {"triple": ["University of Cambridge", "chancellor", "David Sainsbury"], "confidence": 1.0, "cue": "David Sainsbury,... the chancellor", "support": "David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge."}, {"triple": ["University of Cambridge", "viceChancellor", "Leszek Borysiewicz"], "confidence": 1.0, "cue": "Leszek Borysiewicz,... the vice chancellor", "support": "David Sainsbury, also known as the Baron Sainsbury of Turville, and Leszek Borysiewicz are respectively the chancellor and vice chancellor of the University of Cambridge."}]}}}
{"id": "ont_18_scientist_test_105", "input text": "Brandon Carter, whose alma mater is the University of Cambridge with Dennis William Sciama as his doctoral advisor, was born in England on January 1, 1942 and is known for the Doomsday argument.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, whose alma mater is the University of Cambridge with Dennis William Sciama as his doctoral advisor, was born in England on January 1, 1942 and is known for the Doomsday argument.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose alma mater is the University of Cambridge\",\n      \"support\": \"whose alma mater is the University of Cambridge\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in England\",\n      \"support\": \"born in England\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on January 1, 1942\",\n      \"support\": \"born on January 1, 1942\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"his doctoral advisor\",\n      \"support\": \"his doctoral advisor\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "whose alma mater is the University of Cambridge", "support": "whose alma mater is the University of Cambridge"}, {"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in England", "support": "born in England"}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "born on January 1, 1942", "support": "born on January 1, 1942"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "his doctoral advisor", "support": "his doctoral advisor"}]}}}
{"id": "ont_18_scientist_test_106", "input text": "Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"country\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on January 1st, 1942\",\n      \"support\": \"Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "country", "England"], "confidence": 1.0, "cue": "born in", "support": "Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "born on January 1st, 1942", "support": "Known for his contributions to the no-hair theorem, Brandon Carter was born on January 1st, 1942 in England.", "notes": "domain/range check"}]}}}
{"id": "ont_18_scientist_test_107", "input text": "Brandon Carter, who was born on January 1st, 1942, is a graduate from the University of Cambridge.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter, who was born on January 1st, 1942, is a graduate from the University of Cambridge.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a graduate from\",\n      \"support\": \"University of Cambridge\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"country\", \"United Kingdom\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on January 1st, 1942\",\n      \"support\": \"January 1st, 1942\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "is a graduate from", "support": "University of Cambridge"}, {"triple": ["Brandon Carter", "country", "United Kingdom"], "confidence": 1.0, "cue": "born on January 1st, 1942", "support": "January 1st, 1942"}]}}}
{"id": "ont_18_scientist_test_108", "input text": "Dennis William Sciama was the doctoral advisor of Brandon Carter, an England native, who contributed to the No-Hair Theorem, but is more famously known for the Carter Constant and his work in General Relativity.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Dennis William Sciama was the doctoral advisor of Brandon Carter, an England native, who contributed to the No-Hair Theorem, but is more famously known for the Carter Constant and his work in General Relativity.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Dennis William Sciama\", \"citizenship\", \"England\"],\n      \"confidence\": 0.8,\n      \"cue\": \"who\",\n      \"support\": \"who contributed to the No-Hair Theorem\",\n      \"notes\": \"Brandon Carter is the subject of the sentence, but the text does not specify his citizenship. Inferring that Brandon Carter is English as he is a native of England.\"\n    },\n    {\n      \"triple\": [\"Dennis William Sciama\", \"professionalField\", \"General Relativity\"],\n      \"confidence\": 0.9,\n      \"cue\": \"is more famously known for\",\n      \"support\": \"is more famously known for the Carter Constant and his work in General Relativity.\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Dennis William Sciama\", \"doctoralAdvisor\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was the doctoral advisor of\",\n      \"support\": \"was the doctoral advisor of Brandon Carter,\"\n      \"notes\": \"\"\n    }\n  ]\n}", "json": null}}
{"id": "ont_18_scientist_test_109", "input text": "Lady Anne Monson professional field of study was botany.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson professional field of study was botany.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"professionalField\", \"botany\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was botany\",\n      \"support\": \"Lady Anne Monson was botany.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "professionalField", "botany"], "confidence": 1.0, "cue": "was botany", "support": "Lady Anne Monson was botany.", "notes": "The subject and object are correctly typed according to the ontology."}]}}}
{"id": "ont_18_scientist_test_110", "input text": "Darinka Dentcheva ( birth date January 1st, 1958) studied Mathematical optimization from Humboldt University. She is known for her work with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva ( birth date January 1st, 1958) studied Mathematical optimization from Humboldt University. She is known for her work with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthDate\", \"January 1st, 1958\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied\",\n      \"support\": \"Darinka Dentcheva ( birth date January 1st, 1958)\",\n      \"notes\": \"Explicit date mentioned in the text\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"studied Mathematical optimization\",\n      \"support\": \"Darinka Dentcheva ( birth date January 1st, 1958) studied Mathematical optimization\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"her work with stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"She is known for her work with stochastic programming\",\n      \"notes\": \"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "birthDate", "January 1st, 1958"], "confidence": 1.0, "cue": "studied", "support": "Darinka Dentcheva ( birth date January 1st, 1958)", "notes": "Explicit date mentioned in the text"}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "studied Mathematical optimization", "support": "Darinka Dentcheva ( birth date January 1st, 1958) studied Mathematical optimization", "notes": ""}, {"triple": ["Darinka Dentcheva", "knownFor", "her work with stochastic programming"], "confidence": 1.0, "cue": "known for", "support": "She is known for her work with stochastic programming", "notes": ""}]}}}
{"id": "ont_18_scientist_test_111", "input text": "The motto of the United States is \"In God We Trust\".", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The motto of the United States is \\\"In God We Trust\\\".\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"United States\", \"motto\", \"\\\"In God We Trust\\\"\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The motto of the United States is\",\n      \"support\": \"The motto of the United States is \\\"In God We Trust\\\".\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["United States", "motto", "\"In God We Trust\""], "confidence": 1.0, "cue": "The motto of the United States is", "support": "The motto of the United States is \"In God We Trust\"."}]}}}
{"id": "ont_18_scientist_test_112", "input text": "Darinka Dentcheva was born on January 1, 1958.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva was born on January 1, 1958.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"January 1, 1958\"],\n      \"confidence\": 0.0,\n      \"cue\": \"was born on\",\n      \"support\": \"Darinka Dentcheva was born on January 1, 1958\",\n      \"notes\": \"The date is a literal mention in the text, but it does not fit any of the specified ontology's range types.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "birthPlace", "January 1, 1958"], "confidence": 0.0, "cue": "was born on", "support": "Darinka Dentcheva was born on January 1, 1958", "notes": "The date is a literal mention in the text, but it does not fit any of the specified ontology's range types."}]}}}
{"id": "ont_18_scientist_test_113", "input text": "Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen. The area total of the United States is 9,833,520.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen. The area total of the United States is 9,833,520.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a United States Citizen\",\n      \"support\": \"Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen.\",\n      \"notes\": \"The subject is explicitly mentioned and the citizenship relation is inferred from the text.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a United States Citizen\",\n      \"support\": \"Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen.\",\n      \"notes\": \"The country of citizenship is inferred from the text.\"\n    },\n    {\n      \"triple\": [\"United States\", \"areaTotal\", \"9,833,520\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The area total of the United States is\",\n      \"support\": \"The area total of the United States is 9,833,520.\",\n      \"notes\": \"The areaTotal relation is explicitly mentioned and the number is extracted from the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "is a United States Citizen", "support": "Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen.", "notes": "The subject is explicitly mentioned and the citizenship relation is inferred from the text."}, {"triple": ["Darinka Dentcheva", "country", "United States"], "confidence": 1.0, "cue": "is a United States Citizen", "support": "Darinka Dentcheva, who was born on January 1, 1958 and was influenced by Andrzej Piotr Ruszczynski, is a United States Citizen.", "notes": "The country of citizenship is inferred from the text."}, {"triple": ["United States", "areaTotal", "9,833,520"], "confidence": 1.0, "cue": "The area total of the United States is", "support": "The area total of the United States is 9,833,520.", "notes": "The areaTotal relation is explicitly mentioned and the number is extracted from the text."}]}}}
{"id": "ont_18_scientist_test_114", "input text": "Humboldt University is Darinka Dentcheva's alma mater.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Humboldt University is Darinka Dentcheva's alma mater.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Humboldt University\", \"almaMater\", \"Darinka Dentcheva\"],\n      \"confidence\": 1.0,\n      \"cue\": \"alma mater\",\n      \"support\": \"Humboldt University is Darinka Dentcheva's alma mater.\",\n      \"notes\": \"The subject and object are correctly typed as University and Scientist respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Humboldt University", "almaMater", "Darinka Dentcheva"], "confidence": 1.0, "cue": "alma mater", "support": "Humboldt University is Darinka Dentcheva's alma mater.", "notes": "The subject and object are correctly typed as University and Scientist respectively."}]}}}
{"id": "ont_18_scientist_test_115", "input text": "Even though Darinka Dentcheva's alma mater is Humboldt University, she currently resides in New Jersey (United States).", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Even though Darinka Dentcheva's alma mater is Humboldt University, she currently resides in New Jersey (United States).\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Darinka Dentcheva's alma mater is Humboldt University\",\n      \"support\": \"Darinka Dentcheva's alma mater is Humboldt University\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey (United States)\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she currently resides in New Jersey (United States)\",\n      \"support\": \"she currently resides in New Jersey (United States)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "Darinka Dentcheva's alma mater is Humboldt University", "support": "Darinka Dentcheva's alma mater is Humboldt University"}, {"triple": ["Darinka Dentcheva", "residence", "New Jersey (United States)"], "confidence": 1.0, "cue": "she currently resides in New Jersey (United States)", "support": "she currently resides in New Jersey (United States)"}]}}}
{"id": "ont_18_scientist_test_116", "input text": "Influenced by Andrzej Piotr Ruszczyński, Darinka Dentcheva became a professional in the field of mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Influenced by Andrzej Piotr Ruszczyński, Darinka Dentcheva became a professional in the field of mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczyński\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Influenced by Andrzej Piotr Ruszczyński\",\n      \"support\": \"Influenced by Andrzej Piotr Ruszczyński\",\n      \"notes\": \"The relation 'influencedBy' is not directly between a Scientist and a Country, Field, etc., but it is a valid relation in the given context.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczyński"], "confidence": 1.0, "cue": "Influenced by Andrzej Piotr Ruszczyński", "support": "Influenced by Andrzej Piotr Ruszczyński", "notes": "The relation 'influencedBy' is not directly between a Scientist and a Country, Field, etc., but it is a valid relation in the given context."}]}}}
{"id": "ont_18_scientist_test_117", "input text": "The long name for India is the Republic of India.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The long name for India is the Republic of India.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Republic of India\", \"longName\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The long name for India is\",\n      \"support\": \"The long name for India is the Republic of India.\",\n      \"notes\": \"The subject and object are explicitly mentioned in the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Republic of India", "longName", "India"], "confidence": 1.0, "cue": "The long name for India is", "support": "The long name for India is the Republic of India.", "notes": "The subject and object are explicitly mentioned in the text."}]}}}
{"id": "ont_18_scientist_test_118", "input text": "Darinka Dentcheva resides in New Jersey, USA. Her alma mater is Humboldt University. Darinka Dentcheva is known for stochastic programming and was influenced by Andrzej Piotr Ruszczynski.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva resides in New Jersey, USA. Her alma mater is Humboldt University. Darinka Dentcheva is known for stochastic programming and was influenced by Andrzej Piotr Ruszczynski.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey, USA\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resides in\",\n      \"support\": \"Darinka Dentcheva resides in New Jersey, USA\",\n      \"notes\": \"Darinka Dentcheva is the subject, and 'New Jersey, USA' is the object; the relation'residence' is inferred from the cue'resides in'.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Her alma mater is\",\n      \"support\": \"Her alma mater is Humboldt University\",\n      \"notes\": \"Darinka Dentcheva is the subject, and 'Humboldt University' is the object; the relation 'almaMater' is inferred from the cue 'Her alma mater is'.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Darinka Dentcheva is known for stochastic programming\",\n      \"notes\": \"Darinka Dentcheva is the subject, and'stochastic programming' is the object; the relation 'knownFor' is inferred from the cue 'is known for'.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was influenced by\",\n      \"support\": \"Darinka Dentcheva was influenced by Andrzej Piotr Ruszczynski\",\n      \"notes\": \"Darinka Dentcheva is the subject, and 'Andrzej Piotr Ruszczynski' is the object; the relation 'influencedBy' is inferred from the cue 'was influenced by'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "residence", "New Jersey, USA"], "confidence": 1.0, "cue": "resides in", "support": "Darinka Dentcheva resides in New Jersey, USA", "notes": "Darinka Dentcheva is the subject, and 'New Jersey, USA' is the object; the relation'residence' is inferred from the cue'resides in'."}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "Her alma mater is", "support": "Her alma mater is Humboldt University", "notes": "Darinka Dentcheva is the subject, and 'Humboldt University' is the object; the relation 'almaMater' is inferred from the cue 'Her alma mater is'."}, {"triple": ["Darinka Dentcheva", "knownFor", "stochastic programming"], "confidence": 1.0, "cue": "is known for", "support": "Darinka Dentcheva is known for stochastic programming", "notes": "Darinka Dentcheva is the subject, and'stochastic programming' is the object; the relation 'knownFor' is inferred from the cue 'is known for'."}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "was influenced by", "support": "Darinka Dentcheva was influenced by Andrzej Piotr Ruszczynski", "notes": "Darinka Dentcheva is the subject, and 'Andrzej Piotr Ruszczynski' is the object; the relation 'influencedBy' is inferred from the cue 'was influenced by'."}]}}}
{"id": "ont_18_scientist_test_119", "input text": "Darinka Dentcheva's professional field is Mathematical optimization, particularly with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva's professional field is Mathematical optimization, particularly with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"professional field is\",\n      \"support\": \"Darinka Dentcheva's professional field is Mathematical optimization\",\n      \"notes\": \"Scientist (Darinka Dentcheva) - Field (Mathematical optimization)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "professional field is", "support": "Darinka Dentcheva's professional field is Mathematical optimization", "notes": "Scientist (Darinka Dentcheva) - Field (Mathematical optimization)"}]}}}
{"id": "ont_18_scientist_test_120", "input text": "Lady Anne Monson died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on February 18, 1776\",\n      \"support\": \"Lady Anne Monson died on February 18, 1776\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on February 18, 1776", "support": "Lady Anne Monson died on February 18, 1776"}]}}}
{"id": "ont_18_scientist_test_121", "input text": "Darinka Dentcheva, born on January 1st,1958, is a US citizen.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, born on January 1st,1958, is a US citizen.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"US\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a US citizen.\",\n      \"support\": \"Darinka Dentcheva, born on January 1st,1958, is a US citizen.\",\n      \"notes\": \"The pronoun 'is' refers to the subject 'Darinka Dentcheva'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "US"], "confidence": 1.0, "cue": "is a US citizen.", "support": "Darinka Dentcheva, born on January 1st,1958, is a US citizen.", "notes": "The pronoun 'is' refers to the subject 'Darinka Dentcheva'."}]}}}
{"id": "ont_18_scientist_test_122", "input text": "Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"residence\", \"India\"],\n      \"confidence\": 0.8,\n      \"cue\": \"was a resident of\",\n      \"support\": \"Before her death on February 18, 1776, Lady Anne Monson was a resident of India.\",\n      \"notes\": \"The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"India\"],\n      \"confidence\": 0.8,\n      \"cue\": \"died in\",\n      \"support\": \"Before her death on February 18, 1776, Lady Anne Monson was a resident of India.\",\n      \"notes\": \"The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"nationality\", \"Country\"],\n      \"confidence\": 0.6,\n      \"cue\": \"was\",\n      \"support\": \"Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.\",\n      \"notes\": \"The nationality of Lady Anne Monson is not explicitly stated in the text. The 'Country' domain is used as a placeholder for further investigation.\"\n    },\n    {\n      \"triple\": [\"George Monson\", \"spouse\", \"Lady Anne Monson\"],\n      \"confidence\": 0.8,\n      \"cue\": \"married to\",\n      \"support\": \"Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.\",\n      \"notes\": \"The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "residence", "India"], "confidence": 0.8, "cue": "was a resident of", "support": "Before her death on February 18, 1776, Lady Anne Monson was a resident of India.", "notes": "The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'."}, {"triple": ["Lady Anne Monson", "deathPlace", "India"], "confidence": 0.8, "cue": "died in", "support": "Before her death on February 18, 1776, Lady Anne Monson was a resident of India.", "notes": "The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'."}, {"triple": ["Lady Anne Monson", "nationality", "Country"], "confidence": 0.6, "cue": "was", "support": "Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.", "notes": "The nationality of Lady Anne Monson is not explicitly stated in the text. The 'Country' domain is used as a placeholder for further investigation."}, {"triple": ["George Monson", "spouse", "Lady Anne Monson"], "confidence": 0.8, "cue": "married to", "support": "Before her death on February 18, 1776, Lady Anne Monson was a resident of India married to George Monson.", "notes": "The pronoun 'her' is resolved to the nearest valid antecedent, which is 'Lady Anne Monson'."}]}}}
{"id": "ont_18_scientist_test_123", "input text": "Physicist Brandon Carter, born January 1st, 1942, is a professional in the field of General Relativity. His doctoral advisor was Dennis William Sciama.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Physicist Brandon Carter, born January 1st, 1942, is a professional in the field of General Relativity. His doctoral advisor was Dennis William Sciama.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"General Relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a professional in the field of General Relativity\",\n      \"support\": \"is a professional in the field of General Relativity\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"January 1st, 1942\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born January 1st, 1942\",\n      \"support\": \"born January 1st, 1942\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"His doctoral advisor was Dennis William Sciama\",\n      \"support\": \"His doctoral advisor was Dennis William Sciama\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "professionalField", "General Relativity"], "confidence": 1.0, "cue": "is a professional in the field of General Relativity", "support": "is a professional in the field of General Relativity"}, {"triple": ["Brandon Carter", "birthPlace", "January 1st, 1942"], "confidence": 1.0, "cue": "born January 1st, 1942", "support": "born January 1st, 1942"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "His doctoral advisor was Dennis William Sciama", "support": "His doctoral advisor was Dennis William Sciama"}]}}}
{"id": "ont_18_scientist_test_124", "input text": "The chancellor of the University of Cambridge is David Sainsbury, Baron Sainsbury of Turville.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The chancellor of the University of Cambridge is David Sainsbury, Baron Sainsbury of Turville.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"University of Cambridge\", \"chancellor\", \"David Sainsbury, Baron Sainsbury of Turville\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The chancellor of the University of Cambridge\",\n      \"support\": \"The chancellor of the University of Cambridge is David Sainsbury, Baron Sainsbury of Turville.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["University of Cambridge", "chancellor", "David Sainsbury, Baron Sainsbury of Turville"], "confidence": 1.0, "cue": "The chancellor of the University of Cambridge", "support": "The chancellor of the University of Cambridge is David Sainsbury, Baron Sainsbury of Turville."}]}}}
{"id": "ont_18_scientist_test_125", "input text": "Darinka Dentcheva, a United States citizen born January 1, 1958, is a mathematician working in the field of mathematical optimization. She is known for her work with stochastic programming and developed a theory influenced by Andrzej Piotr Ruszczyński. Humboldt University is her alma mater.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, a United States citizen born January 1, 1958, is a mathematician working in the field of mathematical optimization. She is known for her work with stochastic programming and developed a theory influenced by Andrzej Piotr Ruszczyński. Humboldt University is her alma mater.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a United States citizen\",\n      \"support\": \"a United States citizen born January 1, 1958,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"working in the field of mathematical optimization\",\n      \"support\": \"working in the field of mathematical optimization\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"Place\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born January 1, 1958\",\n      \"support\": \"born January 1, 1958,\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Humboldt University is her alma mater\",\n      \"support\": \"Humboldt University is her alma mater.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "a United States citizen", "support": "a United States citizen born January 1, 1958,"}, {"triple": ["Darinka Dentcheva", "professionalField", "mathematical optimization"], "confidence": 1.0, "cue": "working in the field of mathematical optimization", "support": "working in the field of mathematical optimization"}, {"triple": ["Darinka Dentcheva", "birthPlace", "Place"], "confidence": 1.0, "cue": "born January 1, 1958", "support": "born January 1, 1958,"}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "Humboldt University is her alma mater", "support": "Humboldt University is her alma mater."}]}}}
{"id": "ont_18_scientist_test_126", "input text": "Brandon Carter was born in England on January 1st, 1942. He is a physicist and a professional in the field of general relativity. Dennis William Sciama was his doctoral advisor. Carter is known for inventing the Carter constant and proving the no-hair theorem.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born in England on January 1st, 1942. He is a physicist and a professional in the field of general relativity. Dennis William Sciama was his doctoral advisor. Carter is known for inventing the Carter constant and proving the no-hair theorem.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter was born in England on January 1st, 1942.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"nationality\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter was born in England on January 1st, 1942.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"birthDate\", \"1942-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born on\",\n      \"support\": \"Brandon Carter was born in England on January 1st, 1942.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"professionalField\", \"general relativity\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a professional in the field of\",\n      \"support\": \"He is a physicist and a professional in the field of general relativity.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"his doctoral advisor\",\n      \"support\": \"Dennis William Sciama was his doctoral advisor.\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", [\"Carter constant\", \"no-hair theorem\"]],\n      \"confidence\": 1.0,\n      \"cue\": \"known for inventing\",\n      \"support\": \"Carter is known for inventing the Carter constant and proving the no-hair theorem.\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter was born in England on January 1st, 1942.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "nationality", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter was born in England on January 1st, 1942.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "birthDate", "1942-01-01"], "confidence": 1.0, "cue": "born on", "support": "Brandon Carter was born in England on January 1st, 1942.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "professionalField", "general relativity"], "confidence": 1.0, "cue": "a professional in the field of", "support": "He is a physicist and a professional in the field of general relativity.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "his doctoral advisor", "support": "Dennis William Sciama was his doctoral advisor.", "notes": "domain/range check"}, {"triple": ["Brandon Carter", "knownFor", ["Carter constant", "no-hair theorem"]], "confidence": 1.0, "cue": "known for inventing", "support": "Carter is known for inventing the Carter constant and proving the no-hair theorem.", "notes": "domain/range check"}]}}}
{"id": "ont_18_scientist_test_127", "input text": "John Roberts is a leader in the United States, where Darinka Dentcheva resides (in New Jersey), and Joe Biden is also one of the leaders.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"John Roberts is a leader in the United States, where Darinka Dentcheva resides (in New Jersey), and Joe Biden is also one of the leaders.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"John Roberts\", \"leader\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a leader in the United States\",\n      \"support\": \"John Roberts is a leader in the United States\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey\"],\n      \"confidence\": 1.0,\n      \"cue\": \"resides (in New Jersey)\",\n      \"support\": \"Darinka Dentcheva resides (in New Jersey)\"\n    },\n    {\n      \"triple\": [\"Joe Biden\", \"leader\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is also one of the leaders\",\n      \"support\": \"Joe Biden is also one of the leaders\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["John Roberts", "leader", "United States"], "confidence": 1.0, "cue": "is a leader in the United States", "support": "John Roberts is a leader in the United States"}, {"triple": ["Darinka Dentcheva", "residence", "New Jersey"], "confidence": 1.0, "cue": "resides (in New Jersey)", "support": "Darinka Dentcheva resides (in New Jersey)"}, {"triple": ["Joe Biden", "leader", "United States"], "confidence": 1.0, "cue": "is also one of the leaders", "support": "Joe Biden is also one of the leaders"}]}}}
{"id": "ont_18_scientist_test_128", "input text": "Lady Anne Monson was born on January 1, 1726 in Darlington, the Kingdom of England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born on January 1, 1726 in Darlington, the Kingdom of England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington, the Kingdom of England\",\n      \"notes\": \"Scientist domain is not mentioned, but Lady Anne Monson is a person, and the text does not suggest any other domain. Place domain is inferred from the context.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington, the Kingdom of England\",\n      \"notes\": \"Scientist domain is not mentioned, but Lady Anne Monson is a person, and the text does not suggest any other domain. Country domain is inferred from the context.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on January 1,\",\n      \"support\": \"1726-01-01\",\n      \"notes\": \"No explicit domain is mentioned, but the date is associated with the subject, Lady Anne Monson, so it is inferred that this is a date related to her.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington, the Kingdom of England", "notes": "Scientist domain is not mentioned, but Lady Anne Monson is a person, and the text does not suggest any other domain. Place domain is inferred from the context."}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Darlington, the Kingdom of England", "notes": "Scientist domain is not mentioned, but Lady Anne Monson is a person, and the text does not suggest any other domain. Country domain is inferred from the context."}, {"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "was born on January 1,", "support": "1726-01-01", "notes": "No explicit domain is mentioned, but the date is associated with the subject, Lady Anne Monson, so it is inferred that this is a date related to her."}]}}}
{"id": "ont_18_scientist_test_129", "input text": "British national Lady Anne Monson lived in the Kingdom of England whose capital was Winchester.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"British national Lady Anne Monson lived in the Kingdom of England whose capital was Winchester.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"lived in the Kingdom of England\",\n      \"support\": \"lived in the Kingdom of England\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"capital\", \"Winchester\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose capital was Winchester\",\n      \"support\": \"whose capital was Winchester\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "lived in the Kingdom of England", "support": "lived in the Kingdom of England"}, {"triple": ["Lady Anne Monson", "capital", "Winchester"], "confidence": 1.0, "cue": "whose capital was Winchester", "support": "whose capital was Winchester"}]}}}
{"id": "ont_18_scientist_test_130", "input text": "New Jersey has a total area of 22,591.38.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"New Jersey has a total area of 22,591.38.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"New Jersey\", \"areaTotal\", \"22591.38\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a total area of\",\n      \"support\": \"New Jersey has a total area of 22,591.38.\",\n      \"notes\": \"The subject and object are both Place entities.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["New Jersey", "areaTotal", "22591.38"], "confidence": 1.0, "cue": "has a total area of", "support": "New Jersey has a total area of 22,591.38.", "notes": "The subject and object are both Place entities."}]}}}
{"id": "ont_18_scientist_test_131", "input text": "Lady Anne Monson was born in England and died on February 18th, 1776. She was a botanist and was married to George Monson.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in England and died on February 18th, 1776. She was a botanist and was married to George Monson.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Lady Anne Monson was born in England\",\n      \"notes\": \"subject is a Scientist and Country is the domain of the 'country' relation\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"She died on February 18th, 1776.\",\n      \"notes\": \"subject is a Scientist and Date is the range of the 'deathDate' relation\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"spouse\", \"George Monson\"],\n      \"confidence\": 1.0,\n      \"cue\": \"married to\",\n      \"support\": \"She was married to George Monson.\",\n      \"notes\": \"subject is a Scientist and Person is the range of the'spouse' relation\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "country", "England"], "confidence": 1.0, "cue": "born in", "support": "Lady Anne Monson was born in England", "notes": "subject is a Scientist and Country is the domain of the 'country' relation"}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on", "support": "She died on February 18th, 1776.", "notes": "subject is a Scientist and Date is the range of the 'deathDate' relation"}, {"triple": ["Lady Anne Monson", "spouse", "George Monson"], "confidence": 1.0, "cue": "married to", "support": "She was married to George Monson.", "notes": "subject is a Scientist and Person is the range of the'spouse' relation"}]}}}
{"id": "ont_18_scientist_test_132", "input text": "Lady Anne Monson was born in Darlington which is located in Kingdom of England.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington which is located in Kingdom of England.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in Darlington\",\n      \"notes\": \"subject: Lady Anne Monson (Scientist, Person); object: Darlington (Place)\"\n    },\n    {\n      \"triple\": [\"Darlington\", \"isPartOf\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Darlington which is located in Kingdom of England\",\n      \"notes\": \"subject: Darlington (Place); object: Kingdom of England (Country)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in Darlington", "notes": "subject: Lady Anne Monson (Scientist, Person); object: Darlington (Place)"}, {"triple": ["Darlington", "isPartOf", "Kingdom of England"], "confidence": 1.0, "cue": "located in", "support": "Darlington which is located in Kingdom of England", "notes": "subject: Darlington (Place); object: Kingdom of England (Country)"}]}}}
{"id": "ont_18_scientist_test_133", "input text": "Darinka Dentcheva's current residence is in New Jersey, United States where the most commonly spoken language is English.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva's current residence is in New Jersey, United States where the most commonly spoken language is English.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"residence\", \"New Jersey, United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Darinka Dentcheva's current residence is in New Jersey, United States\",\n      \"notes\": \"The subject is inferred as Darinka Dentcheva from the text.\"\n    },\n    {\n      \"triple\": [\"New Jersey, United States\", \"capital\", \"not mentioned in the text\"],\n      \"confidence\": 0.0,\n      \"cue\": \"not mentioned in the text\",\n      \"notes\": \"The capital of New Jersey, United States is not mentioned in the text.\"\n    },\n    {\n      \"triple\": [\"New Jersey, United States\", \"officialLanguage\", \"English\"],\n      \"confidence\": 1.0,\n      \"cue\": \"the most commonly spoken language is English\",\n      \"notes\": \"The language is inferred as English from the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "residence", "New Jersey, United States"], "confidence": 1.0, "cue": "Darinka Dentcheva's current residence is in New Jersey, United States", "notes": "The subject is inferred as Darinka Dentcheva from the text."}, {"triple": ["New Jersey, United States", "capital", "not mentioned in the text"], "confidence": 0.0, "cue": "not mentioned in the text", "notes": "The capital of New Jersey, United States is not mentioned in the text."}, {"triple": ["New Jersey, United States", "officialLanguage", "English"], "confidence": 1.0, "cue": "the most commonly spoken language is English", "notes": "The language is inferred as English from the text."}]}}}
{"id": "ont_18_scientist_test_134", "input text": "Andrzej Piotr Ruszczyński is known for his work with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Andrzej Piotr Ruszczyński is known for his work with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Andrzej Piotr Ruszczyński\", \"knownFor\", \"stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for\",\n      \"support\": \"Andrzej Piotr Ruszczyński is known for his work with stochastic programming\",\n      \"notes\": \"The subject is a Scientist and the object is a Field\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Andrzej Piotr Ruszczyński", "knownFor", "stochastic programming"], "confidence": 1.0, "cue": "is known for", "support": "Andrzej Piotr Ruszczyński is known for his work with stochastic programming", "notes": "The subject is a Scientist and the object is a Field"}]}}}
{"id": "ont_18_scientist_test_135", "input text": "Lady Anne Monson was born in Darlington, located in the Kingdom of England, on January 1, 1726. She later died on February 18, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson was born in Darlington, located in the Kingdom of England, on January 1, 1726. She later died on February 18, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Lady Anne Monson was born in Darlington,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"located in the Kingdom of England,\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on\",\n      \"support\": \"was born in Darlington, located in the Kingdom of England, on January 1, 1726.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"not_mentioned\"],\n      \"confidence\": 0.5,\n      \"cue\": \"died on\",\n      \"support\": \"She later died on February 18, 1776.\",\n      \"notes\": \"No specific death place is mentioned.\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"1776-02-18\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"She later died on February 18, 1776.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Lady Anne Monson was born in Darlington,"}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "located in", "support": "located in the Kingdom of England,"}, {"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "was born on", "support": "was born in Darlington, located in the Kingdom of England, on January 1, 1726."}, {"triple": ["Lady Anne Monson", "deathPlace", "not_mentioned"], "confidence": 0.5, "cue": "died on", "support": "She later died on February 18, 1776.", "notes": "No specific death place is mentioned."}, {"triple": ["Lady Anne Monson", "deathDate", "1776-02-18"], "confidence": 1.0, "cue": "died on", "support": "She later died on February 18, 1776."}]}}}
{"id": "ont_18_scientist_test_136", "input text": "Even though Darinka Dentcheva was never the Vice President, one of the leaders, of the United States, she was in fact a citizen of the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Even though Darinka Dentcheva was never the Vice President, one of the leaders, of the United States, she was in fact a citizen of the United States.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was a citizen of the United States.\",\n      \"support\": \"United States\",\n      \"notes\": \"The subject is explicitly stated as Darinka Dentcheva, and the country of citizenship is inferred from the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "was a citizen of the United States.", "support": "United States", "notes": "The subject is explicitly stated as Darinka Dentcheva, and the country of citizenship is inferred from the text."}]}}}
{"id": "ont_18_scientist_test_137", "input text": "Brandon Carter was born on January 1st, 1942, in England. He graduated from the University of Cambridge, where his doctoral advisor was Dennis William Sciama. Carter is know for his Doomsday argument and for his work on the Anthropic Principle.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter was born on January 1st, 1942, in England. He graduated from the University of Cambridge, where his doctoral advisor was Dennis William Sciama. Carter is know for his Doomsday argument and for his work on the Anthropic Principle.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"\\\"Brandon Carter was born on January 1st, 1942, in England.\\\"\",\n      \"notes\": \"domain/range check: Scientist (Brandon Carter) - Place (England)\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"\\\"He graduated from the University of Cambridge,\\\"\",\n      \"notes\": \"domain/range check: Scientist (Brandon Carter) - University (University of Cambridge)\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"doctoralAdvisor\", \"Dennis William Sciama\"],\n      \"confidence\": 1.0,\n      \"cue\": \"doctoral advisor was\",\n      \"support\": \"\\\"where his doctoral advisor was Dennis William Sciama.\\\"\",\n      \"notes\": \"domain/range check: Scientist (Brandon Carter) - Scientist (Dennis William Sciama)\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"knownFor\", [\"Doomsday argument\", \"work on the Anthropic Principle\"]],\n      \"confidence\": 1.0,\n      \"cue\": \"known for\",\n      \"support\": \"\\\"Carter is know for his Doomsday argument and for his work on the Anthropic Principle.\\\"\",\n      \"notes\": \"domain/range check: Scientist (Brandon Carter) - Thing (Doomsday argument, work on the Anthropic Principle)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "\"Brandon Carter was born on January 1st, 1942, in England.\"", "notes": "domain/range check: Scientist (Brandon Carter) - Place (England)"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "\"He graduated from the University of Cambridge,\"", "notes": "domain/range check: Scientist (Brandon Carter) - University (University of Cambridge)"}, {"triple": ["Brandon Carter", "doctoralAdvisor", "Dennis William Sciama"], "confidence": 1.0, "cue": "doctoral advisor was", "support": "\"where his doctoral advisor was Dennis William Sciama.\"", "notes": "domain/range check: Scientist (Brandon Carter) - Scientist (Dennis William Sciama)"}, {"triple": ["Brandon Carter", "knownFor", ["Doomsday argument", "work on the Anthropic Principle"]], "confidence": 1.0, "cue": "known for", "support": "\"Carter is know for his Doomsday argument and for his work on the Anthropic Principle.\"", "notes": "domain/range check: Scientist (Brandon Carter) - Thing (Doomsday argument, work on the Anthropic Principle)"}]}}}
{"id": "ont_18_scientist_test_138", "input text": "The birthdate of Lady Anne Monson is 1726-01-01.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The birthdate of Lady Anne Monson is 1726-01-01.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthDate\", \"1726-01-01\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The birthdate of Lady Anne Monson is\",\n      \"support\": \"The birthdate of Lady Anne Monson is 1726-01-01.\",\n      \"notes\": \"Explicitly stated date\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "birthDate", "1726-01-01"], "confidence": 1.0, "cue": "The birthdate of Lady Anne Monson is", "support": "The birthdate of Lady Anne Monson is 1726-01-01.", "notes": "Explicitly stated date"}]}}}
{"id": "ont_18_scientist_test_139", "input text": "The total area of water in New Jersey is 3,544 square kilometers.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The total area of water in New Jersey is 3,544 square kilometers.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"New Jersey\", \"areaOfWater\", \"3,544 square kilometers\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is\",\n      \"support\": \"The total area of water in New Jersey is 3,544 square kilometers\",\n      \"notes\": \"The pronoun 'New Jersey' is the subject of the sentence, and '3,544 square kilometers' is the object.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["New Jersey", "areaOfWater", "3,544 square kilometers"], "confidence": 1.0, "cue": "is", "support": "The total area of water in New Jersey is 3,544 square kilometers", "notes": "The pronoun 'New Jersey' is the subject of the sentence, and '3,544 square kilometers' is the object."}]}}}
{"id": "ont_18_scientist_test_140", "input text": "Brandon Carter (born in 1942 on January, 1) graduated from the University of Cambridge where David Sainsbury is the chancellor, who is also known as Baron Sainsbury of Turville. Dennis William Sciama was the advisor of Carter.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter (born in 1942 on January, 1) graduated from the University of Cambridge where David Sainsbury is the chancellor, who is also known as Baron Sainsbury of Turville. Dennis William Sciama was the advisor of Carter.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"University of Cambridge\",\n      \"notes\": \"Brandon Carter is the subject, and 'University of Cambridge' is the object, both are of type 'Scientist' and 'University' respectively.\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"affiliation\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"graduated from\",\n      \"support\": \"University of Cambridge\",\n      \"notes\": \"Brandon Carter is the subject, and 'University of Cambridge' is the object, both are of type 'Scientist' and 'University' respectively.\"\n    },\n    {\n      \"triple\": [\"David Sainsbury\", \"chancellor\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the chancellor\",\n      \"support\": \"David Sainsbury\",\n      \"notes\": \"David Sainsbury is the subject, and 'University of Cambridge' is the object, both are of type 'Person' and 'University' respectively.\"\n    },\n    {\n      \"triple\": [\"Dennis William Sciama\", \"doctoralAdvisor\", \"Brandon Carter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was the advisor of\",\n      \"support\": \"Dennis William Sciama was the advisor of Carter.\",\n      \"notes\": \"Dennis William Sciama is the subject, and 'Brandon Carter' is the object, both are of type 'Scientist' respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "University of Cambridge", "notes": "Brandon Carter is the subject, and 'University of Cambridge' is the object, both are of type 'Scientist' and 'University' respectively."}, {"triple": ["Brandon Carter", "affiliation", "University of Cambridge"], "confidence": 1.0, "cue": "graduated from", "support": "University of Cambridge", "notes": "Brandon Carter is the subject, and 'University of Cambridge' is the object, both are of type 'Scientist' and 'University' respectively."}, {"triple": ["David Sainsbury", "chancellor", "University of Cambridge"], "confidence": 1.0, "cue": "is the chancellor", "support": "David Sainsbury", "notes": "David Sainsbury is the subject, and 'University of Cambridge' is the object, both are of type 'Person' and 'University' respectively."}, {"triple": ["Dennis William Sciama", "doctoralAdvisor", "Brandon Carter"], "confidence": 1.0, "cue": "was the advisor of", "support": "Dennis William Sciama was the advisor of Carter.", "notes": "Dennis William Sciama is the subject, and 'Brandon Carter' is the object, both are of type 'Scientist' respectively."}]}}}
{"id": "ont_18_scientist_test_141", "input text": "The town of Darlington has a population of one hundred and six thousand.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The town of Darlington has a population of one hundred and six thousand.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darlington\", \"populationTotal\", \"one hundred and six thousand\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a population of\",\n      \"support\": \"The town of Darlington has a population of one hundred and six thousand.\",\n      \"notes\": \"City domain\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darlington", "populationTotal", "one hundred and six thousand"], "confidence": 1.0, "cue": "has a population of", "support": "The town of Darlington has a population of one hundred and six thousand.", "notes": "City domain"}]}}}
{"id": "ont_18_scientist_test_142", "input text": "Lady Anne Monson, a British national was born on January 1, 1726 in Darlington, in the Kingdom of England. She died on February 18th, 1776.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lady Anne Monson, a British national was born on January 1, 1726 in Darlington, in the Kingdom of England. She died on February 18th, 1776.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Lady Anne Monson\", \"citizenship\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington, in the Kingdom of England\",\n      \"notes\": \"Inferred citizenship from birth place\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"birthPlace\", \"Darlington\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington, in the Kingdom of England\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathDate\", \"February 18th, 1776\"],\n      \"confidence\": 1.0,\n      \"cue\": \"died on\",\n      \"support\": \"February 18th, 1776\",\n      \"notes\": \"\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"deathPlace\", \"\"],\n      \"confidence\": 0.0,\n      \"cue\": \"\",\n      \"support\": \"\",\n      \"notes\": \"No explicit death place mentioned in the text\"\n    },\n    {\n      \"triple\": [\"Lady Anne Monson\", \"country\", \"Kingdom of England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born in\",\n      \"support\": \"Darlington, in the Kingdom of England\",\n      \"notes\": \"Inferred country from birth place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Lady Anne Monson", "citizenship", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Darlington, in the Kingdom of England", "notes": "Inferred citizenship from birth place"}, {"triple": ["Lady Anne Monson", "birthPlace", "Darlington"], "confidence": 1.0, "cue": "was born in", "support": "Darlington, in the Kingdom of England", "notes": ""}, {"triple": ["Lady Anne Monson", "deathDate", "February 18th, 1776"], "confidence": 1.0, "cue": "died on", "support": "February 18th, 1776", "notes": ""}, {"triple": ["Lady Anne Monson", "deathPlace", ""], "confidence": 0.0, "cue": "", "support": "", "notes": "No explicit death place mentioned in the text"}, {"triple": ["Lady Anne Monson", "country", "Kingdom of England"], "confidence": 1.0, "cue": "was born in", "support": "Darlington, in the Kingdom of England", "notes": "Inferred country from birth place"}]}}}
{"id": "ont_18_scientist_test_143", "input text": "Brandon Carter born in England attended University of Cambridge whose vice chancellor is Leszek Borysiewicz.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Brandon Carter born in England attended University of Cambridge whose vice chancellor is Leszek Borysiewicz.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Brandon Carter\", \"birthPlace\", \"England\"],\n      \"confidence\": 1.0,\n      \"cue\": \"born in\",\n      \"support\": \"Brandon Carter born in England\",\n      \"notes\": \"Subject is a Scientist and Place is a Country\"\n    },\n    {\n      \"triple\": [\"Brandon Carter\", \"almaMater\", \"University of Cambridge\"],\n      \"confidence\": 1.0,\n      \"cue\": \"attended\",\n      \"support\": \"Brandon Carter attended University of Cambridge\",\n      \"notes\": \"Subject is a Scientist and University is a School\"\n    },\n    {\n      \"triple\": [\"University of Cambridge\", \"viceChancellor\", \"Leszek Borysiewicz\"],\n      \"confidence\": 1.0,\n      \"cue\": \"whose vice chancellor is\",\n      \"support\": \"whose vice chancellor is Leszek Borysiewicz\",\n      \"notes\": \"University is a School and Person is a valid range for the relation\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Brandon Carter", "birthPlace", "England"], "confidence": 1.0, "cue": "born in", "support": "Brandon Carter born in England", "notes": "Subject is a Scientist and Place is a Country"}, {"triple": ["Brandon Carter", "almaMater", "University of Cambridge"], "confidence": 1.0, "cue": "attended", "support": "Brandon Carter attended University of Cambridge", "notes": "Subject is a Scientist and University is a School"}, {"triple": ["University of Cambridge", "viceChancellor", "Leszek Borysiewicz"], "confidence": 1.0, "cue": "whose vice chancellor is", "support": "whose vice chancellor is Leszek Borysiewicz", "notes": "University is a School and Person is a valid range for the relation"}]}}}
{"id": "ont_18_scientist_test_144", "input text": "Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"knownFor\", \"her work with stochastic programming\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is known for her work with\",\n      \"support\": \"Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was influenced by\",\n      \"support\": \"Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "knownFor", "her work with stochastic programming"], "confidence": 1.0, "cue": "is known for her work with", "support": "Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming"}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "was influenced by", "support": "Darinka Dentcheva, who was influenced by Andrzej Piotr Ruszczynski, is known for her work with stochastic programming"}]}}}
{"id": "ont_18_scientist_test_145", "input text": "Kingdom of England is a unitary state form of government which is also Lady Anne Monson's nationality.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Kingdom of England is a unitary state form of government which is also Lady Anne Monson's nationality.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Kingdom of England\", \"nationality\", \"Country\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is also Lady Anne Monson's nationality.\",\n      \"support\": \"which is also Lady Anne Monson's nationality.\",\n      \"notes\": \"The subject is inferred as Kingdom of England as it is the only country mentioned in the text. The object is inferred as Country as it is the range for the nationality relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Kingdom of England", "nationality", "Country"], "confidence": 1.0, "cue": "which is also Lady Anne Monson's nationality.", "support": "which is also Lady Anne Monson's nationality.", "notes": "The subject is inferred as Kingdom of England as it is the only country mentioned in the text. The object is inferred as Country as it is the range for the nationality relation."}]}}}
{"id": "ont_18_scientist_test_146", "input text": "India has a total area of 3287590000000.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"India has a total area of 3287590000000.0.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"India\", \"areaTotal\", \"3287590000000.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a total area of\",\n      \"support\": \"India has a total area of 3287590000000.0\",\n      \"notes\": \"The text provides a numerical value for the area of India.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["India", "areaTotal", "3287590000000.0"], "confidence": 1.0, "cue": "has a total area of", "support": "India has a total area of 3287590000000.0", "notes": "The text provides a numerical value for the area of India."}]}}}
{"id": "ont_18_scientist_test_147", "input text": "The University of Cambridge is a research university.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The University of Cambridge is a research university.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The University of Cambridge\", \"type\", \"research university\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is a\",\n      \"support\": \"The University of Cambridge is a research university\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The University of Cambridge", "type", "research university"], "confidence": 1.0, "cue": "is a", "support": "The University of Cambridge is a research university", "notes": "domain/range check"}]}}}
{"id": "ont_18_scientist_test_148", "input text": "American citizen Darinka Dentcheva was born on January 1st, 1958. After graduating from her alma mater Humboldt University, she began to work professionally in the field of Mathematical Optimization. This New Jersey resident is known for her work with stochastic programming, being heavily influenced by Andrzej Piotr Ruszczynski.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"American citizen Darinka Dentcheva was born on January 1st, 1958. After graduating from her alma mater Humboldt University, she began to work professionally in the field of Mathematical Optimization. This New Jersey resident is known for her work with stochastic programming, being heavily influenced by Andrzej Piotr Ruszczynski.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"America\"],\n      \"confidence\": 1.0,\n      \"cue\": \"American citizen\",\n      \"support\": \"American citizen Darinka Dentcheva\",\n      \"notes\": \"Darinka Dentcheva is explicitly stated to be an American citizen.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical Optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"she began to work professionally in the field of Mathematical Optimization\",\n      \"support\": \"she began to work professionally in the field of Mathematical Optimization\",\n      \"notes\": \"The profession of Darinka Dentcheva is explicitly stated as Mathematical Optimization.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"birthPlace\", \"January 1st, 1958\"],\n      \"confidence\": 1.0,\n      \"cue\": \"was born on January 1st, 1958\",\n      \"support\": \"was born on January 1st, 1958\",\n      \"notes\": \"The birth date of Darinka Dentcheva is explicitly stated as January 1st, 1958.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"almaMater\", \"Humboldt University\"],\n      \"confidence\": 1.0,\n      \"cue\": \"after graduating from her alma mater Humboldt University\",\n      \"support\": \"after graduating from her alma mater Humboldt University\",\n      \"notes\": \"Humboldt University is explicitly stated as the alma mater of Darinka Dentcheva.\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"influencedBy\", \"Andrzej Piotr Ruszczynski\"],\n      \"confidence\": 1.0,\n      \"cue\": \"being heavily influenced by Andrzej Piotr Ruszczynski\",\n      \"support\": \"being heavily influenced by Andrzej Piotr Ruszczynski\",\n      \"notes\": \"Darinka Dentcheva is explicitly stated to be heavily influenced by Andrzej Piotr Ruszczynski.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "America"], "confidence": 1.0, "cue": "American citizen", "support": "American citizen Darinka Dentcheva", "notes": "Darinka Dentcheva is explicitly stated to be an American citizen."}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical Optimization"], "confidence": 1.0, "cue": "she began to work professionally in the field of Mathematical Optimization", "support": "she began to work professionally in the field of Mathematical Optimization", "notes": "The profession of Darinka Dentcheva is explicitly stated as Mathematical Optimization."}, {"triple": ["Darinka Dentcheva", "birthPlace", "January 1st, 1958"], "confidence": 1.0, "cue": "was born on January 1st, 1958", "support": "was born on January 1st, 1958", "notes": "The birth date of Darinka Dentcheva is explicitly stated as January 1st, 1958."}, {"triple": ["Darinka Dentcheva", "almaMater", "Humboldt University"], "confidence": 1.0, "cue": "after graduating from her alma mater Humboldt University", "support": "after graduating from her alma mater Humboldt University", "notes": "Humboldt University is explicitly stated as the alma mater of Darinka Dentcheva."}, {"triple": ["Darinka Dentcheva", "influencedBy", "Andrzej Piotr Ruszczynski"], "confidence": 1.0, "cue": "being heavily influenced by Andrzej Piotr Ruszczynski", "support": "being heavily influenced by Andrzej Piotr Ruszczynski", "notes": "Darinka Dentcheva is explicitly stated to be heavily influenced by Andrzej Piotr Ruszczynski."}]}}}
{"id": "ont_18_scientist_test_149", "input text": "Darinka Dentcheva is a citizen of the United States who works in the field of Mathematical optimization.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Darinka Dentcheva is a citizen of the United States who works in the field of Mathematical optimization.\"\n\n    Ontology concepts:\n    - Scientist\n- Country\n- Field\n- Type\n- City\n- University\n- Place\n- Demonym\n- Organisation\n- Timezone\n- Currency\n- School\n- Language\n- WrittenWork\n- Award\n\n    Ontology relations (domain → range):\n    - citizenship(Scientist,Country)\n- professionalField(Scientist,Field)\n- type(Scientist,Type)\n- areaCode(City,string)\n- deathDate(Scientist,Date)\n- chancellor(University,Person)\n- capital(Country,City)\n- residence(Scientist,Place)\n- populationMetroDensity(City,number)\n- demonym(Scientist,Demonym)\n- birthPlace(Scientist,Place)\n- nationality(Scientist,Country)\n- foundingDate(Organisation,Date)\n- deathPlace(Scientist,Place)\n- postalCode(City,string)\n- motto(University,string)\n- timeZone(City,Timezone)\n- religion(Scientist,Religion)\n- areaMetro(City,number)\n- affiliation(Scientist,University)\n- viceChancellor(University,Person)\n- currency(Country,Currency)\n- dissolutionYear(Organisation,Year)\n- leaderTitle(Country,string)\n- knownFor(Scientist,Thing)\n- gridReference(City,string)\n- leader(Country,leader)\n- isPartOf(City,Country)\n- region(Scientist,Place)\n- areaTotal(City,number)\n- longName(Scientist,string)\n- doctoralAdvisor(Scientist,Scientist)\n- spouse(Scientist,Person)\n- almaMater(Scientist,School)\n- country(Scientist,Country)\n- areaOfWater(Place,number)\n- governmentType(Country,governmentType)\n- birthName(Scientist,string)\n- populationTotal(City,number)\n- language(WrittenWork,Language)\n- award(Scientist,Award)\n- percentageOfAreaWater(Place,number)\n- birthDate(Scientist,Date)\n- utcOffset(Place,number)\n- influencedBy(Scientist,Scientist)\n- foundingYear(Organisation,Year)\n- officialLanguage(Country,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Extract ALL clearly stated factual triples in the text.\n    - If no relation cue appears, return an empty list of triples.\n    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n    - Do not invent entities that are not mentioned in the text.\n    - Output MUST be valid JSON and nothing else.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"citizenship\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"who works in the field of\",\n      \"support\": \"Darinka Dentcheva who works in the field of\",\n      \"notes\": \"implicit domain (Scientist) and range (Country)\"\n    },\n    {\n      \"triple\": [\"Darinka Dentcheva\", \"professionalField\", \"Mathematical optimization\"],\n      \"confidence\": 1.0,\n      \"cue\": \"works in the field of\",\n      \"support\": \"works in the field of Mathematical optimization\",\n      \"notes\": \"implicit domain (Scientist) and range (Field)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Darinka Dentcheva", "citizenship", "United States"], "confidence": 1.0, "cue": "who works in the field of", "support": "Darinka Dentcheva who works in the field of", "notes": "implicit domain (Scientist) and range (Country)"}, {"triple": ["Darinka Dentcheva", "professionalField", "Mathematical optimization"], "confidence": 1.0, "cue": "works in the field of", "support": "works in the field of Mathematical optimization", "notes": "implicit domain (Scientist) and range (Field)"}]}}}
