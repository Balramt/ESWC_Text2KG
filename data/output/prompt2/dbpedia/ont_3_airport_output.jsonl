{"id": "ont_3_airport_test_1", "input text": "Abilene Regional Airport serves the city of Abilene in Jones County, Texas, United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Abilene Regional Airport serves the city of Abilene in Jones County, Texas, United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"capital\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Abilene\",\n      \"support\": \"serves the city of Abilene in Jones County, Texas, United States.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"Abilene in Jones County, Texas, United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Abilene\",\n      \"support\": \"serves the city of Abilene in Jones County, Texas, United States.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"ceremonialCounty\", \"Jones County\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in Jones County\",\n      \"support\": \"in Jones County, Texas, United States.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"United States\",\n      \"support\": \"United States.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "capital", "Abilene"], "confidence": 1.0, "cue": "serves the city of Abilene", "support": "serves the city of Abilene in Jones County, Texas, United States."}, {"triple": ["Abilene Regional Airport", "location", "Abilene in Jones County, Texas, United States"], "confidence": 1.0, "cue": "serves the city of Abilene", "support": "serves the city of Abilene in Jones County, Texas, United States."}, {"triple": ["Abilene Regional Airport", "ceremonialCounty", "Jones County"], "confidence": 1.0, "cue": "in Jones County", "support": "in Jones County, Texas, United States."}, {"triple": ["Abilene Regional Airport", "country", "United States"], "confidence": 1.0, "cue": "United States", "support": "United States."}]}}}
{"id": "ont_3_airport_test_2", "input text": "Abilene Regional Airport serves the city of Abilene which is in Texas, in the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Abilene Regional Airport serves the city of Abilene which is in Texas, in the United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"capital\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is in\",\n      \"support\": \"which is in Texas, in the United States.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and City respectively).\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"Texas\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in Texas\",\n      \"support\": \"which is in Texas, in the United States.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in the United States\",\n      \"support\": \"which is in Texas, in the United States.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "capital", "Abilene"], "confidence": 1.0, "cue": "which is in", "support": "which is in Texas, in the United States.", "notes": "The subject and object are correctly typed according to the ontology (Airport and City respectively)."}, {"triple": ["Abilene Regional Airport", "location", "Texas"], "confidence": 1.0, "cue": "in Texas", "support": "which is in Texas, in the United States.", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}, {"triple": ["Abilene Regional Airport", "location", "United States"], "confidence": 1.0, "cue": "in the United States", "support": "which is in Texas, in the United States.", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}]}}}
{"id": "ont_3_airport_test_3", "input text": "Serving the city of Abilene in Texas, Abilene Regional Airport is 546 metres above sea level and has a runway length of 1,121 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Serving the city of Abilene in Texas, Abilene Regional Airport is 546 metres above sea level and has a runway length of 1,121 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"capital\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Serving the city of Abilene\",\n      \"support\": \"Serving the city of Abilene in Texas\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Serving the city of Abilene\",\n      \"support\": \"Serving the city of Abilene in Texas\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"regionServed\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Serving the city of Abilene\",\n      \"support\": \"Serving the city of Abilene in Texas\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", 546],\n      \"confidence\": 1.0,\n      \"cue\": \"is 546 metres above sea level\",\n      \"support\": \"Abilene Regional Airport is 546 metres above sea level\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"runwayLength\", 1121],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 1,121 metres\",\n      \"support\": \"has a runway length of 1,121 metres\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "capital", "Abilene"], "confidence": 1.0, "cue": "Serving the city of Abilene", "support": "Serving the city of Abilene in Texas"}, {"triple": ["Abilene Regional Airport", "location", "Abilene"], "confidence": 1.0, "cue": "Serving the city of Abilene", "support": "Serving the city of Abilene in Texas"}, {"triple": ["Abilene Regional Airport", "regionServed", "Abilene"], "confidence": 1.0, "cue": "Serving the city of Abilene", "support": "Serving the city of Abilene in Texas"}, {"triple": ["Abilene Regional Airport", "elevationAboveTheSeaLevelInMetres", 546], "confidence": 1.0, "cue": "is 546 metres above sea level", "support": "Abilene Regional Airport is 546 metres above sea level"}, {"triple": ["Abilene Regional Airport", "runwayLength", 1121], "confidence": 1.0, "cue": "has a runway length of 1,121 metres", "support": "has a runway length of 1,121 metres"}]}}}
{"id": "ont_3_airport_test_4", "input text": "Abilene Regional Airport serves the city of Abilene in Texas. Its runway is 2194 feet and is 546 m above sea level.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Abilene Regional Airport serves the city of Abilene in Texas. Its runway is 2194 feet and is 546 m above sea level.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"capital\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Abilene\",\n      \"support\": \"Abilene Regional Airport serves the city of Abilene in Texas.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"Abilene in Texas\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Abilene in Texas\",\n      \"support\": \"Abilene Regional Airport serves the city of Abilene in Texas.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", \"546\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 546 m above sea level\",\n      \"support\": \"Its runway is 2194 feet and is 546 m above sea level.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "capital", "Abilene"], "confidence": 1.0, "cue": "serves the city of Abilene", "support": "Abilene Regional Airport serves the city of Abilene in Texas."}, {"triple": ["Abilene Regional Airport", "location", "Abilene in Texas"], "confidence": 1.0, "cue": "serves the city of Abilene in Texas", "support": "Abilene Regional Airport serves the city of Abilene in Texas."}, {"triple": ["Abilene Regional Airport", "elevationAboveTheSeaLevelInMetres", "546"], "confidence": 1.0, "cue": "is 546 m above sea level", "support": "Its runway is 2194 feet and is 546 m above sea level."}]}}}
{"id": "ont_3_airport_test_5", "input text": "Abilene Regional Airport, located 546 metres above sea level, serves the city of Abilene, Texas with a runway length of 2,195.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Abilene Regional Airport, located 546 metres above sea level, serves the city of Abilene, Texas with a runway length of 2,195.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", \"546\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located 546 metres above sea level\",\n      \"support\": \"located 546 metres above sea level\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"regionServed\", \"Abilene, Texas\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Abilene, Texas\",\n      \"support\": \"serves the city of Abilene, Texas\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"runwayLength\", \"2,195\"],\n      \"confidence\": 1.0,\n      \"cue\": \"with a runway length of 2,195\",\n      \"support\": \"with a runway length of 2,195\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "elevationAboveTheSeaLevelInMetres", "546"], "confidence": 1.0, "cue": "located 546 metres above sea level", "support": "located 546 metres above sea level", "notes": "domain/range check"}, {"triple": ["Abilene Regional Airport", "regionServed", "Abilene, Texas"], "confidence": 1.0, "cue": "serves the city of Abilene, Texas", "support": "serves the city of Abilene, Texas", "notes": "domain/range check"}, {"triple": ["Abilene Regional Airport", "runwayLength", "2,195"], "confidence": 1.0, "cue": "with a runway length of 2,195", "support": "with a runway length of 2,195", "notes": "domain/range check"}]}}}
{"id": "ont_3_airport_test_6", "input text": "Lake Placid, N.Y. is served by the Adirondack Regional Airport. This airport is 507 metres above sea level and serves the city of Saranac Lake, New York.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Lake Placid, N.Y. is served by the Adirondack Regional Airport. This airport is 507 metres above sea level and serves the city of Saranac Lake, New York.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", \"507\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 507 metres above sea level\",\n      \"support\": \"Adirondack Regional Airport is 507 metres above sea level\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"regionServed\", \"Saranac Lake, New York\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Saranac Lake, New York\",\n      \"support\": \"This airport is 507 metres above sea level and serves the city of Saranac Lake, New York\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Adirondack Regional Airport", "elevationAboveTheSeaLevelInMetres", "507"], "confidence": 1.0, "cue": "is 507 metres above sea level", "support": "Adirondack Regional Airport is 507 metres above sea level"}, {"triple": ["Adirondack Regional Airport", "regionServed", "Saranac Lake, New York"], "confidence": 1.0, "cue": "serves the city of Saranac Lake, New York", "support": "This airport is 507 metres above sea level and serves the city of Saranac Lake, New York"}]}}}
{"id": "ont_3_airport_test_7", "input text": "Curitiba is part of the State of Parana, in the south region of Brazil. The Afonso Pena International airport serves the city.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Curitiba is part of the State of Parana, in the south region of Brazil. The Afonso Pena International airport serves the city.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Curitiba\", \"capital\", \"City\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Curitiba is part of the State of Parana, in the south region of Brazil.\",\n      \"support\": \"Curitiba is part of the State of Parana, in the south region of Brazil.\",\n      \"notes\": \"The cue is the prepositional phrase 'capital' which is a natural-language realization of the ontology relation 'capital'.\"\n    },\n    {\n      \"triple\": [\"Curitiba\", \"location\", \"Place\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Curitiba is part of the State of Parana, in the south region of Brazil.\",\n      \"support\": \"Curitiba is part of the State of Parana, in the south region of Brazil.\",\n      \"notes\": \"The cue is the prepositional phrase 'in the south region of Brazil' which is a natural-language realization of the ontology relation 'location'.\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"serves\", \"City\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The Afonso Pena International airport serves the city.\",\n      \"support\": \"The Afonso Pena International airport serves the city.\",\n      \"notes\": \"The cue is the verb phrase'serves' which is a natural-language realization of the ontology relation'serves'.\"\n    ]\n}", "json": null}}
{"id": "ont_3_airport_test_8", "input text": "Afonso Pena International Airport serves Curitiba, which is part of the State of Parana and led by the Democratic Labour Party of Brazil.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Afonso Pena International Airport serves Curitiba, which is part of the State of Parana and led by the Democratic Labour Party of Brazil.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"headed by\", \"Democratic Labour Party of Brazil\"],\n      \"confidence\": 1.0,\n      \"cue\": \"led by the Democratic Labour Party of Brazil\",\n      \"support\": \"led by the Democratic Labour Party of Brazil\",\n      \"notes\": \"domain: Airport, range: Party\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"capital\", \"Curitiba\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves Curitiba\",\n      \"support\": \"serves Curitiba\",\n      \"notes\": \"domain: Airport, range: City\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"regionServed\", \"State of Parana\"],\n      \"confidence\": 1.0,\n      \"cue\": \"part of the State of Parana\",\n      \"support\": \"part of the State of Parana\",\n      \"notes\": \"domain: Airport, range: County\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International Airport", "headed by", "Democratic Labour Party of Brazil"], "confidence": 1.0, "cue": "led by the Democratic Labour Party of Brazil", "support": "led by the Democratic Labour Party of Brazil", "notes": "domain: Airport, range: Party"}, {"triple": ["Afonso Pena International Airport", "capital", "Curitiba"], "confidence": 1.0, "cue": "serves Curitiba", "support": "serves Curitiba", "notes": "domain: Airport, range: City"}, {"triple": ["Afonso Pena International Airport", "regionServed", "State of Parana"], "confidence": 1.0, "cue": "part of the State of Parana", "support": "part of the State of Parana", "notes": "domain: Airport, range: County"}]}}}
{"id": "ont_3_airport_test_9", "input text": "Curitiba is part of the South Region, Brazil, is led by the Democratic Labour Party (Brazil) and is served by Afonso Pena International airport.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Curitiba is part of the South Region, Brazil, is led by the Democratic Labour Party (Brazil) and is served by Afonso Pena International airport.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Curitiba\", \"regionServed\", \"South Region, Brazil\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is part of\",\n      \"support\": \"Curitiba is part of the South Region, Brazil\"\n    },\n    {\n      \"triple\": [\"Curitiba\", \"headquarter\", \"Democratic Labour Party (Brazil)\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is led by\",\n      \"support\": \"Curitiba is led by the Democratic Labour Party (Brazil)\"\n    },\n    {\n      \"triple\": [\"Curitiba\", \"cityServed\", \"Curitiba\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is served by\",\n      \"support\": \"Curitiba is served by Afonso Pena International airport\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Curitiba", "regionServed", "South Region, Brazil"], "confidence": 1.0, "cue": "is part of", "support": "Curitiba is part of the South Region, Brazil"}, {"triple": ["Curitiba", "headquarter", "Democratic Labour Party (Brazil)"], "confidence": 1.0, "cue": "is led by", "support": "Curitiba is led by the Democratic Labour Party (Brazil)"}, {"triple": ["Curitiba", "cityServed", "Curitiba"], "confidence": 1.0, "cue": "is served by", "support": "Curitiba is served by Afonso Pena International airport"}]}}}
{"id": "ont_3_airport_test_10", "input text": "Afonso Pena International airport is located in Sao Jose dos Pinhais and is operated by Infraero. It is 911 metres above sea level.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Afonso Pena International airport is located in Sao Jose dos Pinhais and is operated by Infraero. It is 911 metres above sea level.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"elevationAboveTheSeaLevelInMetres\", \"911\"],\n      \"confidence\": 1.0,\n      \"cue\": \"It is 911 metres above sea level.\",\n      \"support\": \"It is 911 metres above sea level.\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"location\", \"Sao Jose dos Pinhais\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in Sao Jose dos Pinhais\",\n      \"support\": \"located in Sao Jose dos Pinhais\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"operatingOrganisation\", \"Infraero\"],\n      \"confidence\": 1.0,\n      \"cue\": \"operated by Infraero\",\n      \"support\": \"operated by Infraero\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International airport", "elevationAboveTheSeaLevelInMetres", "911"], "confidence": 1.0, "cue": "It is 911 metres above sea level.", "support": "It is 911 metres above sea level."}, {"triple": ["Afonso Pena International airport", "location", "Sao Jose dos Pinhais"], "confidence": 1.0, "cue": "located in Sao Jose dos Pinhais", "support": "located in Sao Jose dos Pinhais"}, {"triple": ["Afonso Pena International airport", "operatingOrganisation", "Infraero"], "confidence": 1.0, "cue": "operated by Infraero", "support": "operated by Infraero"}]}}}
{"id": "ont_3_airport_test_11", "input text": "Located at Sao Jose dos Pinhais, Afonso Pena International Airport has the runway name 11/29, and a runway length of 1800.0 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Located at Sao Jose dos Pinhais, Afonso Pena International Airport has the runway name 11/29, and a runway length of 1800.0 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"location\", \"Sao Jose dos Pinhais\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Located at Sao Jose dos Pinhais\",\n      \"support\": \"Located at Sao Jose dos Pinhais\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"runwayLength\", \"1800.0 metres\"],\n      \"confidence\": 1.0,\n      \"cue\": \"a runway length of 1800.0 metres\",\n      \"support\": \"a runway length of 1800.0 metres\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International Airport", "location", "Sao Jose dos Pinhais"], "confidence": 1.0, "cue": "Located at Sao Jose dos Pinhais", "support": "Located at Sao Jose dos Pinhais"}, {"triple": ["Afonso Pena International Airport", "runwayLength", "1800.0 metres"], "confidence": 1.0, "cue": "a runway length of 1800.0 metres", "support": "a runway length of 1800.0 metres"}]}}}
{"id": "ont_3_airport_test_12", "input text": "Afonso Pena International Airport is located in Sao Jose dos Pinhais; its runway name is 15/33 and it is 1800 metres long.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Afonso Pena International Airport is located in Sao Jose dos Pinhais; its runway name is 15/33 and it is 1800 metres long.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"location\", \"Sao Jose dos Pinhais\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Afonso Pena International Airport is located in Sao Jose dos Pinhais\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"runwayLength\", \"1800 metres\"],\n      \"confidence\": 1.0,\n      \"cue\": \"it is 1800 metres long\",\n      \"support\": \"it is 1800 metres long\",\n      \"notes\": \"domain: Airport, range: number\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International Airport", "location", "Sao Jose dos Pinhais"], "confidence": 1.0, "cue": "located in", "support": "Afonso Pena International Airport is located in Sao Jose dos Pinhais", "notes": "domain: Airport, range: Place"}, {"triple": ["Afonso Pena International Airport", "runwayLength", "1800 metres"], "confidence": 1.0, "cue": "it is 1800 metres long", "support": "it is 1800 metres long", "notes": "domain: Airport, range: number"}]}}}
{"id": "ont_3_airport_test_13", "input text": "The 11/29 runway in Afonso Pena International airport in Sao Jose dos Pinhais has a length of 2215.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 11/29 runway in Afonso Pena International airport in Sao Jose dos Pinhais has a length of 2215.0.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"runwayLength\", \"2215.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a length of\",\n      \"support\": \"The 11/29 runway in Afonso Pena International airport in Sao Jose dos Pinhais has a length of 2215.0.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International airport", "runwayLength", "2215.0"], "confidence": 1.0, "cue": "has a length of", "support": "The 11/29 runway in Afonso Pena International airport in Sao Jose dos Pinhais has a length of 2215.0.", "notes": "The subject and object are correctly typed according to the ontology's domain and range."}]}}}
{"id": "ont_3_airport_test_14", "input text": "Afonso Pena International Airport is located in São José dos Pinhais and has a runway length of 2215.0 which is named 15/33.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Afonso Pena International Airport is located in São José dos Pinhais and has a runway length of 2215.0 which is named 15/33.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"location\", \"São José dos Pinhais\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Afonso Pena International Airport is located in São José dos Pinhais\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"runwayLength\", \"2215.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"has a runway length of 2215.0\",\n      \"notes\": \"domain: Airport, range: number\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International Airport\", \"runwayName\", \"15/33\"],\n      \"confidence\": 1.0,\n      \"cue\": \"named\",\n      \"support\": \"which is named 15/33\",\n      \"notes\": \"domain: Airport, range: string\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International Airport", "location", "São José dos Pinhais"], "confidence": 1.0, "cue": "located in", "support": "Afonso Pena International Airport is located in São José dos Pinhais", "notes": "domain: Airport, range: Place"}, {"triple": ["Afonso Pena International Airport", "runwayLength", "2215.0"], "confidence": 1.0, "cue": "has a runway length of", "support": "has a runway length of 2215.0", "notes": "domain: Airport, range: number"}, {"triple": ["Afonso Pena International Airport", "runwayName", "15/33"], "confidence": 1.0, "cue": "named", "support": "which is named 15/33", "notes": "domain: Airport, range: string"}]}}}
{"id": "ont_3_airport_test_15", "input text": "Agra Airport is in Uttar Pradesh, Awadh, and is located at 167.94 metres above sea level.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra Airport is in Uttar Pradesh, Awadh, and is located at 167.94 metres above sea level.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"elevationAboveTheSeaLevelInMetres\", \"167.94\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is located at 167.94 metres above sea level.\",\n      \"support\": \"is located at 167.94 metres above sea level.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "elevationAboveTheSeaLevelInMetres", "167.94"], "confidence": 1.0, "cue": "is located at 167.94 metres above sea level.", "support": "is located at 167.94 metres above sea level."}]}}}
{"id": "ont_3_airport_test_16", "input text": "Agra Airport with an elevation of 167.94 meters above sea level is in Uttar Pradesh which is part of Bundelkhand.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra Airport with an elevation of 167.94 meters above sea level is in Uttar Pradesh which is part of Bundelkhand.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"elevationAboveTheSeaLevelInMetres\", \"167.94\"],\n      \"confidence\": 1.0,\n      \"cue\": \"with an elevation of\",\n      \"support\": \"Agra Airport with an elevation of 167.94 meters above sea level\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"location\", \"Uttar Pradesh\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is in\",\n      \"support\": \"Uttar Pradesh which is part of Bundelkhand\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "elevationAboveTheSeaLevelInMetres", "167.94"], "confidence": 1.0, "cue": "with an elevation of", "support": "Agra Airport with an elevation of 167.94 meters above sea level", "notes": "domain/range check"}, {"triple": ["Agra Airport", "location", "Uttar Pradesh"], "confidence": 1.0, "cue": "which is in", "support": "Uttar Pradesh which is part of Bundelkhand", "notes": "domain/range check"}]}}}
{"id": "ont_3_airport_test_17", "input text": "The Airports Authority of India is the operating organisation of Agra Airport which is located in India and has the location identifier VIAG.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Airports Authority of India is the operating organisation of Agra Airport which is located in India and has the location identifier VIAG.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The Airports Authority of India\", \"operatingOrganisation\", \"Agra Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the operating organisation of\",\n      \"support\": \"The Airports Authority of India is the operating organisation of Agra Airport\",\n      \"notes\": \"domain/range check: Airport (subject) and Organisation (object) match the domain→range of operatingOrganisation\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"location\", \"India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Agra Airport which is located in India\",\n      \"notes\": \"domain/range check: Airport (subject) and Country (object) match the domain→range of location\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"icaoLocationIdentifier\", \"VIAG\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has the location identifier\",\n      \"support\": \"Agra Airport has the location identifier VIAG\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The Airports Authority of India", "operatingOrganisation", "Agra Airport"], "confidence": 1.0, "cue": "is the operating organisation of", "support": "The Airports Authority of India is the operating organisation of Agra Airport", "notes": "domain/range check: Airport (subject) and Organisation (object) match the domain→range of operatingOrganisation"}, {"triple": ["Agra Airport", "location", "India"], "confidence": 1.0, "cue": "located in", "support": "Agra Airport which is located in India", "notes": "domain/range check: Airport (subject) and Country (object) match the domain→range of location"}, {"triple": ["Agra Airport", "icaoLocationIdentifier", "VIAG"], "confidence": 1.0, "cue": "has the location identifier", "support": "Agra Airport has the location identifier VIAG"}]}}}
{"id": "ont_3_airport_test_18", "input text": "Agra airport is located in India, where two of the leaders are T. S. Thakur and Narendra Modi.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra airport is located in India, where two of the leaders are T. S. Thakur and Narendra Modi.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra airport\", \"capital\", \"India\"],\n      \"confidence\": 0.8,\n      \"cue\": \"located in\",\n      \"support\": \"Agra airport is located in India\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Country respectively).\"\n    },\n    {\n      \"triple\": [\"Agra airport\", \"leader\", \"Narendra Modi\"],\n      \"confidence\": 0.8,\n      \"cue\": \"where two of the leaders are\",\n      \"support\": \"where two of the leaders are T. S. Thakur and Narendra Modi\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Person respectively).\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra airport", "capital", "India"], "confidence": 0.8, "cue": "located in", "support": "Agra airport is located in India", "notes": "The subject and object are correctly typed according to the ontology (Airport and Country respectively)."}, {"triple": ["Agra airport", "leader", "Narendra Modi"], "confidence": 0.8, "cue": "where two of the leaders are", "support": "where two of the leaders are T. S. Thakur and Narendra Modi", "notes": "The subject and object are correctly typed according to the ontology (Airport and Person respectively)."}]}}}
{"id": "ont_3_airport_test_19", "input text": "The ICAO Location Identifier of Agra Airport in Uttar Pradesh is VIAG. The airport is operated by the AIrports Authority of India.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The ICAO Location Identifier of Agra Airport in Uttar Pradesh is VIAG. The airport is operated by the AIrports Authority of India.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"icaoLocationIdentifier\", \"VIAG\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The ICAO Location Identifier of Agra Airport is VIAG\",\n      \"support\": \"The ICAO Location Identifier of Agra Airport in Uttar Pradesh is VIAG.\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"operatingOrganisation\", \"Airports Authority of India\"],\n      \"confidence\": 1.0,\n      \"cue\": \"operated by the Airports Authority of India\",\n      \"support\": \"The airport is operated by the Airports Authority of India.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "icaoLocationIdentifier", "VIAG"], "confidence": 1.0, "cue": "The ICAO Location Identifier of Agra Airport is VIAG", "support": "The ICAO Location Identifier of Agra Airport in Uttar Pradesh is VIAG."}, {"triple": ["Agra Airport", "operatingOrganisation", "Airports Authority of India"], "confidence": 1.0, "cue": "operated by the Airports Authority of India", "support": "The airport is operated by the Airports Authority of India."}]}}}
{"id": "ont_3_airport_test_20", "input text": "Agra Airport is located in Uttar Pradesh which is part of Awadh and Bundelkhand.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra Airport is located in Uttar Pradesh which is part of Awadh and Bundelkhand.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"location\", \"Uttar Pradesh\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Agra Airport is located in Uttar Pradesh\",\n      \"notes\": \"domain: Airport, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "location", "Uttar Pradesh"], "confidence": 1.0, "cue": "located in", "support": "Agra Airport is located in Uttar Pradesh", "notes": "domain: Airport, range: Place"}]}}}
{"id": "ont_3_airport_test_21", "input text": "Awadh is part of Uttar Pradesh which is where Agra Airport is and where Ram Naik is the leader.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Awadh is part of Uttar Pradesh which is where Agra Airport is and where Ram Naik is the leader.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Awadh\", \"isPartOf\", \"Uttar Pradesh\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is part of\",\n      \"support\": \"Awadh is part of Uttar Pradesh\",\n      \"notes\": \"domain: Place, range: County\"\n    },\n    {\n      \"triple\": [\"Uttar Pradesh\", \"capital\", \"Agra Airport is\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Agra Airport is\",\n      \"support\": \"Uttar Pradesh which is where Agra Airport is\",\n      \"notes\": \"domain: County, range: City (indirect, inferred from Airport)\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"leader\", \"Ram Naik is the leader.\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Ram Naik is the leader.\",\n      \"support\": \"and where Ram Naik is the leader.\",\n      \"notes\": \"domain: Airport, range: Person\"\n    ]\n  }", "json": null}}
{"id": "ont_3_airport_test_22", "input text": "Agra Airport is in Uttar Pradesh, Bundelkhand, whose leader is Ram Naik.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra Airport is in Uttar Pradesh, Bundelkhand, whose leader is Ram Naik.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"capital\", \"Uttar Pradesh\"],\n      \"confidence\": 0.8,\n      \"cue\": \"is in Uttar Pradesh\",\n      \"support\": \"Agra Airport is in Uttar Pradesh\",\n      \"notes\": \"The relation 'capital' is inferred from the ontology, as 'Uttar Pradesh' is the political entity that 'Agra Airport' is located in.\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"headquarter\", \"Bundelkhand\"],\n      \"confidence\": 0.8,\n      \"cue\": \"whose leader is Ram Naik\",\n      \"support\": \"whose leader is Ram Naik\",\n      \"notes\": \"The relation 'headquarter' is inferred from the ontology, as 'Bundelkhand' is the entity that has a leader, and the text suggests a connection between 'Bundelkhand' and 'Agra Airport'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "capital", "Uttar Pradesh"], "confidence": 0.8, "cue": "is in Uttar Pradesh", "support": "Agra Airport is in Uttar Pradesh", "notes": "The relation 'capital' is inferred from the ontology, as 'Uttar Pradesh' is the political entity that 'Agra Airport' is located in."}, {"triple": ["Agra Airport", "headquarter", "Bundelkhand"], "confidence": 0.8, "cue": "whose leader is Ram Naik", "support": "whose leader is Ram Naik", "notes": "The relation 'headquarter' is inferred from the ontology, as 'Bundelkhand' is the entity that has a leader, and the text suggests a connection between 'Bundelkhand' and 'Agra Airport'."}]}}}
{"id": "ont_3_airport_test_23", "input text": "The ICAO Location Identifier of Agra Airport is VIAG and the airport in Agra is operated by the Indian Air Force.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The ICAO Location Identifier of Agra Airport is VIAG and the airport in Agra is operated by the Indian Air Force.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"icaoLocationIdentifier\", \"VIAG\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The ICAO Location Identifier of Agra Airport is\",\n      \"support\": \"The ICAO Location Identifier of Agra Airport is VIAG\",\n      \"notes\": \"The subject and object are both Airport type\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"operatingOrganisation\", \"Indian Air Force\"],\n      \"confidence\": 1.0,\n      \"cue\": \"operated by the\",\n      \"support\": \"operated by the Indian Air Force\",\n      \"notes\": \"The subject and object are both Airport type\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "icaoLocationIdentifier", "VIAG"], "confidence": 1.0, "cue": "The ICAO Location Identifier of Agra Airport is", "support": "The ICAO Location Identifier of Agra Airport is VIAG", "notes": "The subject and object are both Airport type"}, {"triple": ["Agra Airport", "operatingOrganisation", "Indian Air Force"], "confidence": 1.0, "cue": "operated by the", "support": "operated by the Indian Air Force", "notes": "The subject and object are both Airport type"}]}}}
{"id": "ont_3_airport_test_24", "input text": "Alderney is served by Alderney Airport. This airport's 1st runway is made from Poaceae and it has a runway length of 497.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Alderney is served by Alderney Airport. This airport's 1st runway is made from Poaceae and it has a runway length of 497.0.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alderney\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"this airport's 1st runway is made from Poaceae\",\n      \"support\": \"this airport's 1st runway is made from Poaceae\",\n      \"notes\": \"The subject 'Alderney' is an Airport, and the object 'Poaceae' matches the RunwaySurfaceType domain.\"\n    },\n    {\n      \"triple\": [\"Alderney Airport\", \"runwayLength\", \"497.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"it has a runway length of 497.0\",\n      \"support\": \"it has a runway length of 497.0\",\n      \"notes\": \"The subject 'Alderney Airport' is an Airport, and the object '497.0' matches the number range for runwayLength.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alderney", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "this airport's 1st runway is made from Poaceae", "support": "this airport's 1st runway is made from Poaceae", "notes": "The subject 'Alderney' is an Airport, and the object 'Poaceae' matches the RunwaySurfaceType domain."}, {"triple": ["Alderney Airport", "runwayLength", "497.0"], "confidence": 1.0, "cue": "it has a runway length of 497.0", "support": "it has a runway length of 497.0", "notes": "The subject 'Alderney Airport' is an Airport, and the object '497.0' matches the number range for runwayLength."}]}}}
{"id": "ont_3_airport_test_25", "input text": "The 1st runway at Alderney Airport is made from Poaceae which belongs to the order of Commelinids and is in the class Monocotyledon.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Poaceae which belongs to the order of Commelinids and is in the class Monocotyledon.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The 1st runway at Alderney Airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"made from\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Poaceae\",\n      \"notes\": \"domain: RunwaySurfaceType, range: Poaceae\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The 1st runway at Alderney Airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "made from", "support": "The 1st runway at Alderney Airport is made from Poaceae", "notes": "domain: RunwaySurfaceType, range: Poaceae"}]}}}
{"id": "ont_3_airport_test_26", "input text": "The 1st runway at Alderney Airport is made from Poaceae, a Monocotyledon classed member of the order of Poales.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Poaceae, a Monocotyledon classed member of the order of Poales.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The 1st runway at Alderney Airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"made from\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Poaceae\",\n      \"notes\": \"domain: 1stRunwaySurfaceType (implicit from relation); range: RunwaySurfaceType\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The 1st runway at Alderney Airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "made from", "support": "The 1st runway at Alderney Airport is made from Poaceae", "notes": "domain: 1stRunwaySurfaceType (implicit from relation); range: RunwaySurfaceType"}]}}}
{"id": "ont_3_airport_test_27", "input text": "The surface of the 1st runway at Alderney airport is made from Poaceae of the orders Poales and Commelinids.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The surface of the 1st runway at Alderney airport is made from Poaceae of the orders Poales and Commelinids.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The 1st runway\", \"runwaySurfaceType\", \"Poaceae of the orders Poales and Commelinids\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made from\",\n      \"support\": \"The surface of the 1st runway at Alderney airport is made from Poaceae of the orders Poales and Commelinids.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range for the 'runwaySurfaceType' relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The 1st runway", "runwaySurfaceType", "Poaceae of the orders Poales and Commelinids"], "confidence": 1.0, "cue": "is made from", "support": "The surface of the 1st runway at Alderney airport is made from Poaceae of the orders Poales and Commelinids.", "notes": "The subject and object are correctly typed according to the ontology's domain and range for the 'runwaySurfaceType' relation."}]}}}
{"id": "ont_3_airport_test_28", "input text": "The city of Alderney is served by Alderney Airport. This airport has a runway length of 497.0 metres and its 1st runway has an asphalt surface.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The city of Alderney is served by Alderney Airport. This airport has a runway length of 497.0 metres and its 1st runway has an asphalt surface.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alderney Airport\", \"cityServed\", \"Alderney\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The city of Alderney is served by Alderney Airport\",\n      \"support\": \"The city of Alderney is served by Alderney Airport\"\n    },\n    {\n      \"triple\": [\"Alderney Airport\", \"runwayLength\", \"497.0 metres\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 497.0 metres\",\n      \"support\": \"has a runway length of 497.0 metres\"\n    },\n    {\n      \"triple\": [\"Alderney Airport\", \"1stRunwaySurfaceType\", \"asphalt\"],\n      \"confidence\": 1.0,\n      \"cue\": \"its 1st runway has an asphalt surface\",\n      \"support\": \"its 1st runway has an asphalt surface\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alderney Airport", "cityServed", "Alderney"], "confidence": 1.0, "cue": "The city of Alderney is served by Alderney Airport", "support": "The city of Alderney is served by Alderney Airport"}, {"triple": ["Alderney Airport", "runwayLength", "497.0 metres"], "confidence": 1.0, "cue": "has a runway length of 497.0 metres", "support": "has a runway length of 497.0 metres"}, {"triple": ["Alderney Airport", "1stRunwaySurfaceType", "asphalt"], "confidence": 1.0, "cue": "its 1st runway has an asphalt surface", "support": "its 1st runway has an asphalt surface"}]}}}
{"id": "ont_3_airport_test_29", "input text": "Alderney is served by Alderney Airport which has a runway length of 733.0 metres. Additionally, its 1st runway has an asphalt surface.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Alderney is served by Alderney Airport which has a runway length of 733.0 metres. Additionally, its 1st runway has an asphalt surface.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alderney Airport\", \"runwayLength\", \"733.0 metres\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"Alderney Airport which has a runway length of 733.0 metres.\",\n      \"notes\": \"runwayLength(Airport, number)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alderney Airport", "runwayLength", "733.0 metres"], "confidence": 1.0, "cue": "has a runway length of", "support": "Alderney Airport which has a runway length of 733.0 metres.", "notes": "runwayLength(Airport, number)"}]}}}
{"id": "ont_3_airport_test_30", "input text": "The 1st runway at Alderney Airport is made from Asphalt and has a length of 877.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Asphalt and has a length of 877.0.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The 1st runway\", \"runwayLength\", \"877.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a length of\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Asphalt and has a length of 877.0.\",\n      \"notes\": \"runwayLength(Airport,number)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The 1st runway", "runwayLength", "877.0"], "confidence": 1.0, "cue": "has a length of", "support": "The 1st runway at Alderney Airport is made from Asphalt and has a length of 877.0.", "notes": "runwayLength(Airport,number)"}]}}}
{"id": "ont_3_airport_test_31", "input text": "Alpena County Regional Airport, located in Maple Ridge Township, Michigan has a runway length of 1533.0 and has an elevation of 210 meters.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Alpena County Regional Airport, located in Maple Ridge Township, Michigan has a runway length of 1533.0 and has an elevation of 210 meters.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Maple Ridge Township, Michigan\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"located in Maple Ridge Township, Michigan\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", \"210\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has an elevation of\",\n      \"support\": \"has an elevation of 210 meters\",\n      \"notes\": \"domain: Airport, range: number\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", \"1533.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"has a runway length of 1533.0\",\n      \"notes\": \"domain: Airport, range: number\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Maple Ridge Township, Michigan"], "confidence": 1.0, "cue": "located in", "support": "located in Maple Ridge Township, Michigan", "notes": "domain: Airport, range: Place"}, {"triple": ["Alpena County Regional Airport", "elevationAboveTheSeaLevelInMetres", "210"], "confidence": 1.0, "cue": "has an elevation of", "support": "has an elevation of 210 meters", "notes": "domain: Airport, range: number"}, {"triple": ["Alpena County Regional Airport", "runwayLength", "1533.0"], "confidence": 1.0, "cue": "has a runway length of", "support": "has a runway length of 1533.0", "notes": "domain: Airport, range: number"}]}}}
{"id": "ont_3_airport_test_32", "input text": "The runway length of Alpena County Regional Airport is 1,533 and it is located at Maple Ridge Township, Alpena County, Michigan, United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The runway length of Alpena County Regional Airport is 1,533 and it is located at Maple Ridge Township, Alpena County, Michigan, United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Maple Ridge Township, Alpena County, Michigan, United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"it is located at\",\n      \"support\": \"it is located at Maple Ridge Township, Alpena County, Michigan, United States\",\n      \"notes\": \"The subject and object are both places.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Maple Ridge Township, Alpena County, Michigan, United States"], "confidence": 1.0, "cue": "it is located at", "support": "it is located at Maple Ridge Township, Alpena County, Michigan, United States", "notes": "The subject and object are both places."}]}}}
{"id": "ont_3_airport_test_33", "input text": "Alpena County Regional Airport has a runway length is 2744.0 metres and is located in Maple Ridge Township, Alpena County, Michigan, in the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Alpena County Regional Airport has a runway length is 2744.0 metres and is located in Maple Ridge Township, Alpena County, Michigan, in the United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Maple Ridge Township, Alpena County, Michigan, in the United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Maple Ridge Township, Alpena County, Michigan, in the United States\",\n      \"notes\": \"domain/range check: Airport (subject) and Place (object)\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"ceremonialCounty\", \"Alpena County\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Alpena County\",\n      \"support\": \"Alpena County\",\n      \"notes\": \"domain/range check: Airport (subject) and County (object)\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in the United States\",\n      \"support\": \"in the United States\",\n      \"notes\": \"domain/range check: Airport (subject) and Country (object)\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", \"2744.0 metres\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length\",\n      \"support\": \"has a runway length is 2744.0 metres\",\n      \"notes\": \"domain/range check: Airport (subject) and number (object)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Maple Ridge Township, Alpena County, Michigan, in the United States"], "confidence": 1.0, "cue": "located in", "support": "Maple Ridge Township, Alpena County, Michigan, in the United States", "notes": "domain/range check: Airport (subject) and Place (object)"}, {"triple": ["Alpena County Regional Airport", "ceremonialCounty", "Alpena County"], "confidence": 1.0, "cue": "Alpena County", "support": "Alpena County", "notes": "domain/range check: Airport (subject) and County (object)"}, {"triple": ["Alpena County Regional Airport", "country", "United States"], "confidence": 1.0, "cue": "in the United States", "support": "in the United States", "notes": "domain/range check: Airport (subject) and Country (object)"}, {"triple": ["Alpena County Regional Airport", "runwayLength", "2744.0 metres"], "confidence": 1.0, "cue": "has a runway length", "support": "has a runway length is 2744.0 metres", "notes": "domain/range check: Airport (subject) and number (object)"}]}}}
{"id": "ont_3_airport_test_34", "input text": "Found in Wilson Township, Alpena County, Michigan, is Alpena County Regional Airport. This airport is 210 metres above sea level and has a runway length of 2744.0 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Found in Wilson Township, Alpena County, Michigan, is Alpena County Regional Airport. This airport is 210 metres above sea level and has a runway length of 2744.0 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", \"210\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 210 metres above sea level\",\n      \"support\": \"Alpena County Regional Airport is 210 metres above sea level\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", \"2744.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 2744.0 metres\",\n      \"support\": \"Alpena County Regional Airport has a runway length of 2744.0 metres\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "elevationAboveTheSeaLevelInMetres", "210"], "confidence": 1.0, "cue": "is 210 metres above sea level", "support": "Alpena County Regional Airport is 210 metres above sea level"}, {"triple": ["Alpena County Regional Airport", "runwayLength", "2744.0"], "confidence": 1.0, "cue": "has a runway length of 2744.0 metres", "support": "Alpena County Regional Airport has a runway length of 2744.0 metres"}]}}}
{"id": "ont_3_airport_test_35", "input text": "With an elevation of 210 metres above sea level, Alpena County Regional Airport is located in the Wilson Township, in Alpena County, Michigan, in the U.S.A.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"With an elevation of 210 metres above sea level, Alpena County Regional Airport is located in the Wilson Township, in Alpena County, Michigan, in the U.S.A.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Wilson Township\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Wilson Township, in Alpena County, Michigan, in the U.S.A.\",\n      \"notes\": \"domain/range check: Airport (subject) and Place (object) match the location relation\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"ceremonialCounty\", \"Alpena County\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in Alpena County\",\n      \"support\": \"Alpena County, Michigan, in the U.S.A.\",\n      \"notes\": \"domain/range check: Airport (subject) and County (object) match the ceremonialCounty relation\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"country\", \"U.S.A.\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in the U.S.A.\",\n      \"support\": \"in the U.S.A.\",\n      \"notes\": \"domain/range check: Airport (subject) and Country (object) match the country relation\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", 210],\n      \"confidence\": 1.0,\n      \"cue\": \"With an elevation of 210 metres above sea level\",\n      \"support\": \"With an elevation of 210 metres above sea level\",\n      \"notes\": \"domain/range check: Airport (subject) and number (object) match the elevationAboveTheSeaLevelInMetres relation\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Wilson Township"], "confidence": 1.0, "cue": "located in", "support": "Wilson Township, in Alpena County, Michigan, in the U.S.A.", "notes": "domain/range check: Airport (subject) and Place (object) match the location relation"}, {"triple": ["Alpena County Regional Airport", "ceremonialCounty", "Alpena County"], "confidence": 1.0, "cue": "in Alpena County", "support": "Alpena County, Michigan, in the U.S.A.", "notes": "domain/range check: Airport (subject) and County (object) match the ceremonialCounty relation"}, {"triple": ["Alpena County Regional Airport", "country", "U.S.A."], "confidence": 1.0, "cue": "in the U.S.A.", "support": "in the U.S.A.", "notes": "domain/range check: Airport (subject) and Country (object) match the country relation"}, {"triple": ["Alpena County Regional Airport", "elevationAboveTheSeaLevelInMetres", 210], "confidence": 1.0, "cue": "With an elevation of 210 metres above sea level", "support": "With an elevation of 210 metres above sea level", "notes": "domain/range check: Airport (subject) and number (object) match the elevationAboveTheSeaLevelInMetres relation"}]}}}
{"id": "ont_3_airport_test_36", "input text": "Alpena County Regional Airport is located in Wilson Township, Alpena County, Michigan in the United States. It has a runway length of 1533.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Alpena County Regional Airport is located in Wilson Township, Alpena County, Michigan in the United States. It has a runway length of 1533.0.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Wilson Township, Alpena County, Michigan in the United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Wilson Township, Alpena County, Michigan in the United States\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", \"1533.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"1533.0\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Wilson Township, Alpena County, Michigan in the United States"], "confidence": 1.0, "cue": "located in", "support": "Wilson Township, Alpena County, Michigan in the United States", "notes": "domain: Airport, range: Place"}, {"triple": ["Alpena County Regional Airport", "runwayLength", "1533.0"], "confidence": 1.0, "cue": "has a runway length of", "support": "1533.0"}]}}}
{"id": "ont_3_airport_test_37", "input text": "The location of Alpena County Regional Airport is Wilson Township, Alpena County, Michigan, USA, and has a runway length of 2744.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The location of Alpena County Regional Airport is Wilson Township, Alpena County, Michigan, USA, and has a runway length of 2744.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Wilson Township, Alpena County, Michigan, USA\"],\n      \"confidence\": 1.0,\n      \"cue\": \"The location of Alpena County Regional Airport is\",\n      \"support\": \"The location of Alpena County Regional Airport is Wilson Township, Alpena County, Michigan, USA\",\n      \"notes\": \"domain/range check: Airport (subject) and Place (object) match the location relation\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", \"2744\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"has a runway length of 2744\",\n      \"notes\": \"domain/range check: Airport (subject) and number (object) match the runwayLength relation\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "location", "Wilson Township, Alpena County, Michigan, USA"], "confidence": 1.0, "cue": "The location of Alpena County Regional Airport is", "support": "The location of Alpena County Regional Airport is Wilson Township, Alpena County, Michigan, USA", "notes": "domain/range check: Airport (subject) and Place (object) match the location relation"}, {"triple": ["Alpena County Regional Airport", "runwayLength", "2744"], "confidence": 1.0, "cue": "has a runway length of", "support": "has a runway length of 2744", "notes": "domain/range check: Airport (subject) and number (object) match the runwayLength relation"}]}}}
{"id": "ont_3_airport_test_38", "input text": "Located at Ícolo e Bengo, Angola International Airport is 159 metres above the sea level and has a runway length of 3,800 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Located at Ícolo e Bengo, Angola International Airport is 159 metres above the sea level and has a runway length of 3,800 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ícolo e Bengo\", \"location\", \"Angola International Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Located at\",\n      \"support\": \"Located at Ícolo e Bengo,\"\n    },\n    {\n      \"triple\": [\"Angola International Airport\", \"elevationAboveTheSeaLevelInMetres\", \"159\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is\",\n      \"support\": \" and has a runway length of 3,800 metres.\"\n    },\n    {\n      \"triple\": [\"Angola International Airport\", \"runwayLength\", \"3800\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \" and has a runway length of 3,800 metres.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ícolo e Bengo", "location", "Angola International Airport"], "confidence": 1.0, "cue": "Located at", "support": "Located at Ícolo e Bengo,"}, {"triple": ["Angola International Airport", "elevationAboveTheSeaLevelInMetres", "159"], "confidence": 1.0, "cue": "is", "support": " and has a runway length of 3,800 metres."}, {"triple": ["Angola International Airport", "runwayLength", "3800"], "confidence": 1.0, "cue": "has a runway length of", "support": " and has a runway length of 3,800 metres."}]}}}
{"id": "ont_3_airport_test_39", "input text": "Located in Ícolo e Bengo, Angola International Airport is 159 meters above sea level and has a runway length of 4000ft.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Located in Ícolo e Bengo, Angola International Airport is 159 meters above sea level and has a runway length of 4000ft.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"International Airport\", \"elevationAboveTheSeaLevelInMetres\", \"159\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 159 meters above sea level\",\n      \"support\": \"International Airport is 159 meters above sea level\"\n    },\n    {\n      \"triple\": [\"International Airport\", \"runwayLength\", \"4000ft\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 4000ft\",\n      \"support\": \"International Airport has a runway length of 4000ft\"\n    },\n    {\n      \"triple\": [\"Ícolo e Bengo\", \"location\", \"International Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Located in Ícolo e Bengo\",\n      \"support\": \"Located in Ícolo e Bengo, Angola International Airport\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["International Airport", "elevationAboveTheSeaLevelInMetres", "159"], "confidence": 1.0, "cue": "is 159 meters above sea level", "support": "International Airport is 159 meters above sea level"}, {"triple": ["International Airport", "runwayLength", "4000ft"], "confidence": 1.0, "cue": "has a runway length of 4000ft", "support": "International Airport has a runway length of 4000ft"}, {"triple": ["Ícolo e Bengo", "location", "International Airport"], "confidence": 1.0, "cue": "Located in Ícolo e Bengo", "support": "Located in Ícolo e Bengo, Angola International Airport"}]}}}
{"id": "ont_3_airport_test_40", "input text": "At 159 meters above sea level, Angola International airport is located in Icolo e Bengo, in Angola.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"At 159 meters above sea level, Angola International airport is located in Icolo e Bengo, in Angola.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International airport\", \"location\", \"Icolo e Bengo\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"At 159 meters above sea level, Angola International airport is located in Icolo e Bengo,\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International airport", "location", "Icolo e Bengo"], "confidence": 1.0, "cue": "located in", "support": "At 159 meters above sea level, Angola International airport is located in Icolo e Bengo,"}]}}}
{"id": "ont_3_airport_test_41", "input text": "With a runway length of 3800 metres, Angola International Airport is located at Ícolo e Bengo, in Angola.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"With a runway length of 3800 metres, Angola International Airport is located at Ícolo e Bengo, in Angola.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International Airport\", \"location\", \"Ícolo e Bengo, in Angola\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located at\",\n      \"support\": \"Ícolo e Bengo, in Angola\",\n      \"notes\": \"domain: Airport, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International Airport", "location", "Ícolo e Bengo, in Angola"], "confidence": 1.0, "cue": "located at", "support": "Ícolo e Bengo, in Angola", "notes": "domain: Airport, range: Place"}]}}}
{"id": "ont_3_airport_test_42", "input text": "05L/23R is the name of the runway at Angola International airport in Icolo e Bengo, Angola.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"05L/23R is the name of the runway at Angola International airport in Icolo e Bengo, Angola.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International airport\", \"runwayName\", \"05L/23R\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the name of\",\n      \"support\": \"Angola International airport is the name of the runway at Angola International airport in Icolo e Bengo, Angola.\",\n      \"notes\": \"The subject and object are both Airports, and the cue 'is the name of' expresses the 'runwayName' relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International airport", "runwayName", "05L/23R"], "confidence": 1.0, "cue": "is the name of", "support": "Angola International airport is the name of the runway at Angola International airport in Icolo e Bengo, Angola.", "notes": "The subject and object are both Airports, and the cue 'is the name of' expresses the 'runwayName' relation."}]}}}
{"id": "ont_3_airport_test_43", "input text": "Angola International Airport is located in Icolo e Bengo, Angolo, Luanda Province.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Angola International Airport is located in Icolo e Bengo, Angolo, Luanda Province.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International Airport\", \"location\", \"Icolo e Bengo, Angola, Luanda Province\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Icolo e Bengo, Angola, Luanda Province\",\n      \"notes\": \"domain: Airport, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International Airport", "location", "Icolo e Bengo, Angola, Luanda Province"], "confidence": 1.0, "cue": "located in", "support": "Icolo e Bengo, Angola, Luanda Province", "notes": "domain: Airport, range: Place"}]}}}
{"id": "ont_3_airport_test_44", "input text": "The city of Luanda is served by Angola International Airport. This airport is is 159 meters above sea level and has a runway length of 4000 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The city of Luanda is served by Angola International Airport. This airport is is 159 meters above sea level and has a runway length of 4000 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International Airport\", \"elevationAboveTheSeaLevelInMetres\", \"159\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 159 meters above sea level\",\n      \"support\": \"Angola International Airport is 159 meters above sea level\"\n    },\n    {\n      \"triple\": [\"Angola International Airport\", \"runwayLength\", \"4000\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 4000 metres\",\n      \"support\": \"Angola International Airport has a runway length of 4000 metres\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International Airport", "elevationAboveTheSeaLevelInMetres", "159"], "confidence": 1.0, "cue": "is 159 meters above sea level", "support": "Angola International Airport is 159 meters above sea level"}, {"triple": ["Angola International Airport", "runwayLength", "4000"], "confidence": 1.0, "cue": "has a runway length of 4000 metres", "support": "Angola International Airport has a runway length of 4000 metres"}]}}}
{"id": "ont_3_airport_test_45", "input text": "The 3rd runway at Ardmore Airport in New Zealand (which is 34 meters above sea level) is made of Poaceae and has a length of 518.0.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 3rd runway at Ardmore Airport in New Zealand (which is 34 meters above sea level) is made of Poaceae and has a length of 518.0.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport\", \"elevationAboveTheSeaLevelInMetres\", \"34\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 34 meters above sea level\",\n      \"support\": \"The 3rd runway at Ardmore Airport in New Zealand (which is 34 meters above sea level)\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Ardmore Airport\", \"runwayLength\", \"518.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a length of 518.0\",\n      \"support\": \"The 3rd runway at Ardmore Airport in New Zealand has a length of 518.0\",\n      \"notes\": \"domain/range check\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport", "elevationAboveTheSeaLevelInMetres", "34"], "confidence": 1.0, "cue": "is 34 meters above sea level", "support": "The 3rd runway at Ardmore Airport in New Zealand (which is 34 meters above sea level)", "notes": "domain/range check"}, {"triple": ["Ardmore Airport", "runwayLength", "518.0"], "confidence": 1.0, "cue": "has a length of 518.0", "support": "The 3rd runway at Ardmore Airport in New Zealand has a length of 518.0", "notes": "domain/range check"}]}}}
{"id": "ont_3_airport_test_46", "input text": "At the Ardmore airport in New Zealand, the 3rd runway is made from Poaceae which belongs to the flowering plants division and is in the class of Monocotyledon.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"At the Ardmore airport in New Zealand, the 3rd runway is made from Poaceae which belongs to the flowering plants division and is in the class of Monocotyledon.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"the 3rd runway is made from Poaceae\",\n      \"support\": \"Ardmore airport to Poaceae\",\n      \"notes\": \"runwaySurfaceType domain is RunwaySurfaceType, range is string but Poaceae is a type of grass, not a string, so the triple is valid\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "the 3rd runway is made from Poaceae", "support": "Ardmore airport to Poaceae", "notes": "runwaySurfaceType domain is RunwaySurfaceType, range is string but Poaceae is a type of grass, not a string, so the triple is valid"}]}}}
{"id": "ont_3_airport_test_47", "input text": "The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae which is class as monocotyledon and part of the commelinids order.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae which is class as monocotyledon and part of the commelinids order.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport (New Zealand)\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made of\",\n      \"support\": \"The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae\",\n      \"notes\": \"runwaySurfaceType(Airport, RunwaySurfaceType)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport (New Zealand)", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made of", "support": "The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae", "notes": "runwaySurfaceType(Airport, RunwaySurfaceType)"}]}}}
{"id": "ont_3_airport_test_48", "input text": "Ardmore Airport (New Zealand)'s 3rd runway surface type is Poaceae which is of the poales order and the class of Monocotyledon.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Ardmore Airport (New Zealand)'s 3rd runway surface type is Poaceae which is of the poales order and the class of Monocotyledon.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport (New Zealand)\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is of the poales order and the class of Monocotyledon.\",\n      \"support\": \"is of the poales order and the class of Monocotyledon.\",\n      \"notes\": \"The subject and object match the domain and range of the runwaySurfaceType relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport (New Zealand)", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is of the poales order and the class of Monocotyledon.", "support": "is of the poales order and the class of Monocotyledon.", "notes": "The subject and object match the domain and range of the runwaySurfaceType relation."}]}}}
{"id": "ont_3_airport_test_49", "input text": "In New Zealand, the 3rd runway at Ardmore Airport is made of Poaceae, a plant that belongs to the division of flowering plants and is part of the Commelinids order.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"In New Zealand, the 3rd runway at Ardmore Airport is made of Poaceae, a plant that belongs to the division of flowering plants and is part of the Commelinids order.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport\", \"3rdRunwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made of\",\n      \"support\": \"Ardmore Airport is made of Poaceae,\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport", "3rdRunwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made of", "support": "Ardmore Airport is made of Poaceae,"}]}}}
{"id": "ont_3_airport_test_50", "input text": "The Ardmore Airport in New Zealand, located 34 meters above sea level, has a runway length of 1411.0 and the third runway is made of Poaceae.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Ardmore Airport in New Zealand, located 34 meters above sea level, has a runway length of 1411.0 and the third runway is made of Poaceae.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The Ardmore Airport\", \"elevationAboveTheSeaLevelInMetres\", \"34\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located 34 meters above sea level\",\n      \"support\": \"The Ardmore Airport, located 34 meters above sea level,\"\n    },\n    {\n      \"triple\": [\"The Ardmore Airport\", \"runwayLength\", \"1411.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 1411.0\",\n      \"support\": \"has a runway length of 1411.0\"\n    },\n    {\n      \"triple\": [\"The third runway\", \"3rdRunwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"the third runway is made of Poaceae.\",\n      \"support\": \"the third runway is made of Poaceae.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The Ardmore Airport", "elevationAboveTheSeaLevelInMetres", "34"], "confidence": 1.0, "cue": "located 34 meters above sea level", "support": "The Ardmore Airport, located 34 meters above sea level,"}, {"triple": ["The Ardmore Airport", "runwayLength", "1411.0"], "confidence": 1.0, "cue": "has a runway length of 1411.0", "support": "has a runway length of 1411.0"}, {"triple": ["The third runway", "3rdRunwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "the third runway is made of Poaceae.", "support": "the third runway is made of Poaceae."}]}}}
{"id": "ont_3_airport_test_51", "input text": "New Zealand's Ardmore Airport; is 34.0 metres above sea level, has a runway length of 597.0 metres and its 3rd runway surface type is Poaceae.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"New Zealand's Ardmore Airport; is 34.0 metres above sea level, has a runway length of 597.0 metres and its 3rd runway surface type is Poaceae.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"New Zealand's Ardmore Airport\", \"elevationAboveTheSeaLevelInMetres\", \"34.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is 34.0 metres above sea level\",\n      \"support\": \"is 34.0 metres above sea level\"\n    },\n    {\n      \"triple\": [\"New Zealand's Ardmore Airport\", \"runwayLength\", \"597.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of 597.0 metres\",\n      \"support\": \"has a runway length of 597.0 metres\"\n    },\n    {\n      \"triple\": [\"New Zealand's Ardmore Airport\", \"3rdRunwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"its 3rd runway surface type is Poaceae\",\n      \"support\": \"its 3rd runway surface type is Poaceae\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["New Zealand's Ardmore Airport", "elevationAboveTheSeaLevelInMetres", "34.0"], "confidence": 1.0, "cue": "is 34.0 metres above sea level", "support": "is 34.0 metres above sea level"}, {"triple": ["New Zealand's Ardmore Airport", "runwayLength", "597.0"], "confidence": 1.0, "cue": "has a runway length of 597.0 metres", "support": "has a runway length of 597.0 metres"}, {"triple": ["New Zealand's Ardmore Airport", "3rdRunwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "its 3rd runway surface type is Poaceae", "support": "its 3rd runway surface type is Poaceae"}]}}}
{"id": "ont_3_airport_test_52", "input text": "Athens International Airport serves the city of Athens which is in Greece where Nikos Voutsis is the leader.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Athens International Airport serves the city of Athens which is in Greece where Nikos Voutsis is the leader.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Athens International Airport\", \"capital\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is in\",\n      \"support\": \"which is in Greece where Nikos Voutsis is the leader.\",\n      \"notes\": \"The subject and object are both cities, and the cue 'which is in' indicates the 'capital' relation.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"city\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is in\",\n      \"support\": \"which is in Greece where Nikos Voutsis is the leader.\",\n      \"notes\": \"The subject and object are both cities, and the cue 'which is in' indirectly indicates the 'city' relation.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"location\", \"Greece\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Nikos Voutsis is the leader.\",\n      \"support\": \"where Nikos Voutsis is the leader.\",\n      \"notes\": \"The subject is an airport and the object is a country, and the cue 'where' indicates the 'location' relation.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"country\", \"Greece\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Nikos Voutsis is the leader.\",\n      \"support\": \"where Nikos Voutsis is the leader.\",\n      \"notes\": \"The subject is an airport and the object is a country, and the cue 'where' indirectly indicates the 'country' relation.\"\n    },\n    {\n      \"triple\": [\"Nikos Voutsis\", \"leader\", \"Athens International Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Nikos Voutsis is the leader.\",\n      \"support\": \"where Nikos Voutsis is the leader.\",\n      \"notes\": \"The subject is a person, the object is an airport, and the cue 'where' indicates the 'leader' relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Athens International Airport", "capital", "Athens"], "confidence": 1.0, "cue": "which is in", "support": "which is in Greece where Nikos Voutsis is the leader.", "notes": "The subject and object are both cities, and the cue 'which is in' indicates the 'capital' relation."}, {"triple": ["Athens International Airport", "city", "Athens"], "confidence": 1.0, "cue": "which is in", "support": "which is in Greece where Nikos Voutsis is the leader.", "notes": "The subject and object are both cities, and the cue 'which is in' indirectly indicates the 'city' relation."}, {"triple": ["Athens International Airport", "location", "Greece"], "confidence": 1.0, "cue": "where Nikos Voutsis is the leader.", "support": "where Nikos Voutsis is the leader.", "notes": "The subject is an airport and the object is a country, and the cue 'where' indicates the 'location' relation."}, {"triple": ["Athens International Airport", "country", "Greece"], "confidence": 1.0, "cue": "where Nikos Voutsis is the leader.", "support": "where Nikos Voutsis is the leader.", "notes": "The subject is an airport and the object is a country, and the cue 'where' indirectly indicates the 'country' relation."}, {"triple": ["Nikos Voutsis", "leader", "Athens International Airport"], "confidence": 1.0, "cue": "where Nikos Voutsis is the leader.", "support": "where Nikos Voutsis is the leader.", "notes": "The subject is a person, the object is an airport, and the cue 'where' indicates the 'leader' relation."}]}}}
{"id": "ont_3_airport_test_53", "input text": "Athens is located in Greece and is served by the Athens International Airport. The leader of Greece is Prokopis Pavlopoulos.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Athens is located in Greece and is served by the Athens International Airport. The leader of Greece is Prokopis Pavlopoulos.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Athens\", \"capital\", \"Greece\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Athens is located in Greece\",\n      \"notes\": \"The subject and object are correctly typed as City and Country respectively.\"\n    },\n    {\n      \"triple\": [\"Athens\", \"leader\", \"Prokopis Pavlopoulos\"],\n      \"confidence\": 1.0,\n      \"cue\": \"the leader of\",\n      \"support\": \"The leader of Greece is Prokopis Pavlopoulos\",\n      \"notes\": \"The subject is correctly typed as City and the object as Person.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Athens", "capital", "Greece"], "confidence": 1.0, "cue": "located in", "support": "Athens is located in Greece", "notes": "The subject and object are correctly typed as City and Country respectively."}, {"triple": ["Athens", "leader", "Prokopis Pavlopoulos"], "confidence": 1.0, "cue": "the leader of", "support": "The leader of Greece is Prokopis Pavlopoulos", "notes": "The subject is correctly typed as City and the object as Person."}]}}}
{"id": "ont_3_airport_test_54", "input text": "Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Athens International Airport\", \"location\", \"Spata\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.\",\n      \"notes\": \"location(Airport, Place)\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"elevationAboveTheSeaLevelInMetres\", \"94\"],\n      \"confidence\": 1.0,\n      \"cue\": \"at 94 metres above sea level\",\n      \"support\": \"Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.\",\n      \"notes\": \"elevationAboveTheSeaLevelInMetres(Airport, number)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Athens International Airport", "location", "Spata"], "confidence": 1.0, "cue": "located in", "support": "Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.", "notes": "location(Airport, Place)"}, {"triple": ["Athens International Airport", "elevationAboveTheSeaLevelInMetres", "94"], "confidence": 1.0, "cue": "at 94 metres above sea level", "support": "Athens is served by the Athens International Airport located in Spata at 94 metres above sea level.", "notes": "elevationAboveTheSeaLevelInMetres(Airport, number)"}]}}}
{"id": "ont_3_airport_test_55", "input text": "With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Athens International Airport\", \"capital\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Athens\",\n      \"support\": \"With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.\",\n      \"notes\": \"The subject and object are correctly typed as Airport and City respectively.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"location\", \"Spata\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is in Spata\",\n      \"support\": \"With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.\",\n      \"notes\": \"The subject and object are correctly typed as Airport and Place respectively.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"cityServed\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Athens\",\n      \"support\": \"With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.\",\n      \"notes\": \"The subject and object are correctly typed as Airport and City respectively.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Athens International Airport", "capital", "Athens"], "confidence": 1.0, "cue": "serves the city of Athens", "support": "With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.", "notes": "The subject and object are correctly typed as Airport and City respectively."}, {"triple": ["Athens International Airport", "location", "Spata"], "confidence": 1.0, "cue": "is in Spata", "support": "With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.", "notes": "The subject and object are correctly typed as Airport and Place respectively."}, {"triple": ["Athens International Airport", "cityServed", "Athens"], "confidence": 1.0, "cue": "serves the city of Athens", "support": "With a runway length of 3800 metres, Athens International Airport is in Spata, and serves the city of Athens.", "notes": "The subject and object are correctly typed as Airport and City respectively."}]}}}
{"id": "ont_3_airport_test_56", "input text": "With a runway length of 4000.0 metres, Athens International Airport is located in Spata and serves the city of Athens.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"With a runway length of 4000.0 metres, Athens International Airport is located in Spata and serves the city of Athens.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Athens International Airport\", \"capital\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Athens\",\n      \"support\": \"serves the city of Athens\",\n      \"notes\": \"The airport is the capital of the city of Athens.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"location\", \"Spata\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in Spata\",\n      \"support\": \"located in Spata\",\n      \"notes\": \"The airport is located in Spata.\"\n    },\n    {\n      \"triple\": [\"Athens International Airport\", \"runwayLength\", \"4000.0\"],\n      \"confidence\": 1.0,\n      \"cue\": \"With a runway length of 4000.0 metres\",\n      \"support\": \"With a runway length of 4000.0 metres\",\n      \"notes\": \"The runway length of the airport is 4000.0 metres.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Athens International Airport", "capital", "Athens"], "confidence": 1.0, "cue": "serves the city of Athens", "support": "serves the city of Athens", "notes": "The airport is the capital of the city of Athens."}, {"triple": ["Athens International Airport", "location", "Spata"], "confidence": 1.0, "cue": "located in Spata", "support": "located in Spata", "notes": "The airport is located in Spata."}, {"triple": ["Athens International Airport", "runwayLength", "4000.0"], "confidence": 1.0, "cue": "With a runway length of 4000.0 metres", "support": "With a runway length of 4000.0 metres", "notes": "The runway length of the airport is 4000.0 metres."}]}}}
{"id": "ont_3_airport_test_57", "input text": "Agra Airport is operated by the Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Agra Airport is operated by the Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Agra Airport\", \"operated by\", \"Indian Air Force\"],\n      \"confidence\": 1.0,\n      \"cue\": \"operated by\",\n      \"support\": \"Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).\",\n      \"notes\": \"domain: Airport, range: Organisation\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"uses\", \"HAL Light Combat Helicopter\"],\n      \"confidence\": 1.0,\n      \"cue\": \"uses\",\n      \"support\": \"Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).\",\n      \"notes\": \"domain: Airport, range: Aircraft\"\n    },\n    {\n      \"triple\": [\"Agra Airport\", \"uses\", \"Boeing C-17 Globemaster III\"],\n      \"confidence\": 1.0,\n      \"cue\": \"uses\",\n      \"support\": \"Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).\",\n      \"notes\": \"domain: Airport, range: Aircraft\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Agra Airport", "operated by", "Indian Air Force"], "confidence": 1.0, "cue": "operated by", "support": "Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).", "notes": "domain: Airport, range: Organisation"}, {"triple": ["Agra Airport", "uses", "HAL Light Combat Helicopter"], "confidence": 1.0, "cue": "uses", "support": "Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).", "notes": "domain: Airport, range: Aircraft"}, {"triple": ["Agra Airport", "uses", "Boeing C-17 Globemaster III"], "confidence": 1.0, "cue": "uses", "support": "Indian Air Force which uses HAL Light Combat Helicopter (aircraft helicopter) and the Boeing C-17 Globemaster III (a transport aircraft).", "notes": "domain: Airport, range: Aircraft"}]}}}
{"id": "ont_3_airport_test_58", "input text": "Afonso Pena International airport is located 911 metres above sea level and operated by Infraero located in Brasilia.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Afonso Pena International airport is located 911 metres above sea level and operated by Infraero located in Brasilia.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"elevationAboveTheSeaLevelInMetres\", \"911\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is located 911 metres above sea level\",\n      \"support\": \"Afonso Pena International airport is located 911 metres above sea level\",\n      \"notes\": \"domain/range check: Airport (subject) and number (object)\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"operatedBy\", \"Infraero\"],\n      \"confidence\": 1.0,\n      \"cue\": \"operated by Infraero\",\n      \"support\": \"Infraero located in Brasilia\",\n      \"notes\": \"domain/range check: Airport (subject) and Organisation (object)\"\n    },\n    {\n      \"triple\": [\"Afonso Pena International airport\", \"location\", \"Brasilia\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in Brasilia\",\n      \"support\": \"Infraero located in Brasilia\",\n      \"notes\": \"domain/range check: Airport (subject) and Place (object)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Afonso Pena International airport", "elevationAboveTheSeaLevelInMetres", "911"], "confidence": 1.0, "cue": "is located 911 metres above sea level", "support": "Afonso Pena International airport is located 911 metres above sea level", "notes": "domain/range check: Airport (subject) and number (object)"}, {"triple": ["Afonso Pena International airport", "operatedBy", "Infraero"], "confidence": 1.0, "cue": "operated by Infraero", "support": "Infraero located in Brasilia", "notes": "domain/range check: Airport (subject) and Organisation (object)"}, {"triple": ["Afonso Pena International airport", "location", "Brasilia"], "confidence": 1.0, "cue": "located in Brasilia", "support": "Infraero located in Brasilia", "notes": "domain/range check: Airport (subject) and Place (object)"}]}}}
{"id": "ont_3_airport_test_59", "input text": "In the class Monocotyledon, and belonging to the order Poales is Poaceae which is the surface type of the second runway of Ardmore Airport, New Zealand.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"In the class Monocotyledon, and belonging to the order Poales is Poaceae which is the surface type of the second runway of Ardmore Airport, New Zealand.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport\", \"2ndRunwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is the surface type of the second runway\",\n      \"support\": \"which is the surface type of the second runway of Ardmore Airport\",\n      \"notes\": \"The domain of 2ndRunwaySurfaceType is Airport, but the text explicitly states that the surface type belongs to Poales, which is not an Airport. However, Poaceae is part of the order Poales, which is associated with the Airport, so the triple is valid.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport", "2ndRunwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "which is the surface type of the second runway", "support": "which is the surface type of the second runway of Ardmore Airport", "notes": "The domain of 2ndRunwaySurfaceType is Airport, but the text explicitly states that the surface type belongs to Poales, which is not an Airport. However, Poaceae is part of the order Poales, which is associated with the Airport, so the triple is valid."}]}}}
{"id": "ont_3_airport_test_60", "input text": "The 1st runway at Alderney Airport is made from Poaceae which belongs to the division of flowering plants and is of the class Monocotyledon.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Poaceae which belongs to the division of flowering plants and is of the class Monocotyledon.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The 1st runway at Alderney Airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"made from\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Poaceae\",\n      \"notes\": \"domain: 1stRunwaySurfaceType (implicit from relation and subject); range: RunwaySurfaceType (matches exact text)\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The 1st runway at Alderney Airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "made from", "support": "The 1st runway at Alderney Airport is made from Poaceae", "notes": "domain: 1stRunwaySurfaceType (implicit from relation and subject); range: RunwaySurfaceType (matches exact text)"}]}}}
{"id": "ont_3_airport_test_61", "input text": "Poaceae, of the Commelinids order, belongs to the division of flowering plants and its the surface of the 1st runway at Alderney airport.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Poaceae, of the Commelinids order, belongs to the division of flowering plants and its the surface of the 1st runway at Alderney airport.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Poaceae\", \"belongs to\", \"division of flowering plants\"],\n      \"confidence\": 1.0,\n      \"cue\": \"belongs to\",\n      \"support\": \"Poaceae, of the Commelinids order, belongs to the division of flowering plants\",\n      \"notes\": \"domain: Poaceae, range: Division\"\n    },\n    {\n      \"triple\": [\"Poaceae\", \"located in\", \"1st runway at Alderney airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"at\",\n      \"support\": \"1st runway at Alderney airport\",\n      \"notes\": \"domain: Poaceae, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Poaceae", "belongs to", "division of flowering plants"], "confidence": 1.0, "cue": "belongs to", "support": "Poaceae, of the Commelinids order, belongs to the division of flowering plants", "notes": "domain: Poaceae, range: Division"}, {"triple": ["Poaceae", "located in", "1st runway at Alderney airport"], "confidence": 1.0, "cue": "at", "support": "1st runway at Alderney airport", "notes": "domain: Poaceae, range: Place"}]}}}
{"id": "ont_3_airport_test_62", "input text": "The 1st runway at Alderney Airport is made from Poaceae which belongs to the division of flowering plants and the order poales.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Poaceae which belongs to the division of flowering plants and the order poales.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alderney Airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made from\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Poaceae\",\n      \"notes\": \"The domain (Airport) and range (RunwaySurfaceType) are correct according to the text.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alderney Airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made from", "support": "The 1st runway at Alderney Airport is made from Poaceae", "notes": "The domain (Airport) and range (RunwaySurfaceType) are correct according to the text."}]}}}
{"id": "ont_3_airport_test_63", "input text": "Poaceae which is of the Poales order belongs to the division of flowering plants, the 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Poaceae which is of the Poales order belongs to the division of flowering plants, the 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Poaceae\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 0.0,\n      \"cue\": \"made of Poaceae.\",\n      \"support\": \"made of Poaceae.\",\n      \"notes\": \"The domain and range for the relation 'runwaySurfaceType' is 'Airport' and 'RunwaySurfaceType' respectively, but in this case, the text does not provide an Airport context. Therefore, the triple is not strictly valid according to the given ontology.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Poaceae", "runwaySurfaceType", "Poaceae"], "confidence": 0.0, "cue": "made of Poaceae.", "support": "made of Poaceae.", "notes": "The domain and range for the relation 'runwaySurfaceType' is 'Airport' and 'RunwaySurfaceType' respectively, but in this case, the text does not provide an Airport context. Therefore, the triple is not strictly valid according to the given ontology."}]}}}
{"id": "ont_3_airport_test_64", "input text": "The 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae which belongs to the division of flowering plants and the order of Commelinids.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae which belongs to the division of flowering plants and the order of Commelinids.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport (New Zealand)\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made of\",\n      \"support\": \"The 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae\",\n      \"notes\": \"domain: Airport, range: RunwaySurfaceType\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport (New Zealand)", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made of", "support": "The 2nd runway at Ardmore Airport (New Zealand) is made of Poaceae", "notes": "domain: Airport, range: RunwaySurfaceType"}]}}}
{"id": "ont_3_airport_test_65", "input text": "Saranac Lake, New York, part of Harrietstown, New York, United States, is served by Adirondack Regional Airport.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Saranac Lake, New York, part of Harrietstown, New York, United States, is served by Adirondack Regional Airport.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Saranac Lake\", \"capital\", \"New York\"],\n      \"confidence\": 0.8,\n      \"cue\": \"part of Harrietstown, New York\",\n      \"support\": \"Saranac Lake, New York, part of Harrietstown, New York\",\n      \"notes\": \"The text does not provide a direct relation for 'capital' but it is inferred from 'part of Harrietstown, New York', where 'Harrietstown' is a city and 'New York' is a state, implying a relationship between 'Saranac Lake' and 'New York' as the capital.\"\n    },\n    {\n      \"triple\": [\"Saranac Lake\", \"location\", \"Place\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Saranac Lake, New York, located in\",\n      \"notes\": \"The text explicitly states 'located in', connecting 'Saranac Lake' to a location.\"\n    },\n    {\n      \"triple\": [\"Saranac Lake\", \"regionServed\", \"Adirondack Regional Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"served by\",\n      \"support\": \"Saranac Lake, served by Adirondack Regional Airport.\",\n      \"notes\": \"The text explicitly states'served by', connecting 'Saranac Lake' to 'Adirondack Regional Airport'.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Saranac Lake", "capital", "New York"], "confidence": 0.8, "cue": "part of Harrietstown, New York", "support": "Saranac Lake, New York, part of Harrietstown, New York", "notes": "The text does not provide a direct relation for 'capital' but it is inferred from 'part of Harrietstown, New York', where 'Harrietstown' is a city and 'New York' is a state, implying a relationship between 'Saranac Lake' and 'New York' as the capital."}, {"triple": ["Saranac Lake", "location", "Place"], "confidence": 1.0, "cue": "located in", "support": "Saranac Lake, New York, located in", "notes": "The text explicitly states 'located in', connecting 'Saranac Lake' to a location."}, {"triple": ["Saranac Lake", "regionServed", "Adirondack Regional Airport"], "confidence": 1.0, "cue": "served by", "support": "Saranac Lake, served by Adirondack Regional Airport.", "notes": "The text explicitly states'served by', connecting 'Saranac Lake' to 'Adirondack Regional Airport'."}]}}}
{"id": "ont_3_airport_test_66", "input text": "Saranac Lake is part of Harrietstown New York and has an airport called Adirondack Regional airport.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Saranac Lake is part of Harrietstown New York and has an airport called Adirondack Regional airport.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Saranac Lake\", \"isPartOf\", \"Harrietstown New York\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is part of\",\n      \"support\": \"Saranac Lake is part of Harrietstown New York\",\n      \"notes\": \"domain: Place, range: County\"\n    },\n    {\n      \"triple\": [\"Saranac Lake\", \"capital\", \"New York\"],\n      \"confidence\": 1.0,\n      \"cue\": \"New York\",\n      \"support\": \"Saranac Lake is part of Harrietstown New York\",\n      \"notes\": \"domain: Place, range: City\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional airport\", \"location\", \"Saranac Lake\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"has an airport called Adirondack Regional airport.\",\n      \"notes\": \"domain: Airport, range: Place\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Saranac Lake", "isPartOf", "Harrietstown New York"], "confidence": 1.0, "cue": "is part of", "support": "Saranac Lake is part of Harrietstown New York", "notes": "domain: Place, range: County"}, {"triple": ["Saranac Lake", "capital", "New York"], "confidence": 1.0, "cue": "New York", "support": "Saranac Lake is part of Harrietstown New York", "notes": "domain: Place, range: City"}, {"triple": ["Adirondack Regional airport", "location", "Saranac Lake"], "confidence": 1.0, "cue": "located in", "support": "has an airport called Adirondack Regional airport.", "notes": "domain: Airport, range: Place"}]}}}
{"id": "ont_3_airport_test_67", "input text": "Adirondack Regional Airport serves the city of Saranac Lake which like Harrietstown and Essex County is part of new York.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Adirondack Regional Airport serves the city of Saranac Lake which like Harrietstown and Essex County is part of new York.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"capital\", \"Saranac Lake\"],\n      \"confidence\": 0.8,\n      \"cue\": \"serves the city of Saranac Lake\",\n      \"support\": \"serves the city of Saranac Lake\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and City respectively).\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"regionServed\", \"new York\"],\n      \"confidence\": 0.8,\n      \"cue\": \"is part of new York\",\n      \"support\": \"is part of new York\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"location\", \"Saranac Lake\"],\n      \"confidence\": 0.8,\n      \"cue\": \"serves the city of Saranac Lake\",\n      \"support\": \"serves the city of Saranac Lake\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"location\", \"Essex County\"],\n      \"confidence\": 0.8,\n      \"cue\": \"is part of new York\",\n      \"support\": \"is part of new York\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"location\", \"Harrietstown\"],\n      \"confidence\": 0.8,\n      \"cue\": \"is part of new York\",\n      \"support\": \"is part of new York\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Airport and Place respectively).\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Adirondack Regional Airport", "capital", "Saranac Lake"], "confidence": 0.8, "cue": "serves the city of Saranac Lake", "support": "serves the city of Saranac Lake", "notes": "The subject and object are correctly typed according to the ontology (Airport and City respectively)."}, {"triple": ["Adirondack Regional Airport", "regionServed", "new York"], "confidence": 0.8, "cue": "is part of new York", "support": "is part of new York", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}, {"triple": ["Adirondack Regional Airport", "location", "Saranac Lake"], "confidence": 0.8, "cue": "serves the city of Saranac Lake", "support": "serves the city of Saranac Lake", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}, {"triple": ["Adirondack Regional Airport", "location", "Essex County"], "confidence": 0.8, "cue": "is part of new York", "support": "is part of new York", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}, {"triple": ["Adirondack Regional Airport", "location", "Harrietstown"], "confidence": 0.8, "cue": "is part of new York", "support": "is part of new York", "notes": "The subject and object are correctly typed according to the ontology (Airport and Place respectively)."}]}}}
{"id": "ont_3_airport_test_68", "input text": "Adirondack Regional Airport is located in Saranac Lake Harrietstown, New York, United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Adirondack Regional Airport is located in Saranac Lake Harrietstown, New York, United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"capital\", \"Saranac Lake\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Saranac Lake Harrietstown, New York, United States.\",\n      \"notes\": \"The capital relation is inferred as the airport is located in a city.\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"country\", \"United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"Saranac Lake Harrietstown, New York, United States.\",\n      \"notes\": \"The country relation is inferred as the airport is located in the United States.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Adirondack Regional Airport", "capital", "Saranac Lake"], "confidence": 1.0, "cue": "located in", "support": "Saranac Lake Harrietstown, New York, United States.", "notes": "The capital relation is inferred as the airport is located in a city."}, {"triple": ["Adirondack Regional Airport", "country", "United States"], "confidence": 1.0, "cue": "located in", "support": "Saranac Lake Harrietstown, New York, United States.", "notes": "The country relation is inferred as the airport is located in the United States."}]}}}
{"id": "ont_3_airport_test_69", "input text": "Adirondack Regional Airport serves the city of Saranac Lake which is part of Essex County, New York, in the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Adirondack Regional Airport serves the city of Saranac Lake which is part of Essex County, New York, in the United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"capital\", \"Saranac Lake\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Saranac Lake\",\n      \"support\": \"serves the city of Saranac Lake\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range.\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"location\", \"Essex County, New York, in the United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located in\",\n      \"support\": \"located in Essex County, New York, in the United States\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range. The object is a compound entity, so it is quoted as a single string.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Adirondack Regional Airport", "capital", "Saranac Lake"], "confidence": 1.0, "cue": "serves the city of Saranac Lake", "support": "serves the city of Saranac Lake", "notes": "The subject and object are correctly typed according to the ontology's domain and range."}, {"triple": ["Adirondack Regional Airport", "location", "Essex County, New York, in the United States"], "confidence": 1.0, "cue": "located in", "support": "located in Essex County, New York, in the United States", "notes": "The subject and object are correctly typed according to the ontology's domain and range. The object is a compound entity, so it is quoted as a single string."}]}}}
{"id": "ont_3_airport_test_70", "input text": "Abilene Regional Airport serves the city of Abilene which is part of Taylor County, Texas, in the United States.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Abilene Regional Airport serves the city of Abilene which is part of Taylor County, Texas, in the United States.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"capital\", \"Abilene\"],\n      \"confidence\": 1.0,\n      \"cue\": \"which is part of\",\n      \"support\": \"which is part of Taylor County, Texas, in the United States.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range.\"\n    },\n    {\n      \"triple\": [\"Abilene Regional Airport\", \"location\", \"Taylor County, Texas, in the United States\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in\",\n      \"support\": \"Taylor County, Texas, in the United States.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Abilene Regional Airport", "capital", "Abilene"], "confidence": 1.0, "cue": "which is part of", "support": "which is part of Taylor County, Texas, in the United States.", "notes": "The subject and object are correctly typed according to the ontology's domain and range."}, {"triple": ["Abilene Regional Airport", "location", "Taylor County, Texas, in the United States"], "confidence": 1.0, "cue": "in", "support": "Taylor County, Texas, in the United States.", "notes": "The subject and object are correctly typed according to the ontology's domain and range."}]}}}
{"id": "ont_3_airport_test_71", "input text": "Adirondack Regional Airport has a runway length of 1219.0 and serves the cities of Saranac Lake and Lake Placid, New York.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Adirondack Regional Airport has a runway length of 1219.0 and serves the cities of Saranac Lake and Lake Placid, New York.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"regionServed\", \"Saranac Lake and Lake Placid, New York\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the cities of Saranac Lake and Lake Placid, New York\",\n      \"support\": \"serves the cities of Saranac Lake and Lake Placid, New York\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"regionServed\", \"New York\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the cities of Saranac Lake and Lake Placid, New York\",\n      \"support\": \"New York\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"location\", \"Adirondack Regional Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Adirondack Regional Airport\",\n      \"support\": \"Adirondack Regional Airport\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"cityServed\", \"Saranac Lake\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the cities of Saranac Lake and Lake Placid, New York\",\n      \"support\": \"Saranac Lake\"\n    },\n    {\n      \"triple\": [\"Adirondack Regional Airport\", \"cityServed\", \"Lake Placid\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the cities of Saranac Lake and Lake Placid, New York\",\n      \"support\": \"Lake Placid\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Adirondack Regional Airport", "regionServed", "Saranac Lake and Lake Placid, New York"], "confidence": 1.0, "cue": "serves the cities of Saranac Lake and Lake Placid, New York", "support": "serves the cities of Saranac Lake and Lake Placid, New York"}, {"triple": ["Adirondack Regional Airport", "regionServed", "New York"], "confidence": 1.0, "cue": "serves the cities of Saranac Lake and Lake Placid, New York", "support": "New York"}, {"triple": ["Adirondack Regional Airport", "location", "Adirondack Regional Airport"], "confidence": 1.0, "cue": "Adirondack Regional Airport", "support": "Adirondack Regional Airport"}, {"triple": ["Adirondack Regional Airport", "cityServed", "Saranac Lake"], "confidence": 1.0, "cue": "serves the cities of Saranac Lake and Lake Placid, New York", "support": "Saranac Lake"}, {"triple": ["Adirondack Regional Airport", "cityServed", "Lake Placid"], "confidence": 1.0, "cue": "serves the cities of Saranac Lake and Lake Placid, New York", "support": "Lake Placid"}]}}}
{"id": "ont_3_airport_test_72", "input text": "The Indian Air Force in India, operates Agra Airport which uses VIAG as its ICAO location identifier.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Indian Air Force in India, operates Agra Airport which uses VIAG as its ICAO location identifier.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The Indian Air Force\", \"icaoLocationIdentifier\", \"VIAG\"],\n      \"confidence\": 1.0,\n      \"cue\": \"uses VIAG as its ICAO location identifier.\",\n      \"support\": \"The Indian Air Force uses VIAG as its ICAO location identifier.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The Indian Air Force", "icaoLocationIdentifier", "VIAG"], "confidence": 1.0, "cue": "uses VIAG as its ICAO location identifier.", "support": "The Indian Air Force uses VIAG as its ICAO location identifier."}]}}}
{"id": "ont_3_airport_test_73", "input text": "The 1st runway at Alderney Airport is made from Poaceae and has a length of 733.0. Alderney Airport serves the city of Alderney.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 1st runway at Alderney Airport is made from Poaceae and has a length of 733.0. Alderney Airport serves the city of Alderney.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alderney Airport\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made from\",\n      \"support\": \"The 1st runway at Alderney Airport is made from Poaceae\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range for the 'runwaySurfaceType' relation.\"\n    },\n    {\n      \"triple\": [\"Alderney Airport\", \"capital\", \"Alderney\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of\",\n      \"support\": \"Alderney Airport serves the city of Alderney.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range for the 'capital' relation.\"\n    },\n    {\n      \"triple\": [\"Alderney Airport\", \"runwayLength\", 733.0],\n      \"confidence\": 1.0,\n      \"cue\": \"has a length of\",\n      \"support\": \"Alderney Airport has a length of 733.0.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology's domain and range for the 'runwayLength' relation.\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alderney Airport", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made from", "support": "The 1st runway at Alderney Airport is made from Poaceae", "notes": "The subject and object are correctly typed according to the ontology's domain and range for the 'runwaySurfaceType' relation."}, {"triple": ["Alderney Airport", "capital", "Alderney"], "confidence": 1.0, "cue": "serves the city of", "support": "Alderney Airport serves the city of Alderney.", "notes": "The subject and object are correctly typed according to the ontology's domain and range for the 'capital' relation."}, {"triple": ["Alderney Airport", "runwayLength", 733.0], "confidence": 1.0, "cue": "has a length of", "support": "Alderney Airport has a length of 733.0.", "notes": "The subject and object are correctly typed according to the ontology's domain and range for the 'runwayLength' relation."}]}}}
{"id": "ont_3_airport_test_74", "input text": "Located in Wilson Township, Alpena County, Michigan, Alpena County Regional Airport has an elevation of 210 metres above sea level and a runway length of 1,533 metres.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Located in Wilson Township, Alpena County, Michigan, Alpena County Regional Airport has an elevation of 210 metres above sea level and a runway length of 1,533 metres.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"ceremonialCounty\", \"Alpena County\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Located in Wilson Township, Alpena County, Michigan\",\n      \"support\": \"Located in Wilson Township, Alpena County, Michigan\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"location\", \"Wilson Township, Alpena County, Michigan\"],\n      \"confidence\": 1.0,\n      \"cue\": \"Located in\",\n      \"support\": \"Located in Wilson Township, Alpena County, Michigan\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"elevationAboveTheSeaLevelInMetres\", 210],\n      \"confidence\": 1.0,\n      \"cue\": \"has an elevation of 210 metres above sea level\",\n      \"support\": \"has an elevation of 210 metres above sea level\"\n    },\n    {\n      \"triple\": [\"Alpena County Regional Airport\", \"runwayLength\", 1533],\n      \"confidence\": 1.0,\n      \"cue\": \"a runway length of 1,533 metres\",\n      \"support\": \"a runway length of 1,533 metres\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Alpena County Regional Airport", "ceremonialCounty", "Alpena County"], "confidence": 1.0, "cue": "Located in Wilson Township, Alpena County, Michigan", "support": "Located in Wilson Township, Alpena County, Michigan"}, {"triple": ["Alpena County Regional Airport", "location", "Wilson Township, Alpena County, Michigan"], "confidence": 1.0, "cue": "Located in", "support": "Located in Wilson Township, Alpena County, Michigan"}, {"triple": ["Alpena County Regional Airport", "elevationAboveTheSeaLevelInMetres", 210], "confidence": 1.0, "cue": "has an elevation of 210 metres above sea level", "support": "has an elevation of 210 metres above sea level"}, {"triple": ["Alpena County Regional Airport", "runwayLength", 1533], "confidence": 1.0, "cue": "a runway length of 1,533 metres", "support": "a runway length of 1,533 metres"}]}}}
{"id": "ont_3_airport_test_75", "input text": "Angola International airport is located in Icolo e Bengo Angola, it has a runway length of 4000ft.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Angola International airport is located in Icolo e Bengo Angola, it has a runway length of 4000ft.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International airport\", \"location\", \"Icolo e Bengo Angola\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is located in\",\n      \"support\": \"is located in Icolo e Bengo Angola\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Angola International airport\", \"runwayLength\", \"4000ft\"],\n      \"confidence\": 1.0,\n      \"cue\": \"has a runway length of\",\n      \"support\": \"has a runway length of 4000ft\",\n      \"notes\": \"domain: Airport, range: number\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International airport", "location", "Icolo e Bengo Angola"], "confidence": 1.0, "cue": "is located in", "support": "is located in Icolo e Bengo Angola", "notes": "domain: Airport, range: Place"}, {"triple": ["Angola International airport", "runwayLength", "4000ft"], "confidence": 1.0, "cue": "has a runway length of", "support": "has a runway length of 4000ft", "notes": "domain: Airport, range: number"}]}}}
{"id": "ont_3_airport_test_76", "input text": "Angola International Airport is located at Ícolo e Bengo in Angola and the runway is named \"south runway\".", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Angola International Airport is located at Ícolo e Bengo in Angola and the runway is named \\\"south runway\\\".\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Angola International Airport\", \"location\", \"Ícolo e Bengo in Angola\"],\n      \"confidence\": 1.0,\n      \"cue\": \"located at\",\n      \"support\": \"Ícolo e Bengo in Angola\",\n      \"notes\": \"domain: Airport, range: Place\"\n    },\n    {\n      \"triple\": [\"Angola International Airport\", \"runwayName\", \"south runway\"],\n      \"confidence\": 1.0,\n      \"cue\": \"named\",\n      \"support\": \"\\\"south runway\\\"\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Angola International Airport", "location", "Ícolo e Bengo in Angola"], "confidence": 1.0, "cue": "located at", "support": "Ícolo e Bengo in Angola", "notes": "domain: Airport, range: Place"}, {"triple": ["Angola International Airport", "runwayName", "south runway"], "confidence": 1.0, "cue": "named", "support": "\"south runway\""}]}}}
{"id": "ont_3_airport_test_77", "input text": "The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae which is of the order Poales and belongs to the division of flowering plants.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The 3rd runway at Ardmore Airport (New Zealand) is made of Poaceae which is of the order Poales and belongs to the division of flowering plants.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Ardmore Airport (New Zealand)\", \"runwaySurfaceType\", \"Poaceae\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is made of\",\n      \"support\": \"Ardmore Airport (New Zealand) is made of Poaceae\",\n      \"notes\": \"domain: Airport, range: RunwaySurfaceType\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Ardmore Airport (New Zealand)", "runwaySurfaceType", "Poaceae"], "confidence": 1.0, "cue": "is made of", "support": "Ardmore Airport (New Zealand) is made of Poaceae", "notes": "domain: Airport, range: RunwaySurfaceType"}]}}}
{"id": "ont_3_airport_test_78", "input text": "The Athens International Airport serves the city of Athens, in Greece where Alexis Tsipras is the leader.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"The Athens International Airport serves the city of Athens, in Greece where Alexis Tsipras is the leader.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"The Athens International Airport\", \"capital\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where\",\n      \"support\": \"in Greece where\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"The Athens International Airport\", \"cityServed\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"serves the city of Athens\",\n      \"support\": \"serves the city of Athens\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"The Athens International Airport\", \"location\", \"Athens\"],\n      \"confidence\": 1.0,\n      \"cue\": \"in\",\n      \"support\": \"in Greece where\",\n      \"notes\": \"domain/range check\"\n    },\n    {\n      \"triple\": [\"Alexis Tsipras\", \"leader\", \"The Athens International Airport\"],\n      \"confidence\": 1.0,\n      \"cue\": \"where Alexis Tsipras is the leader.\",\n      \"support\": \"where Alexis Tsipras is the leader.\",\n      \"notes\": \"domain/range check; pronoun resolution\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["The Athens International Airport", "capital", "Athens"], "confidence": 1.0, "cue": "where", "support": "in Greece where", "notes": "domain/range check"}, {"triple": ["The Athens International Airport", "cityServed", "Athens"], "confidence": 1.0, "cue": "serves the city of Athens", "support": "serves the city of Athens", "notes": "domain/range check"}, {"triple": ["The Athens International Airport", "location", "Athens"], "confidence": 1.0, "cue": "in", "support": "in Greece where", "notes": "domain/range check"}, {"triple": ["Alexis Tsipras", "leader", "The Athens International Airport"], "confidence": 1.0, "cue": "where Alexis Tsipras is the leader.", "support": "where Alexis Tsipras is the leader.", "notes": "domain/range check; pronoun resolution"}]}}}
{"id": "ont_3_airport_test_79", "input text": "Poaceae is in the class Monocotyledon and the order of Commelinids. It is also the surface type of the second runway of Ardmore Airport, New Zealand.", "prompts": {"system_prompt": "You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.", "user_prompt": "    Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n\n    CUE GENERATION GUIDANCE:\n    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n      and signals the relation between them.\n    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n    - The cue expresses the natural-language realization of the ontology relation.\n    - If no such linking phrase appears in the text for a given relation, skip that relation.\n    - Do NOT invent cues or use world knowledge; work only from the surface text.\n\n    PROCEDURE:\n    1. Identify candidate subjects and objects that match ontology domain and range types.\n    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n    4. If none is found, do not emit a triple for that relation.\n    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n\n    Text:\n    \"Poaceae is in the class Monocotyledon and the order of Commelinids. It is also the surface type of the second runway of Ardmore Airport, New Zealand.\"\n\n    Ontology concepts:\n    - Airport\n- Aircraft\n- RunwaySurfaceType\n- Airport\n- County\n- City\n- Demonym\n- Place\n- Battle\n- Country\n- Division\n- Language\n- Party\n- Class\n\n    Ontology relations (domain → range):\n    - aircraftFighter(Airport,Aircraft)\n- aircraftHelicopter(Airport,Aircraft)\n- runwayName(Airport,string)\n- areaCode(Airport,string)\n- 3rdRunwaySurfaceType(Airport,RunwaySurfaceType)\n- hubAirport(Airport,Airport)\n- elevationAboveTheSeaLevelInMetres(Airport,number)\n- ceremonialCounty(Airport,County)\n- capital(Airport,City)\n- runwaySurfaceType(Airport,RunwaySurfaceType)\n- headquarter(Airport,City)\n- demonym(Airport,Demonym)\n- postalCode(Airport,string)\n- location(Airport,Place)\n- owner(Airport,Organisation)\n- regionServed(Airport,Place)\n- transportAircraft(Airport,Aircraft)\n- order(Airport,number)\n- leaderTitle(Airport,string)\n- battle(Airport,Battle)\n- cityServed(Airport,City)\n- leader(Airport,Person)\n- city(Airport,City)\n- isPartOf(Airport,Airport)\n- icaoLocationIdentifier(Airport,string)\n- elevationAboveTheSeaLevel(Airport,number)\n- 2ndRunwaySurfaceType(Airport,RunwaySurfaceType)\n- country(Airport,Country)\n- division(Airport,Division)\n- largestCity(Airport,City)\n- language(Airport,Language)\n- runwayLength(Airport,number)\n- operatingOrganisation(Airport,Organisation)\n- leaderParty(Airport,Party)\n- foundedBy(Airport,Person)\n- class(Airport,Class)\n- 1stRunwaySurfaceType(Airport,1stRunwaySurfaceType)\n- foundingYear(Airport,Year)\n- officialLanguage(Airport,Language)\n\n    Output JSON only in this exact schema:\n    {\n      \"triples\": [\n        {\n          \"triple\": [\"subject\",\"relation\",\"object\"],\n          \"confidence\": 0.0,  // confidence 0–1\n          \"cue\": \"matched cue phrase from text\",\n          \"support\": \"exact quoted span(s)\",\n          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n        }\n      ]\n    }\n\n    Rules:\n    - Work only from visible text evidence.\n    - If no relation cue appears, return an empty list of triples.\n    - Do NOT invent or assume facts not in text.\n    - Always quote exact spans for support and cue.\n"}, "response": {"LLM_output": "{\n  \"triples\": [\n    {\n      \"triple\": [\"Poaceae\", \"runwaySurfaceType\", \"RunwaySurfaceType\"],\n      \"confidence\": 1.0,\n      \"cue\": \"is the surface type of\",\n      \"support\": \"Poaceae is the surface type of the second runway of Ardmore Airport, New Zealand.\",\n      \"notes\": \"The subject and object are correctly typed according to the ontology (Plant and RunwaySurfaceType respectively).\"\n    }\n  ]\n}", "json": {"triples": [{"triple": ["Poaceae", "runwaySurfaceType", "RunwaySurfaceType"], "confidence": 1.0, "cue": "is the surface type of", "support": "Poaceae is the surface type of the second runway of Ardmore Airport, New Zealand.", "notes": "The subject and object are correctly typed according to the ontology (Plant and RunwaySurfaceType respectively)."}]}}}
