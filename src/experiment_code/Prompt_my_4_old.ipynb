{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "922ee2a3-5a35-44b4-a2d6-caece5e0d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct 13 11:51:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   39C    P8             31W /  370W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a9a718-f1fa-4726-ad9c-d0ababffa531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (939297) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 939297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770f49a2-a0f8-40c1-9e68-a60218401786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "Executable: /upb/users/b/balram/profiles/unix/cs/.conda/envs/kg_pipeline/bin/python3\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4155b4b7-09b5-4fc5-b5c8-6add97eaa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0 — Imports & Config\n",
    "\n",
    "import os, json, re, time\n",
    "import torch\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9eba531-8734-4269-95a6-5cb6ca325dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True  # KV caching is fine; doesn't cause \"same output\"\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    #print(\"✅ Model loaded successfully.\")\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e289ef-514b-464d-adce-db8acbdd1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "\n",
    "def _quote(v: object) -> str:\n",
    "    \"\"\"\n",
    "    Convert any value to a safe, double-quoted string for the triple format.\n",
    "    Escapes backslashes and double quotes.\n",
    "    \"\"\"\n",
    "    s = str(v)\n",
    "    s = s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "    return f'\"{s}\"'\n",
    "\n",
    "\n",
    "def _coerce_triples_to_str(triples_val):\n",
    "    \"\"\"\n",
    "    Normalize triples into newline-separated lines: rel(\"Subject\",\"Object\")\n",
    "\n",
    "    Accepts:\n",
    "      - str: a preformatted block\n",
    "      - list[str]: each a preformatted line\n",
    "      - list[dict]: each with keys {sub, rel, obj}\n",
    "    \"\"\"\n",
    "    if triples_val is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Already a full block string\n",
    "    if isinstance(triples_val, str):\n",
    "        return triples_val.strip()\n",
    "\n",
    "    # List of preformatted strings\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, str) for x in triples_val):\n",
    "        return \"\\n\".join(x.strip() for x in triples_val if x and x.strip())\n",
    "\n",
    "    # List of dicts -> format to rel(\"sub\",\"obj\")\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, dict) for x in triples_val):\n",
    "        lines = []\n",
    "        for t in triples_val:\n",
    "            rel = (t.get(\"rel\") or \"\").strip()\n",
    "            sub = t.get(\"sub\")\n",
    "            obj = t.get(\"obj\")\n",
    "            if rel and (sub is not None) and (obj is not None):\n",
    "                lines.append(f'{rel}({_quote(sub)},{_quote(obj)})')\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Fallback\n",
    "    return str(triples_val).strip()\n",
    "\n",
    "\n",
    "# ---------- Few-shot loader for FIXED schema ----------\n",
    "\n",
    "def load_one_shot_examples(few_shot_jsonl_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads {id: {\"example_sentence\": <str>, \"example_triples_output\": <any>}}.\n",
    "    - Reads fields exactly as written in your file:\n",
    "        \"Example sentence\"\n",
    "        \"Example triples output\"   (and falls back to \"Example triples\" if the former is absent)\n",
    "    - No validation / coercion.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for rec in read_jsonl(few_shot_jsonl_path, max_items=None):\n",
    "        rid = rec.get(\"id\")\n",
    "        if rid is None:\n",
    "            continue  # keep this tiny guard so the dict key isn't None\n",
    "        out[str(rid).strip()] = {\n",
    "            \"example_sentence\": rec.get(\"Example sentence\"),\n",
    "            # prefer \"Example triples output\", else fallback to \"Example triples\"\n",
    "            \"example_triples_output\": rec.get(\"Example triples output\", rec.get(\"Example triples\")),\n",
    "        }\n",
    "    print(f\"[FEW-SHOT] Loaded {len(out)} examples from: {few_shot_jsonl_path}\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de81bde-9a09-40e9-8b19-f50ee2974b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Prompt Builder (Reason → Verify → Final Triples)\n",
    "\n",
    "from textwrap import dedent\n",
    "import json\n",
    "\n",
    "# ---------- Block 2 — Prompt Builder (Reason → Verify → Final Triples) ----------\n",
    "\n",
    "# Helpers (strict access; fail fast if ontology keys are missing)\n",
    "def _concept_label(ontology_json, qid):\n",
    "    return next((c[\"label\"] for c in ontology_json[\"concepts\"] if c[\"qid\"] == qid), \"\")\n",
    "\n",
    "def format_ontology_concepts(ontology_json):\n",
    "    return \", \".join(c[\"label\"] for c in ontology_json[\"concepts\"])\n",
    "\n",
    "def format_ontology_relations(ontology_json):\n",
    "    lines = []\n",
    "    for r in ontology_json[\"relations\"]:\n",
    "        dom = _concept_label(ontology_json, r[\"domain\"])\n",
    "        rng = _concept_label(ontology_json, r[\"range\"])\n",
    "        lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_reason_then_extract_prompt(\n",
    "    ontology_json,\n",
    "    test_sentence,\n",
    "    worked_example=None,\n",
    "    allow_light_norm=True,\n",
    "    relation_cues_text=None,   # optional Step 0 paraphrase cues\n",
    "    strict_format=True,        # enforce exact output shape\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a prompt that ends with the header `### FINAL TRIPLES` and *no trailing guidance*,\n",
    "    so generations appear directly under the header in the required line format.\n",
    "\n",
    "    This version matches the downstream extractor which searches for `##/### FINAL TRIPLES`.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts_line   = format_ontology_concepts(ontology_json)\n",
    "    relations_block = format_ontology_relations(ontology_json)\n",
    "    norm_note = (\n",
    "        \"normalize only trivial cases (e.g., Japanese→Japan)\"\n",
    "        if allow_light_norm else\n",
    "        \"copy spans verbatim; no normalization\"\n",
    "    )\n",
    "\n",
    "    # Optional Step 0 cues\n",
    "    step0_block = \"\"\n",
    "    if relation_cues_text:\n",
    "        step0_block = dedent(f\"\"\"\n",
    "        Step 0 (Paraphrase expansion for this sentence):\n",
    "        {relation_cues_text.strip()}\n",
    "        \"\"\")\n",
    "\n",
    "    # Optional worked example — supports either the new shape or the older example fields\n",
    "    example_block = \"\"\n",
    "    if worked_example:\n",
    "        ex_sent = worked_example.get(\"example_sentence\")\n",
    "        ex_out  = worked_example.get(\"example_triples_output\")\n",
    "        #         # Use JSON dumps to preserve quotes/escapes for the sentence,\n",
    "        # and to serialize lists/dicts as-is for the output block.\n",
    "        ex_sent_json = json.dumps(ex_sent, ensure_ascii=False)  # -> \"The number of staff...\"\n",
    "        ex_out_json  = json.dumps(ex_out, ensure_ascii=False)   # -> [{\"sub\":...,\"rel\":...,\"obj\":...}, ...]\n",
    "        example_block = dedent(f'''\n",
    "        \"Example sentence\": {ex_sent_json}\n",
    "\n",
    "        \"Example FINAL TRIPLES\": {ex_out_json}\n",
    "        ''').strip()\n",
    "\n",
    "    # Strict output notes merged into RULES (no trailing text after the header in the final prompt)\n",
    "    strict_notes = \"\"\n",
    "    if strict_format:\n",
    "        strict_notes = dedent(\"\"\"\n",
    "        OUTPUT SHAPE (strict):\n",
    "        - After Step 2, output the header exactly:\n",
    "        ### FINAL TRIPLES\n",
    "        - Then output one triple per line format: predicate(\"Subject\",\"Object\")\n",
    "        - No extra commentary after the triples. If no triples, just output the header.\n",
    "        - Do NOT output JSON, code fences, or any other sections.\n",
    "        \"\"\").strip()\n",
    "\n",
    "    rules = dedent(f\"\"\"\n",
    "    RULES:\n",
    "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
    "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
    "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "    - Base decisions ONLY on the TEST SENTENCE.\n",
    "    - Avoid duplicates; apply light normalization only (e.g., Japanese→Japan).\n",
    "    {strict_notes}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # IMPORTANT: end the prompt with the header and NOTHING after it.\n",
    "    prompt = dedent(f\"\"\"\n",
    "    TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "    - Use the ontology to guide relation labeling when possible.\n",
    "    - {norm_note}.\n",
    "    - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
    "\n",
    "    ONTOLOGY CONCEPTS:\n",
    "    {concepts_line}\n",
    "\n",
    "    ONTOLOGY RELATIONS (argument types):\n",
    "    {relations_block}\n",
    "\n",
    "    {rules}\n",
    "\n",
    "    {step0_block}\n",
    "\n",
    "    {example_block if example_block else \"\"}\n",
    "\n",
    "    ### TEST SENTENCE\n",
    "    \"{test_sentence}\"\n",
    "\n",
    "    Step 1 (Entities & types):\n",
    "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "    ### FINAL TRIPLES\n",
    "    \"\"\").strip()\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838bbb8e-79ec-4ca3-8a7a-58c6cead4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 — Single Inference (chat template; continuation-only)\n",
    "\n",
    "def generate_triples_text(generator, tokenizer, prompt_text: str,\n",
    "                          max_new_tokens: int = 768, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Calls the model once. Returns the full generated continuation (reasoning + FINAL TRIPLES).\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544caf55-fb2f-433b-8c83-435333b65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 — Extract & Parse `FINAL TRIPLES`\n",
    "\n",
    "FINAL_HEADER_REGEX = r'^\\s*#{2,}\\s*FINAL\\s+TRIPLES\\s*$'  # matches ## or ### FINAL TRIPLES\n",
    "\n",
    "def extract_final_triples_block(model_output_text: str) -> str:\n",
    "    # try normal header first\n",
    "    #print(\"model_output_text: \",model_output_text)\n",
    "    \n",
    "    header = re.search(FINAL_HEADER_REGEX, model_output_text, re.IGNORECASE | re.MULTILINE)\n",
    "    if header:\n",
    "        start = header.end()\n",
    "        tail = model_output_text[start:].strip()\n",
    "        nxt = re.search(r'^\\s*#{2,}\\s+[A-Z].*$', tail, re.MULTILINE)\n",
    "        return tail[:nxt.start()].strip() if nxt else tail\n",
    "\n",
    "    # fallback: take everything and let the parser fish valid lines out\n",
    "    return model_output_text\n",
    "\n",
    "def parse_triples_block(block_text: str):\n",
    "    \"\"\"\n",
    "    Parse lines of the form:\n",
    "        predicate(\"Subject\",\"Object\")\n",
    "        predicate('Subject','Object')\n",
    "        predicate(Subject,Object)\n",
    "        predicate(Subject, 250)\n",
    "    Keeps both ontology and non-ontology predicates (open-set allowed).\n",
    "    Returns a list of dicts: {\"sub\": ..., \"rel\": ..., \"obj\": ...}\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "\n",
    "    line_re = re.compile(r\"\"\"\n",
    "    ^\\s*(?:[-*•]\\s*)?                 # optional markdown bullet\n",
    "    (?P<rel>[A-Za-z][A-Za-z0-9_ ]*?)  # predicate name\n",
    "    \\s*\\(\\s*\n",
    "    (?:                               # SUBJECT (quoted or unquoted)\n",
    "        [\"'](?P<sub_q>[^\"']+?)[\"']    # \"Subject\" or 'Subject'\n",
    "        | (?P<sub_u>[^,)\\n]+?)        # Subject (unquoted)\n",
    "    )\n",
    "    \\s*,\\s*\n",
    "    (?:                               # OBJECT (quoted or unquoted)\n",
    "        [\"'](?P<obj_q>[^\"']+?)[\"']    # \"Object\" or 'Object'\n",
    "        | (?P<obj_u>[^)\\n]+?)         # Object (unquoted)\n",
    "    )\n",
    "    \\s*\\)\\s*\\.?\\s*$                   # optional trailing period\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "    for raw in block_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        m = line_re.match(line)\n",
    "        if not m:\n",
    "            # skip non-matching lines quietly\n",
    "            continue\n",
    "\n",
    "        rel = m.group(\"rel\").strip()\n",
    "        sub = (m.group(\"sub_q\") or m.group(\"sub_u\") or \"\").strip()\n",
    "        obj = (m.group(\"obj_q\") or m.group(\"obj_u\") or \"\").strip()\n",
    "\n",
    "        # Optional: guard against empties\n",
    "        if not rel or not sub or not obj:\n",
    "            continue\n",
    "\n",
    "        triples.append({\"sub\": sub, \"rel\": rel, \"obj\": obj})\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fccda78-acb3-4fdb-9b9d-86738232623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 — JSONL I/O Helpers\n",
    "\n",
    "def read_jsonl(path, max_items: int | None = None):\n",
    "    \"\"\"\n",
    "    Yields JSON objects from a .jsonl file.\n",
    "    If max_items is set, stops after that many records (for debugging).\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "def write_jsonl(path, records):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c58533f-4884-4af5-bdd2-6c8bc471060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 — Orchestrator (loop inputs → Steps 1–4 → output)\n",
    "\n",
    "def run_pipeline(\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    max_items: int = 4,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = True,\n",
    "    few_shot_jsonl_path: str | None = None,   # pass as keyword in the call below\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the extraction pipeline.\n",
    "    Required: input_jsonl_path, ontology_json_path, output_jsonl_path.\n",
    "    Optional: max_items, max_new_tokens, temperature, verbose, few_shot_jsonl_path.\n",
    "    \"\"\"\n",
    "    # ✅ Load ontology once\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology = json.load(f)\n",
    "\n",
    "    # ✅ Load one-shot examples (if provided)\n",
    "    if few_shot_jsonl_path:\n",
    "        one_shot_by_id = load_one_shot_examples(few_shot_jsonl_path)\n",
    "    else:\n",
    "        one_shot_by_id = {}\n",
    "        print(\"[FEW-SHOT] No few_shot_jsonl_path provided — proceeding without examples.\")\n",
    "\n",
    "    # ✅ Setup model once\n",
    "    generator, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "    out_records = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ✅ Iterate inputs\n",
    "    for idx, item in enumerate(read_jsonl(input_jsonl_path, max_items=max_items), start=1):\n",
    "        sent_id = item.get(\"id\")\n",
    "        sent    = item.get(\"sent\", \"\")\n",
    "\n",
    "        # Match example by id\n",
    "        worked_example = one_shot_by_id.get(sent_id)\n",
    "        # Build prompt\n",
    "        prompt_text = build_reason_then_extract_prompt(\n",
    "            ontology_json=ontology,\n",
    "            test_sentence=sent,\n",
    "            worked_example=worked_example,\n",
    "            allow_light_norm=True,\n",
    "            relation_cues_text=None\n",
    "        )\n",
    "\n",
    "        print(\"prompt_text:\",prompt_text)\n",
    "\n",
    "\n",
    "        # Generate\n",
    "        t_gen0 = time.time()\n",
    "        model_output = generate_triples_text(\n",
    "            generator, tokenizer, prompt_text,\n",
    "            max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        gen_time = time.time() - t_gen0\n",
    "\n",
    "        # Extract & Parse\n",
    "        final_block = extract_final_triples_block(model_output)\n",
    "\n",
    "        triples = parse_triples_block(final_block)\n",
    "\n",
    "        out_records.append({\"id\": sent_id, \"sentence\": sent, \"triples\": triples})\n",
    "\n",
    "    # Write output\n",
    "    write_jsonl(output_jsonl_path, out_records)\n",
    "    print(f\"\\n✅ Done. Wrote {len(out_records)} lines to: {output_jsonl_path} | Total time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return out_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58b8662f-aaf7-4f62-ac06-13d8d2db2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 — Example Run (edit these paths)\n",
    "\n",
    "INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"     # your 3 sample lines as JSONL\n",
    "ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"         # your ontology JSON\n",
    "OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl\"             # where to write results\n",
    "FEW_SHOT_JSONL = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\"\n",
    "MAX_ITEMS = 1#None   # change to 20 (or None for full file) to control batches during debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f2f2ab7-f568-4e97-9553-83b5d385bef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEW-SHOT] Loaded 840 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1071933b4fb411dbb94295d14fe653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_text: TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - normalize only trivial cases (e.g., Japanese→Japan).\n",
      "    - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
      "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    - Avoid duplicates; apply light normalization only (e.g., Japanese→Japan).\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one triple per line format: predicate(\"Subject\",\"Object\")\n",
      "- No extra commentary after the triples. If no triples, just output the header.\n",
      "- Do NOT output JSON, code fences, or any other sections.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"Resident Evil: Damnation, known as Biohazard: Damnation ( , BaiohazÄdo: DamunÄshon) in Japan, is a 2012 Japanese adult animated biopunk horror action film by Capcom and Sony Pictures Entertainment Japan, directed by Makoto Kamiya and produced by Hiroyuki Kobayashi.\"\n",
      "\n",
      "\"Example FINAL TRIPLES\": [{\"sub\": \"Resident Evil: Damnation\", \"rel\": \"director\", \"obj\": \"Makoto Kamiya\"}]\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\"\n",
      "\n",
      "    Step 1 (Entities & types):\n",
      "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "\n",
      "✅ Done. Wrote 1 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl | Total time: 2.9s\n"
     ]
    }
   ],
   "source": [
    "out = run_pipeline(\n",
    "    input_jsonl_path=INPUT_JSONL,\n",
    "    ontology_json_path=ONTOLOGY_JSON,\n",
    "    output_jsonl_path=OUTPUT_JSONL,\n",
    "    max_items=MAX_ITEMS,\n",
    "    max_new_tokens=768,\n",
    "    temperature=0.25,\n",
    "    verbose=True,\n",
    "    few_shot_jsonl_path=FEW_SHOT_JSONL,  # <-- pass by keyword\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539340-9028-461a-b543-6448bfb359e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101b42f-5f45-4f59-9fad-fd18f41d792d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ea0092-8e32-455e-9418-c736677bc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re\n",
    "\n",
    "# # ---- Fixed base paths (unchanged) ----\n",
    "# BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "# BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "# BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/\"\n",
    "# BASE_FEW   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/\"\n",
    "\n",
    "# # ---- The 19 filenames from your screenshot ----\n",
    "# FILENAMES = [\n",
    "#     \"ont_1_university_test.jsonl\",\n",
    "#     \"ont_10_comicscharacter_test.jsonl\",\n",
    "#     \"ont_11_meanoftransportation_test.jsonl\",\n",
    "#     \"ont_12_monument_test.jsonl\",\n",
    "#     \"ont_13_food_test.jsonl\",\n",
    "#     \"ont_14_writtenwork_test.jsonl\",\n",
    "#     \"ont_15_sportsteam_test.jsonl\",\n",
    "#     \"ont_16_city_test.jsonl\",\n",
    "#     \"ont_17_artist_test.jsonl\",\n",
    "#     \"ont_18_scientist_test.jsonl\",\n",
    "#     \"ont_19_film_test.jsonl\",\n",
    "#     \"ont_2_musicalwork_test.jsonl\",\n",
    "#     \"ont_3_airport_test.jsonl\",\n",
    "#     \"ont_4_building_test.jsonl\",\n",
    "#     \"ont_5_athlete_test.jsonl\",\n",
    "#     \"ont_6_politician_test.jsonl\",\n",
    "#     \"ont_7_company_test.jsonl\",\n",
    "#     \"ont_8_celestialbody_test.jsonl\",\n",
    "#     \"ont_9_astronaut_test.jsonl\",\n",
    "# ]\n",
    "\n",
    "# # ont_{index}_{category}_test.jsonl  ->  {index}_{category}_ontology.json, ont_{index}_{category}_few_shot.jsonl, ont_{index}_{category}_output.jsonl\n",
    "# PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "# def make_paths(filename: str):\n",
    "#     m = PATTERN.match(filename)\n",
    "#     if not m:\n",
    "#         raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "#     idx, cat = m.groups()\n",
    "\n",
    "#     input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "#     ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "#     few_shot_jsonl = os.path.join(BASE_FEW, f\"ont_{idx}_{cat}_few_shot.jsonl\")\n",
    "\n",
    "#     # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "#     out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "#     output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "#     return input_jsonl, ontology_json, few_shot_jsonl, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "# # ---- Run all files ----\n",
    "# for fname in FILENAMES:\n",
    "#     try:\n",
    "#         INPUT_JSONL, ONTOLOGY_JSON, FEW_SHOT_JSONL, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(f\"[RUN] {tag}\")\n",
    "#         print(\"INPUT :\", INPUT_JSONL)\n",
    "#         print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "#         print(\"FEWS  :\", FEW_SHOT_JSONL)\n",
    "#         print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "#         out = run_pipeline(\n",
    "#             input_jsonl_path=INPUT_JSONL,\n",
    "#             ontology_json_path=ONTOLOGY_JSON,\n",
    "#             output_jsonl_path=OUTPUT_JSONL,\n",
    "#             max_items=None,          # keep as-is; change if you want to limit\n",
    "#             max_new_tokens=768,\n",
    "#             temperature=0.25,\n",
    "#             verbose=True,\n",
    "#             few_shot_jsonl_path=FEW_SHOT_JSONL,\n",
    "#         )\n",
    "\n",
    "#         print(f\"[DONE] {tag}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497e6b7-c9f6-424d-976d-76d617176bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b749f01-d557-496f-8a2c-10e054f900be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] ont_2_music\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_2_music_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/2_music_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_2_music_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_2_music_output.jsonl\n",
      "[FEW-SHOT] Loaded 675 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_2_music_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71474089610b4fbc83ba780ca364ed45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 675 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_2_music_output.jsonl | Total time: 3147.6s\n",
      "[DONE] ont_2_music\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_3_sport\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_3_sport_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/3_sport_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_3_sport_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_3_sport_output.jsonl\n",
      "[FEW-SHOT] Loaded 487 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_3_sport_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d28e92d2f4044528354e1b003b9d83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 487 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_3_sport_output.jsonl | Total time: 2923.4s\n",
      "[DONE] ont_3_sport\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_4_book\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_4_book_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/4_book_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_4_book_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_4_book_output.jsonl\n",
      "[FEW-SHOT] Loaded 550 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_4_book_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07ddb214cab4758b5a70a8df242b47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 550 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_4_book_output.jsonl | Total time: 2707.0s\n",
      "[DONE] ont_4_book\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_5_military\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_5_military_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/5_military_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_5_military_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_5_military_output.jsonl\n",
      "[FEW-SHOT] Loaded 230 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_5_military_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cbe69f57934af191c4459fdc1a9fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 230 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_5_military_output.jsonl | Total time: 1132.4s\n",
      "[DONE] ont_5_military\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_6_computer\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_6_computer_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/6_computer_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_6_computer_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_6_computer_output.jsonl\n",
      "[FEW-SHOT] Loaded 230 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_6_computer_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de06815838b4e3e9c2e29d9e85deb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 230 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_6_computer_output.jsonl | Total time: 907.2s\n",
      "[DONE] ont_6_computer\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_7_space\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_7_space_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/7_space_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_7_space_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_7_space_output.jsonl\n",
      "[FEW-SHOT] Loaded 203 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_7_space_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad743bf18d14a488f2dab81a3aabd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 203 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_7_space_output.jsonl | Total time: 849.8s\n",
      "[DONE] ont_7_space\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_8_politics\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_8_politics_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/8_politics_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_8_politics_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_8_politics_output.jsonl\n",
      "[FEW-SHOT] Loaded 214 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_8_politics_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace866a09d674f4ba9722323205f68b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 214 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_8_politics_output.jsonl | Total time: 984.2s\n",
      "[DONE] ont_8_politics\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_9_nature\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_9_nature_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/9_nature_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_9_nature_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_9_nature_output.jsonl\n",
      "[FEW-SHOT] Loaded 474 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_9_nature_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f94cde352814477993833908d5d259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 474 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_9_nature_output.jsonl | Total time: 2541.0s\n",
      "[DONE] ont_9_nature\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_10_culture\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_10_culture_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/10_culture_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_10_culture_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_10_culture_output.jsonl\n",
      "[FEW-SHOT] Loaded 159 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_10_culture_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1e5a8adea941e59ada55838cdd2a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 159 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_10_culture_output.jsonl | Total time: 684.0s\n",
      "[DONE] ont_10_culture\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "\n",
    "# ---- Fixed base paths (unchanged) ----\n",
    "BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/\"\n",
    "BASE_FEW   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/\"\n",
    "\n",
    "# ---- The 19 filenames from your screenshot ----\n",
    "FILENAMES = [\n",
    "    #\"ont_1_movie_test.jsonl\",\n",
    "    \"ont_2_music_test.jsonl\",\n",
    "    \"ont_3_sport_test.jsonl\",\n",
    "    \"ont_4_book_test.jsonl\",\n",
    "    \"ont_5_military_test.jsonl\",\n",
    "    \"ont_6_computer_test.jsonl\",\n",
    "    \"ont_7_space_test.jsonl\",\n",
    "    \"ont_8_politics_test.jsonl\",\n",
    "    \"ont_9_nature_test.jsonl\",\n",
    "    \"ont_10_culture_test.jsonl\",\n",
    "]\n",
    "\n",
    "# ont_{index}_{category}_test.jsonl  ->  {index}_{category}_ontology.json, ont_{index}_{category}_few_shot.jsonl, ont_{index}_{category}_output.jsonl\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "    few_shot_jsonl = os.path.join(BASE_FEW, f\"ont_{idx}_{cat}_few_shot.jsonl\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    return input_jsonl, ontology_json, few_shot_jsonl, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "# ---- Run all files ----\n",
    "for fname in FILENAMES:\n",
    "    try:\n",
    "        INPUT_JSONL, ONTOLOGY_JSON, FEW_SHOT_JSONL, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[RUN] {tag}\")\n",
    "        print(\"INPUT :\", INPUT_JSONL)\n",
    "        print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "        print(\"FEWS  :\", FEW_SHOT_JSONL)\n",
    "        print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "        out = run_pipeline(\n",
    "            input_jsonl_path=INPUT_JSONL,\n",
    "            ontology_json_path=ONTOLOGY_JSON,\n",
    "            output_jsonl_path=OUTPUT_JSONL,\n",
    "            max_items=None,          # keep as-is; change if you want to limit\n",
    "            max_new_tokens=768,\n",
    "            temperature=0.25,\n",
    "            verbose=True,\n",
    "            few_shot_jsonl_path=FEW_SHOT_JSONL,\n",
    "        )\n",
    "\n",
    "        print(f\"[DONE] {tag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97a00f-d83e-4c17-8500-d3427f286d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG Pipeline (GPU)",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
