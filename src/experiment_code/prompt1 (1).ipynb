{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "726ce33b-2cb3-4ba3-94bd-6b8ebe936c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 21 16:52:25 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 79%   57C    P8             36W /  370W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bebe5f8-17eb-4dec-988a-f27ad8dfd80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (29385) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 29385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c435477d-ac85-4f5f-abe0-96066f989ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limbo\n",
      "/opt/miniforge3/envs/jupyterhub/bin/python\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!which python\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e17f40-73fa-4594-bc8d-5b788171a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4888db2-f3a1-46ca-94cb-5d7e0bf63c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6932b1d-72cb-4b26-98a5-7bf9a06ddbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a lookup that maps any known identifier to its human-readable label.\n",
    "    Keys include: concept['qid'], concept['id'], and concept['label'] (all as strings).\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            val = c.get(keyname)\n",
    "            if val is None:\n",
    "                continue\n",
    "            sval = str(val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _label_for(value: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"Return the human-readable label for a given value.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    sval = str(value).strip()\n",
    "    return cindex.get(sval, sval)\n",
    "\n",
    "\n",
    "def format_ontology_concepts(ontology_json: Dict[str, Any]) -> str:\n",
    "    labels: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        lab = str(c.get(\"label\", \"\")).strip()\n",
    "        if lab:\n",
    "            labels.append(lab)\n",
    "    return \", \".join(labels)\n",
    "\n",
    "\n",
    "def format_ontology_relations(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render as:\n",
    "      - director(film,human)\n",
    "      - country of origin(film,country)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a678f30-f3c2-4215-9858-7fe4c4a631e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p1_system() -> str:\n",
    "    return (\n",
    "        \"You are a KG triple proposer in a Tree-of-Thoughts loop. \"\n",
    "        \"First detect entity mentions and assign tentative ontology types. \"\n",
    "        \"Then, using those mentions, propose candidate triples that are valid under the ontology (domain→range). \"\n",
    "        \"Return only JSON.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_p1_user(TEXT: str, ONTO: Dict[str, Any], k: int) -> str:\n",
    "    return dedent(f'''\n",
    "    Task: From the text, 1) list detected mentions with tentative types, 2) propose up to k={k} candidate triples [subject, relation, object]. Use only relations whose domain/range match the types you inferred. For each triple, include confidence ∈ [0,1] and cite the exact supporting span(s).\n",
    "\n",
    "    Text\n",
    "    \"{_escape_multiline(TEXT)}\"\n",
    "\n",
    "    Ontology concepts\n",
    "    {format_ontology_concepts(ONTO)}\n",
    "\n",
    "    Ontology relations (domain → range)\n",
    "    {format_ontology_relations(ONTO)}\n",
    "\n",
    "    Output format (JSON only)\n",
    "    {{\n",
    "      \"mentions\": [\n",
    "        {{\"surface\": \"...\", \"type_candidates\": [\"ConceptA\",\"ConceptB\"], \"span\": [start,end]}}\n",
    "      ],\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,\n",
    "          \"support\": \"exact quote from text\",\n",
    "          \"notes\": \"why domain/range fits\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Constraints\n",
    "    Only output domain/range-valid triples.\n",
    "    Normalize dates to YYYY-MM-DD when possible.\n",
    "    If a pronoun is required, resolve it to the nearest valid antecedent and state that in notes.\n",
    "    Do not invent entities not in the text.\n",
    "    ''').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024c6ae6-618f-4d29-805c-a7abe503fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_json(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    system_text: str,\n",
    "    user_text: str,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    ") -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_text},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c28e7ce-0528-4025-9c78-858876fc8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def _extract_text(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"Return (text, key_used) from an input record.\"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "    # Fallback: choose the longest string-valued field\n",
    "    best_k, best_v = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_v):\n",
    "            best_k, best_v = k, v\n",
    "    return best_v, best_k\n",
    "\n",
    "\n",
    "def run_pipeline_prompt1(\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    max_items: Optional[int] = None,\n",
    "    max_new_tokens: int = 900,\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = False, #True,\n",
    "    k: int = 6,\n",
    "):\n",
    "    \"\"\"\n",
    "    RAW-only pipeline for Prompt 1 (no filtering / no validation).\n",
    "    Writes a JSONL where each line contains:\n",
    "      - id\n",
    "      - input text\n",
    "      - prompts (system/user)\n",
    "      - response: raw + parsed json\n",
    "    \"\"\"\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology_json = json.load(f)\n",
    "\n",
    "    items = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "    if verbose:\n",
    "        print(f\"[RUN] Loaded {len(items)} input items from {input_jsonl_path}\")\n",
    "\n",
    "    generator, tokenizer = setup_model()\n",
    "    outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for i, rec in enumerate(items, start=1):\n",
    "        rid = str(rec.get(\"id\") or f\"item_{i}\")\n",
    "        text, key_used = _extract_text(rec)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[RUN] === ID={rid} ===\")\n",
    "            print(f\"[INFO] text key: {key_used!r}\")\n",
    "\n",
    "        sys_prompt = build_p1_system()\n",
    "        usr_prompt = build_p1_user(text, ontology_json, k)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n==== [DEBUG] SYSTEM PROMPT ====\\n\", sys_prompt)\n",
    "            print(\"\\n==== [DEBUG] USER PROMPT ====\\n\", usr_prompt)\n",
    "\n",
    "        try:\n",
    "            raw = generate_raw_json(\n",
    "                generator,\n",
    "                tokenizer,\n",
    "                sys_prompt,\n",
    "                usr_prompt,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Generation failed for {rid}: {e}\")\n",
    "            raw = \"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n==== [DEBUG] RAW MODEL OUTPUT ====\\n\", raw)\n",
    "\n",
    "        parsed = None\n",
    "        if isinstance(raw, str) and raw.strip():\n",
    "            try:\n",
    "                parsed = json.loads(raw)\n",
    "            except Exception:\n",
    "                import re\n",
    "                m = re.search(r\"\\{[\\s\\S]*\\}\", raw)\n",
    "                if m:\n",
    "                    try:\n",
    "                        parsed = json.loads(m.group(0))\n",
    "                    except Exception:\n",
    "                        parsed = None\n",
    "\n",
    "        out_rec = {\n",
    "            \"id\": rid,\n",
    "            \"input\": text,\n",
    "            \"prompts\": {\n",
    "                \"system\": sys_prompt,\n",
    "                \"user\": usr_prompt,\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"text\": raw,\n",
    "                \"json\": parsed,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        outputs.append(out_rec)\n",
    "\n",
    "    write_jsonl(output_jsonl_path, outputs)\n",
    "    if verbose:\n",
    "        print(f\"\\n[RUN] Wrote {len(outputs)} records to {output_jsonl_path}\")\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b169ef04-7ac3-412b-aa10-d22465334771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "# INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "# OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "\n",
    "# MAX_ITEMS      = 3\n",
    "# MAX_NEW_TOKENS = 900\n",
    "# TEMPERATURE    = 0.25\n",
    "# VERBOSE        = True #True\n",
    "# K_CANDIDATES   = 6\n",
    "\n",
    "# first = next(read_jsonl(INPUT_JSONL, max_items=1), None)\n",
    "# if first is None:\n",
    "#     print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "# else:\n",
    "#     print(\"[DEBUG] First record keys:\", list(first.keys()))\n",
    "#     print(\" id:\", first.get(\"id\"))\n",
    "#     print(\n",
    "#         \" sent:\",\n",
    "#         (first.get(\"sent\") or \"\")[:160]\n",
    "#         + (\"...\" if first.get(\"sent\") and len(first[\"sent\"]) > 160 else \"\"),\n",
    "#     )\n",
    "\n",
    "# _ = run_pipeline_prompt1(\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     ontology_json_path=ONTOLOGY_JSON,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     max_new_tokens=MAX_NEW_TOKENS,\n",
    "#     temperature=TEMPERATURE,\n",
    "#     verbose=VERBOSE,\n",
    "#     k=K_CANDIDATES,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315286a-b8af-46af-90de-c42277d5a81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5dac15-fcf3-4298-a7ff-194d2ed1904f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ebc5d-9b5f-49b7-9d61-9bf8fc80810f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700ac76-ce1a-4925-9044-39891aa03583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e9907-288c-46da-82e1-c61dc6b19c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] ont_1_movie\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\n",
      "FEWS  : None\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67eb2b9dcefc4e8f9b44071cb27b8ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# It is for wikidata sets\n",
    "\n",
    "import os, re\n",
    "\n",
    "# ---- Fixed base paths (unchanged) ----\n",
    "BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/\"\n",
    "\n",
    "# ---- The 10 filenames ----\n",
    "FILENAMES = [\n",
    "    \"ont_1_movie_test.jsonl\",\n",
    "    \"ont_2_music_test.jsonl\",\n",
    "    \"ont_3_sport_test.jsonl\",\n",
    "    \"ont_4_book_test.jsonl\",\n",
    "    \"ont_5_military_test.jsonl\",\n",
    "    \"ont_6_computer_test.jsonl\",\n",
    "    \"ont_7_space_test.jsonl\",\n",
    "    \"ont_8_politics_test.jsonl\",\n",
    "    \"ont_9_nature_test.jsonl\",\n",
    "    \"ont_10_culture_test.jsonl\",\n",
    "]\n",
    "\n",
    "# ont_{index}_{category}_test.jsonl -> {index}_{category}_ontology.json, ont_{index}_{category}_output.jsonl\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "\n",
    "def make_paths(filename: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    # Return compatible tuple with the same unpacking style\n",
    "    return input_jsonl, ontology_json, None, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "\n",
    "# ---- Run all files ----\n",
    "for fname in FILENAMES:\n",
    "    try:\n",
    "        INPUT_JSONL, ONTOLOGY_JSON, FEW_SHOT_JSONL, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[RUN] {tag}\")\n",
    "        print(\"INPUT :\", INPUT_JSONL)\n",
    "        print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "        print(\"FEWS  :\", FEW_SHOT_JSONL)\n",
    "        print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "        out = run_pipeline_prompt1(\n",
    "            input_jsonl_path=INPUT_JSONL,\n",
    "            ontology_json_path=ONTOLOGY_JSON,\n",
    "            output_jsonl_path=OUTPUT_JSONL,\n",
    "            max_items=None,          # process full dataset\n",
    "            max_new_tokens=900,\n",
    "            temperature=0.25,\n",
    "            verbose=False,\n",
    "            k=6,\n",
    "        )\n",
    "\n",
    "        print(f\"[DONE] {tag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896ae64-76e9-4dde-847f-bd0d176f2f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d10f10c-7efa-48e1-8934-2020f9a19006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] ont_14_writtenwork\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_14_writtenwork_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/14_writtenwork_ontology.json\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_14_writtenwork_output.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42339ca919e46f2a042c7ef888d48a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mONTO  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, ONTOLOGY_JSON)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_JSONL)\n\u001b[0;32m---> 64\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_prompt1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_jsonl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43montology_json_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mONTOLOGY_JSON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_jsonl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# process all items\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# fixed value\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m, in \u001b[0;36mrun_pipeline_prompt1\u001b[0;34m(input_jsonl_path, ontology_json_path, output_jsonl_path, max_items, max_new_tokens, temperature, verbose, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== [DEBUG] USER PROMPT ====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, usr_prompt)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_raw_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43musr_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] Generation failed for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mgenerate_raw_json\u001b[0;34m(generator, tokenizer, system_text, user_text, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m      9\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_text},\n\u001b[1;32m     11\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_text},\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m formatted \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     14\u001b[0m     messages,\n\u001b[1;32m     15\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:302\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         )\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:400\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    398\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 400\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    403\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/utils.py:3576\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3573\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mto(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[0;32m-> 3576\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[1;32m   3579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/logits_process.py:88\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m         scores \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/logits_process.py:477\u001b[0m, in \u001b[0;36mTopPLogitsWarper.__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;129m@add_start_docstrings\u001b[39m(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor, scores: torch\u001b[38;5;241m.\u001b[39mFloatTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    476\u001b[0m     sorted_logits, sorted_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msort(scores, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 477\u001b[0m     cumulative_probs \u001b[38;5;241m=\u001b[39m \u001b[43msorted_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcumsum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\u001b[39;00m\n\u001b[1;32m    480\u001b[0m     sorted_indices_to_remove \u001b[38;5;241m=\u001b[39m cumulative_probs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_p)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this is for dbpedia data sets \n",
    "import os, re\n",
    "\n",
    "# ---- Fixed base paths (for DBpedia) ----\n",
    "BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/\"\n",
    "\n",
    "# ---- The 19 DBpedia filenames ----\n",
    "FILENAMES = [\n",
    "    #\"ont_12_monument_test.jsonl\",\n",
    "    #\"ont_1_university_test.jsonl\",\n",
    "    #\"ont_10_comicscharacter_test.jsonl\",\n",
    "    #\"ont_11_meanoftransportation_test.jsonl\",\n",
    "    #\"ont_13_food_test.jsonl\",\n",
    "    \"ont_14_writtenwork_test.jsonl\",\n",
    "    \"ont_15_sportsteam_test.jsonl\",\n",
    "    \"ont_16_city_test.jsonl\",\n",
    "    \"ont_17_artist_test.jsonl\",\n",
    "    \"ont_18_scientist_test.jsonl\",\n",
    "    \"ont_19_film_test.jsonl\",\n",
    "    \"ont_2_musicalwork_test.jsonl\",\n",
    "    \"ont_3_airport_test.jsonl\",\n",
    "    \"ont_4_building_test.jsonl\",\n",
    "    \"ont_5_athlete_test.jsonl\",\n",
    "    \"ont_6_politician_test.jsonl\",\n",
    "    \"ont_7_company_test.jsonl\",\n",
    "    \"ont_8_celestialbody_test.jsonl\",\n",
    "    \"ont_9_astronaut_test.jsonl\",\n",
    "]\n",
    "\n",
    "# ---- Regex pattern for filename parsing ----\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "\n",
    "def make_paths(filename: str):\n",
    "    \"\"\"Build full input/output/ontology paths.\"\"\"\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl → ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    return input_jsonl, ontology_json, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "\n",
    "# ---- Run all DBpedia files ----\n",
    "for fname in FILENAMES:\n",
    "    try:\n",
    "        INPUT_JSONL, ONTOLOGY_JSON, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"[RUN] {tag}\")\n",
    "        print(\"INPUT :\", INPUT_JSONL)\n",
    "        print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "        print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "        out = run_pipeline_prompt1(\n",
    "            input_jsonl_path=INPUT_JSONL,\n",
    "            ontology_json_path=ONTOLOGY_JSON,\n",
    "            output_jsonl_path=OUTPUT_JSONL,\n",
    "            max_items=None,          # process all items\n",
    "            max_new_tokens=900,      # fixed value\n",
    "            temperature=0.25,\n",
    "            verbose=False,\n",
    "            k=6\n",
    "        )\n",
    "\n",
    "        print(f\"[DONE] {tag}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cd389-6c88-44ce-b535-c76f09844b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
