{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922ee2a3-5a35-44b4-a2d6-caece5e0d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 28 22:29:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 71%   65C    P0            293W /  370W |   14406MiB /  24576MiB |     54%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    920395      C   ....conda/envs/kg_pipeline/bin/python3      14396MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0f611-a087-4189-81bf-5b4401a176cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 1306860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770f49a2-a0f8-40c1-9e68-a60218401786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "Executable: /upb/users/b/balram/profiles/unix/cs/.conda/envs/kg_pipeline/bin/python3\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4155b4b7-09b5-4fc5-b5c8-6add97eaa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0 — Imports & Config\n",
    "\n",
    "import os, json, re, time\n",
    "import torch\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9eba531-8734-4269-95a6-5cb6ca325dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True  # KV caching is fine.\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return generator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de81bde-9a09-40e9-8b19-f50ee2974b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "\n",
    "def _quote(v: object) -> str:\n",
    "    \"\"\"\n",
    "    Convert any value to a safe, double-quoted string for the legacy triple format.\n",
    "    Escapes backslashes and double quotes.\n",
    "    \"\"\"\n",
    "    s = str(v)\n",
    "    s = s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "    return f'\"{s}\"'\n",
    "\n",
    "def _coerce_triples_to_str(triples_val):\n",
    "    \"\"\"\n",
    "    Legacy helper (kept for compatibility): normalize triples into newline-separated\n",
    "    lines: rel(\"Subject\",\"Object\"). Not used in the new JSONL prompt, but harmless.\n",
    "    \"\"\"\n",
    "    if triples_val is None:\n",
    "        return \"\"\n",
    "\n",
    "    if isinstance(triples_val, str):\n",
    "        return triples_val.strip()\n",
    "\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, str) for x in triples_val):\n",
    "        return \"\\n\".join(x.strip() for x in triples_val if x and x.strip())\n",
    "\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, dict) for x in triples_val):\n",
    "        lines = []\n",
    "        for t in triples_val:\n",
    "            rel = (t.get(\"rel\") or \"\").strip()\n",
    "            sub = t.get(\"sub\")\n",
    "            obj = t.get(\"obj\")\n",
    "            if rel and (sub is not None) and (obj is not None):\n",
    "                lines.append(f'{rel}({_quote(sub)},{_quote(obj)})')\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    return str(triples_val).strip()\n",
    "\n",
    "\n",
    "def _to_jsonl_block_from_list(triples_list):\n",
    "    \"\"\"\n",
    "    Given a list[dict] of {\"sub\",\"rel\",\"obj\"}, return a JSONL string block.\n",
    "    \"\"\"\n",
    "    if not isinstance(triples_list, list):\n",
    "        return \"\"\n",
    "    lines = []\n",
    "    for t in triples_list:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "        sub = t.get(\"sub\")\n",
    "        rel = t.get(\"rel\")\n",
    "        obj = t.get(\"obj\")\n",
    "        if sub is None or rel is None or obj is None:\n",
    "            continue\n",
    "        lines.append(json.dumps({\"sub\": sub, \"rel\": rel, \"obj\": obj}, ensure_ascii=False))\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "838bbb8e-79ec-4ca3-8a7a-58c6cead4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Few-shot loader for FIXED schema (unchanged IO) ----------\n",
    "\n",
    "def read_jsonl(path, max_items: int | None = None):\n",
    "    \"\"\"\n",
    "    Yields JSON objects from a .jsonl file.\n",
    "    If max_items is set, stops after that many records.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path, records):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_one_shot_examples(few_shot_jsonl_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads {id: {\"example_sentence\": <str>, \"example_triples_output\": <any>}}.\n",
    "    Reads fields exactly:\n",
    "        \"Example sentence\"\n",
    "        \"Example triples output\" (or falls back to \"Example triples\").\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for rec in read_jsonl(few_shot_jsonl_path, max_items=None):\n",
    "        rid = rec.get(\"id\")\n",
    "        if rid is None:\n",
    "            continue\n",
    "        out[str(rid).strip()] = {\n",
    "            \"example_sentence\": rec.get(\"Example sentence\"),\n",
    "            \"example_triples_output\": rec.get(\"Example triples output\", rec.get(\"Example triples\")),\n",
    "        }\n",
    "    print(f\"[FEW-SHOT] Loaded {len(out)} examples from: {few_shot_jsonl_path}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d723cc12-aa2e-4a14-b933-06ca5068be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Prompt Builder (Reason → Verify → FINAL TRIPLES as JSONL)\n",
    "\n",
    "from textwrap import dedent\n",
    "import json\n",
    "\n",
    "# Helpers (strict access; fail fast if ontology keys are missing)\n",
    "def _concept_label(ontology_json, qid):\n",
    "    return next((c[\"label\"] for c in ontology_json[\"concepts\"] if c[\"qid\"] == qid), \"\")\n",
    "\n",
    "def format_ontology_concepts(ontology_json):\n",
    "    return \", \".join(c[\"label\"] for c in ontology_json[\"concepts\"])\n",
    "\n",
    "def format_ontology_relations(ontology_json):\n",
    "    lines = []\n",
    "    for r in ontology_json[\"relations\"]:\n",
    "        dom = _concept_label(ontology_json, r[\"domain\"])\n",
    "        rng = _concept_label(ontology_json, r[\"range\"])\n",
    "        lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_reason_then_extract_prompt(\n",
    "    ontology_json,\n",
    "    test_sentence,\n",
    "    worked_example=None,\n",
    "    allow_light_norm=True,\n",
    "    relation_cues_text=None,   # optional Step 0 paraphrase cues\n",
    "    strict_format=True,        # enforce exact output shape\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a prompt that ends with the header `### FINAL TRIPLES` and *no trailing guidance*,\n",
    "    so generations appear directly under the header in the required JSONL format.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts_line   = format_ontology_concepts(ontology_json)\n",
    "    relations_block = format_ontology_relations(ontology_json)\n",
    "    norm_note = (\n",
    "        \"normalize only trivial cases (e.g., Japanese→Japan)\"\n",
    "        if allow_light_norm else\n",
    "        \"copy spans verbatim; no normalization\"\n",
    "    )\n",
    "\n",
    "    # Optional Step 0 cues\n",
    "    step0_block = \"\"\n",
    "    if relation_cues_text:\n",
    "        step0_block = dedent(f\"\"\"\n",
    "        Step 0 (Paraphrase expansion for this sentence):\n",
    "        {relation_cues_text.strip()}\n",
    "        \"\"\").strip()\n",
    "\n",
    "    # Optional worked example — now rendered as JSONL beneath \"Example FINAL TRIPLES\"\n",
    "    example_block = \"\"\n",
    "    if worked_example:\n",
    "        ex_sent = worked_example.get(\"example_sentence\")\n",
    "        ex_out  = worked_example.get(\"example_triples_output\")\n",
    "        ex_sent_json = json.dumps(ex_sent, ensure_ascii=False)  # quoted sentence\n",
    "        # Convert example triples (often list[dict]) into JSONL text\n",
    "        if isinstance(ex_out, list) and all(isinstance(x, dict) for x in ex_out):\n",
    "            ex_out_jsonl = \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in ex_out)\n",
    "        else:\n",
    "            # If already a string block, keep as-is (user may have supplied JSONL text)\n",
    "            ex_out_jsonl = str(ex_out).strip() if ex_out is not None else \"\"\n",
    "\n",
    "        example_block = dedent(f'''\n",
    "        \"Example sentence\": {ex_sent_json}\n",
    "\n",
    "        \"Example FINAL TRIPLES\":\n",
    "        {ex_out_jsonl}\n",
    "        ''').strip()\n",
    "\n",
    "    # Always include a tiny multi-line FORMAT DEMO to bias the model to output >1 line.\n",
    "    # (This is format-only; not semantic guidance.)\n",
    "    format_demo = dedent(\"\"\"\n",
    "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
    "    {\"sub\":\"Subject 1\",\"rel\":\"<predicate label 1>\",\"obj\":\"<Object 1>\"}\n",
    "    {\"sub\":\"Subject 2\",\"rel\":\"<predicate label 2>\",\"obj\":\"<Object 2>\"}\n",
    "    {\"sub\":\"Subject 3\",\"rel\":\" <predicate label 3>\",\"obj\":\"<Object 3>\"}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # Strict output notes merged into RULES\n",
    "    strict_notes = \"\"\n",
    "    if strict_format:\n",
    "        strict_notes = dedent(\"\"\"\n",
    "        OUTPUT SHAPE (strict):\n",
    "        - After Step 2, output the header exactly:\n",
    "        ### FINAL TRIPLES\n",
    "        - Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
    "          {\"sub\": \"<Subject>\", \"rel\": \"<predicate label>\", \"obj\": \"<Object>\"}\n",
    "        - Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
    "        - Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
    "        - No extra commentary after the JSON lines. If no triples, just output the header.\n",
    "        \"\"\").strip()\n",
    "\n",
    "    rules = dedent(f\"\"\"\n",
    "    RULES:\n",
    "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
    "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
    "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "    - Base decisions ONLY on the TEST SENTENCE.\n",
    "    - Avoid duplicates; {norm_note}.\n",
    "    - For each distinct supported fact, write a separate JSON object on its own line.\n",
    "    {strict_notes}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # IMPORTANT: end the prompt with the header and NOTHING after it.\n",
    "    prompt = dedent(f\"\"\"\n",
    "    TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "    - Use the ontology to guide relation labeling when possible.\n",
    "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
    "\n",
    "    ONTOLOGY CONCEPTS:\n",
    "    {concepts_line}\n",
    "\n",
    "    ONTOLOGY RELATIONS (argument types):\n",
    "    {relations_block}\n",
    "\n",
    "    {rules}\n",
    "\n",
    "    {step0_block}\n",
    "\n",
    "    {example_block if example_block else \"\"}\n",
    "\n",
    "    {format_demo}\n",
    "\n",
    "    ### TEST SENTENCE\n",
    "    \"{test_sentence}\"\n",
    "\n",
    "    Step 1 (Entities & types):\n",
    "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "    ### FINAL TRIPLES\n",
    "    \"\"\").strip()\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de84163-bd30-47de-b58d-ccfa3f5e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Prompt Builder (Reason → Verify → FINAL TRIPLES as JSONL; domain-agnostic)\n",
    "\n",
    "from textwrap import dedent\n",
    "import json\n",
    "\n",
    "# ---------- Helpers (strict access; fail fast if ontology keys are missing) ----------\n",
    "\n",
    "def _concept_label(ontology_json, qid):\n",
    "    return next((c[\"label\"] for c in ontology_json[\"concepts\"] if c[\"qid\"] == qid), \"\")\n",
    "\n",
    "def format_ontology_concepts(ontology_json):\n",
    "    return \", \".join(c[\"label\"] for c in ontology_json[\"concepts\"])\n",
    "\n",
    "def format_ontology_relations(ontology_json):\n",
    "    lines = []\n",
    "    for r in ontology_json[\"relations\"]:\n",
    "        dom = _concept_label(ontology_json, r[\"domain\"])\n",
    "        rng = _concept_label(ontology_json, r[\"range\"])\n",
    "        lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _jsonl_lines_from_list(triples):\n",
    "    if isinstance(triples, list) and all(isinstance(x, dict) for x in triples):\n",
    "        return \"\\n\".join(json.dumps(x, ensure_ascii=False) for x in triples)\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- Prompt Builder ----------\n",
    "\n",
    "def build_reason_then_extract_prompt(\n",
    "    ontology_json,\n",
    "    test_sentence,\n",
    "    worked_example=None,\n",
    "    allow_light_norm=True,\n",
    "    relation_cues_text=None,   # optional Step 0 paraphrase cues\n",
    "    strict_format=True,        # enforce exact output shape\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a prompt that ends with the header `### FINAL TRIPLES` and *no trailing guidance*,\n",
    "    so generations appear directly under the header in JSON Lines (JSONL).\n",
    "    Domain-agnostic: uses whatever concepts/relations are provided by `ontology_json`.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts_line   = format_ontology_concepts(ontology_json)\n",
    "    relations_block = format_ontology_relations(ontology_json)\n",
    "    norm_note = (\n",
    "        \"apply only light normalization (e.g., demonyms→country names; unambiguous years → 'YYYY')\"\n",
    "        if allow_light_norm else\n",
    "        \"copy spans verbatim; no normalization\"\n",
    "    )\n",
    "\n",
    "    # Optional Step 0 cues\n",
    "    step0_block = \"\"\n",
    "    if relation_cues_text:\n",
    "        step0_block = dedent(f\"\"\"\n",
    "        Step 0 (Paraphrase expansion for this sentence):\n",
    "        {relation_cues_text.strip()}\n",
    "        \"\"\").strip()\n",
    "\n",
    "    # Worked example — render as JSONL beneath \"Example FINAL TRIPLES\" (whatever domain the user provides)\n",
    "    example_block = \"\"\n",
    "    if worked_example:\n",
    "        ex_sent = worked_example.get(\"example_sentence\")\n",
    "        ex_out  = worked_example.get(\"example_triples_output\")\n",
    "        ex_sent_json = json.dumps(ex_sent, ensure_ascii=False)\n",
    "\n",
    "        ex_out_jsonl = _jsonl_lines_from_list(ex_out)\n",
    "        if ex_out_jsonl is None:\n",
    "            ex_out_jsonl = (str(ex_out).strip() if ex_out is not None else \"\")\n",
    "\n",
    "        example_block = dedent(f'''\n",
    "        \"Example sentence\": {ex_sent_json}\n",
    "\n",
    "        \"Example FINAL TRIPLES\":\n",
    "        {ex_out_jsonl}\n",
    "        ''').strip()\n",
    "\n",
    "    # Domain-agnostic FORMAT DEMO (no ontology-specific labels; shows out-of-ontology case too)\n",
    "    format_demo = dedent(\"\"\"\n",
    "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
    "    {\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"Entity B\"}\n",
    "    {\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"2020\"}\n",
    "    {\"sub\":\"Entity A\",\"rel\":\"<phrase from sentence>\",\"obj\":\"Entity C\"}  # out-of-ontology relation allowed\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # Strict output notes merged into RULES\n",
    "    strict_notes = \"\"\n",
    "    if strict_format:\n",
    "        strict_notes = dedent(\"\"\"\n",
    "        OUTPUT SHAPE (strict):\n",
    "        - After Step 2, output the header exactly:\n",
    "        ### FINAL TRIPLES\n",
    "        - Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
    "          {\"sub\":\"<Subject>\",\"rel\":\"<predicate label>\",\"obj\":\"<Object>\"}\n",
    "        - Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
    "        - Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
    "        - No extra commentary after the JSON lines. If no triples, just output the header.\n",
    "        \"\"\").strip()\n",
    "\n",
    "    rules = dedent(f\"\"\"\n",
    "    RULES:\n",
    "    - Extract ALL SPO facts explicitly supported by the TEST SENTENCE.\n",
    "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence and argument types.\n",
    "    - If a supported SPO fact has NO matching ontology relation, STILL INCLUDE IT:\n",
    "      use a concise, lowercase predicate phrase taken directly from the trigger wording in the sentence\n",
    "      (e.g., \"announced by\", \"located near\", \"compatible with\", \"launched from\"). Do NOT invent facts.\n",
    "    - Use the provided ONTOLOGY CONCEPTS for typing when possible; if none fits, type as \"other\" internally (do not print types).\n",
    "    - {norm_note}; keep names/titles/IDs verbatim unless trivial normalization applies.\n",
    "    - Avoid duplicates (treat case/spacing variants as identical).\n",
    "    - Quote/mention evidence in Step 2 for each emitted triple (internal reasoning only; do not output the steps).\n",
    "    - Base decisions ONLY on the TEST SENTENCE.\n",
    "    {strict_notes}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # IMPORTANT: end the prompt with the header and NOTHING after it.\n",
    "    prompt = dedent(f\"\"\"\n",
    "    TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "    - Use the ontology to guide relation labeling when possible.\n",
    "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
    "\n",
    "    ONTOLOGY CONCEPTS:\n",
    "    {concepts_line}\n",
    "\n",
    "    ONTOLOGY RELATIONS (argument types):\n",
    "    {relations_block}\n",
    "\n",
    "    {rules}\n",
    "\n",
    "    {step0_block}\n",
    "\n",
    "    {example_block if example_block else \"\"}\n",
    "\n",
    "    {format_demo}\n",
    "\n",
    "    ### TEST SENTENCE\n",
    "    \"{test_sentence}\"\n",
    "\n",
    "    Step 1 (Entities & types — internal scratchpad, do not output):\n",
    "    - Identify candidate SUBJECT and OBJECT spans in the sentence.\n",
    "    - When possible, map each entity to a concept from ONTOLOGY CONCEPTS (via surface form and context).\n",
    "    - Keep spans verbatim except for light normalization noted above.\n",
    "    - Resolve simple coreference (e.g., pronouns or descriptors → the correct entity mention).\n",
    "    - Discard uncertain or implied facts.\n",
    "\n",
    "    Step 2 (Verify relations — internal scratchpad, do not output):\n",
    "    - For each (SUBJECT, OBJECT) pair, check for explicit trigger wording in the sentence.\n",
    "    - If a matching ONTOLOGY RELATION label fits the wording and argument types, use that label.\n",
    "    - Otherwise, if the relation is explicitly stated but not in ontology, emit a concise, lowercase phrase\n",
    "      copied from the trigger wording (e.g., \"compatible with\", \"announced by\", \"launched from\").\n",
    "    - Only include relations explicitly supported by the text; no inference.\n",
    "    - One supported fact → one JSON object line in FINAL TRIPLES.\n",
    "\n",
    "    ### FINAL TRIPLES\n",
    "    \"\"\").strip()\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06416d16-e681-49f6-a707-01b4a96faedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 — Single Inference (chat template; continuation-only)\n",
    "\n",
    "def generate_triples_text(generator, tokenizer, prompt_text: str,\n",
    "                          max_new_tokens: int = 768, temperature: float = 0.25) -> str:\n",
    "    \"\"\"\n",
    "    Calls the model once. Returns the generated continuation.\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c6622b-db13-4815-85bf-fa236ad2ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 — Extract & Parse `FINAL TRIPLES` (JSONL-first; legacy fallback)\n",
    "\n",
    "FINAL_HEADER_REGEX = r'^\\s*#{2,}\\s*FINAL\\s+TRIPLES\\s*$'  # matches ## or ### FINAL TRIPLES\n",
    "\n",
    "\n",
    "def extract_final_triples_block(model_output_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Grab everything after the '##/### FINAL TRIPLES' header.\n",
    "    \"\"\"\n",
    "    header = re.search(FINAL_HEADER_REGEX, model_output_text, re.IGNORECASE | re.MULTILINE)\n",
    "    if header:\n",
    "        start = header.end()\n",
    "        tail = model_output_text[start:].strip()\n",
    "        nxt = re.search(r'^\\s*#{2,}\\s+[A-Z].*$', tail, re.MULTILINE)\n",
    "        return tail[:nxt.start()].strip() if nxt else tail\n",
    "\n",
    "    # fallback: take everything and let the parser fish valid lines out\n",
    "    return model_output_text\n",
    "\n",
    "\n",
    "def _parse_triples_block_jsonl(block_text: str):\n",
    "    \"\"\"\n",
    "    NEW: Parse JSONL lines: each line is a JSON object with keys {sub, rel, obj}.\n",
    "    Ignore invalid lines. Return list[dict].\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "    for raw in block_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        # tolerate bullets before JSON (robustness)\n",
    "        if line[:1] in \"-*•\":\n",
    "            line = line[1:].strip()\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not isinstance(obj, dict):\n",
    "            continue\n",
    "        sub = obj.get(\"sub\")\n",
    "        rel = obj.get(\"rel\")\n",
    "        ob  = obj.get(\"obj\")\n",
    "        if isinstance(sub, str) and isinstance(rel, str) and isinstance(ob, (str, int, float)):\n",
    "            # stringify obj to keep consistent typing downstream\n",
    "            triples.append({\"sub\": sub, \"rel\": rel, \"obj\": str(ob)})\n",
    "    return triples\n",
    "\n",
    "\n",
    "def _parse_triples_block_legacy(block_text: str):\n",
    "    \"\"\"\n",
    "    Legacy fallback: parse lines like predicate(\"Subject\",\"Object\").\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "\n",
    "    line_re = re.compile(r\"\"\"\n",
    "    ^\\s*(?:[-*•]\\s*)?\n",
    "    (?P<rel>[A-Za-z][A-Za-z0-9_ ]*?)\n",
    "    \\s*\\(\\s*\n",
    "        (?:\n",
    "            [\"'](?P<sub_q>[^\"']+?)[\"']\n",
    "            | (?P<sub_u>[^,)\\n]+?)\n",
    "        )\n",
    "    \\s*,\\s*\n",
    "        (?:\n",
    "            [\"'](?P<obj_q>[^\"']+?)[\"']\n",
    "            | (?P<obj_u>[^)\\n]+?)\n",
    "        )\n",
    "    \\s*\\)\\s*\\.?\\s*$\n",
    "    \"\"\", re.VERBOSE)\n",
    "\n",
    "    for raw in block_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        m = line_re.match(line)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        rel = m.group(\"rel\").strip()\n",
    "        sub = (m.group(\"sub_q\") or m.group(\"sub_u\") or \"\").strip()\n",
    "        obj = (m.group(\"obj_q\") or m.group(\"obj_u\") or \"\").strip()\n",
    "        if not rel or not sub or not obj:\n",
    "            continue\n",
    "        triples.append({\"sub\": sub, \"rel\": rel, \"obj\": obj})\n",
    "    return triples\n",
    "\n",
    "\n",
    "def parse_triples_block(block_text: str):\n",
    "    \"\"\"\n",
    "    Main entry: prefer JSONL (new format). If none parsed, fall back to legacy.\n",
    "    Returns list[dict] with keys \"sub\",\"rel\",\"obj\" (all strings).\n",
    "    \"\"\"\n",
    "    triples = _parse_triples_block_jsonl(block_text)\n",
    "    if triples:\n",
    "        return triples\n",
    "    return _parse_triples_block_legacy(block_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d83fbb5-e161-4861-9bd1-b26eb0bed4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 — Orchestrator (loop inputs → Steps 1–4 → output)\n",
    "\n",
    "def run_pipeline(\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    max_items: int = 4,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = True,\n",
    "    few_shot_jsonl_path: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the extraction pipeline.\n",
    "    Required: input_jsonl_path, ontology_json_path, output_jsonl_path.\n",
    "    Optional: max_items, max_new_tokens, temperature, verbose, few_shot_jsonl_path.\n",
    "    \"\"\"\n",
    "    # Load ontology once\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology = json.load(f)\n",
    "\n",
    "    # Load one-shot examples (if provided)\n",
    "    if few_shot_jsonl_path:\n",
    "        one_shot_by_id = load_one_shot_examples(few_shot_jsonl_path)\n",
    "    else:\n",
    "        one_shot_by_id = {}\n",
    "        print(\"[FEW-SHOT] No few_shot_jsonl_path provided — proceeding without examples.\")\n",
    "\n",
    "    # Setup model once\n",
    "    generator, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "    out_records = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Iterate inputs\n",
    "    for idx, item in enumerate(read_jsonl(input_jsonl_path, max_items=max_items), start=1):\n",
    "        sent_id = item.get(\"id\")\n",
    "        sent    = item.get(\"sent\", \"\")\n",
    "\n",
    "        worked_example = one_shot_by_id.get(sent_id)\n",
    "\n",
    "        # Build prompt (ends at ### FINAL TRIPLES; expects JSONL after)\n",
    "        prompt_text = build_reason_then_extract_prompt(\n",
    "            ontology_json=ontology,\n",
    "            test_sentence=sent,\n",
    "            worked_example=worked_example,\n",
    "            allow_light_norm=True,\n",
    "            relation_cues_text=None\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"[{idx}] ID={sent_id}\")\n",
    "            print(\"prompt_text:\\n\", prompt_text)\n",
    "            pass\n",
    "\n",
    "        # Generate\n",
    "        t_gen0 = time.time()\n",
    "        model_output = generate_triples_text(\n",
    "            generator, tokenizer, prompt_text,\n",
    "            max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        gen_time = time.time() - t_gen0\n",
    "        if verbose:\n",
    "            print(\"[GEN OUTPUT]\\n\", model_output)\n",
    "            print(f\"[GEN TIME] {gen_time:.2f}s\")\n",
    "            pass\n",
    "\n",
    "        # Extract & Parse (JSONL-first, legacy fallback)\n",
    "        final_block = extract_final_triples_block(model_output)\n",
    "        triples = parse_triples_block(final_block)\n",
    "\n",
    "        out_records.append({\"id\": sent_id, \"sentence\": sent, \"triples\": triples})\n",
    "\n",
    "    # Write output\n",
    "    write_jsonl(output_jsonl_path, out_records)\n",
    "    print(f\"\\n✅ Done. Wrote {len(out_records)} lines to: {output_jsonl_path} | Total time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return out_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74e147ae-4a2a-4c7d-9234-ae25dfb6a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEW-SHOT] Loaded 840 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72063a547bdb4394b58f904542f56b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1] ID=ont_1_movie_test_1\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Extract ALL SPO facts explicitly supported by the TEST SENTENCE.\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence and argument types.\n",
      "    - If a supported SPO fact has NO matching ontology relation, STILL INCLUDE IT:\n",
      "      use a concise, lowercase predicate phrase taken directly from the trigger wording in the sentence\n",
      "      (e.g., \"announced by\", \"located near\", \"compatible with\", \"launched from\"). Do NOT invent facts.\n",
      "    - Use the provided ONTOLOGY CONCEPTS for typing when possible; if none fits, type as \"other\" internally (do not print types).\n",
      "    - apply only light normalization (e.g., demonyms→country names; unambiguous years → 'YYYY'); keep names/titles/IDs verbatim unless trivial normalization applies.\n",
      "    - Avoid duplicates (treat case/spacing variants as identical).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple (internal reasoning only; do not output the steps).\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\":\"<Subject>\",\"rel\":\"<predicate label>\",\"obj\":\"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"Resident Evil: Damnation, known as Biohazard: Damnation ( , BaiohazÄdo: DamunÄshon) in Japan, is a 2012 Japanese adult animated biopunk horror action film by Capcom and Sony Pictures Entertainment Japan, directed by Makoto Kamiya and produced by Hiroyuki Kobayashi.\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Resident Evil: Damnation\", \"rel\": \"director\", \"obj\": \"Makoto Kamiya\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"Entity B\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"2020\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<phrase from sentence>\",\"obj\":\"Entity C\"}  # out-of-ontology relation allowed\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\"\n",
      "\n",
      "    Step 1 (Entities & types — internal scratchpad, do not output):\n",
      "    - Identify candidate SUBJECT and OBJECT spans in the sentence.\n",
      "    - When possible, map each entity to a concept from ONTOLOGY CONCEPTS (via surface form and context).\n",
      "    - Keep spans verbatim except for light normalization noted above.\n",
      "    - Resolve simple coreference (e.g., pronouns or descriptors → the correct entity mention).\n",
      "    - Discard uncertain or implied facts.\n",
      "\n",
      "    Step 2 (Verify relations — internal scratchpad, do not output):\n",
      "    - For each (SUBJECT, OBJECT) pair, check for explicit trigger wording in the sentence.\n",
      "    - If a matching ONTOLOGY RELATION label fits the wording and argument types, use that label.\n",
      "    - Otherwise, if the relation is explicitly stated but not in ontology, emit a concise, lowercase phrase\n",
      "      copied from the trigger wording (e.g., \"compatible with\", \"announced by\", \"launched from\").\n",
      "    - Only include relations explicitly supported by the text; no inference.\n",
      "    - One supported fact → one JSON object line in FINAL TRIPLES.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "\n",
      "{\"sub\": \"Bleach: Hell Verse\", \"rel\": \"director\", \"obj\": \"Noriyuki Abe\"}\n",
      "[GEN TIME] 2.10s\n",
      "\n",
      "================================================================================\n",
      "[2] ID=ont_1_movie_test_2\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Extract ALL SPO facts explicitly supported by the TEST SENTENCE.\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence and argument types.\n",
      "    - If a supported SPO fact has NO matching ontology relation, STILL INCLUDE IT:\n",
      "      use a concise, lowercase predicate phrase taken directly from the trigger wording in the sentence\n",
      "      (e.g., \"announced by\", \"located near\", \"compatible with\", \"launched from\"). Do NOT invent facts.\n",
      "    - Use the provided ONTOLOGY CONCEPTS for typing when possible; if none fits, type as \"other\" internally (do not print types).\n",
      "    - apply only light normalization (e.g., demonyms→country names; unambiguous years → 'YYYY'); keep names/titles/IDs verbatim unless trivial normalization applies.\n",
      "    - Avoid duplicates (treat case/spacing variants as identical).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple (internal reasoning only; do not output the steps).\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\":\"<Subject>\",\"rel\":\"<predicate label>\",\"obj\":\"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"The Cat Concerto was produced by Fred Quimby and directed by William Hanna and Joseph Barbera, with musical supervision by Scott Bradley, and animation by Kenneth Muse, Ed Barge and Irven Spence and additional animation by Richard Bickenbach (uncredited).\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"The Cat Concerto\", \"rel\": \"screenwriter\", \"obj\": \"William Hanna\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"Entity B\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"2020\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<phrase from sentence>\",\"obj\":\"Entity C\"}  # out-of-ontology relation allowed\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Keyboard Cat's original form was a video originally made in 1984 by Charlie Schmidt of his cat Fatso seemingly playing a piano (though manipulated by Schmidt off-camera) to a cheery tune.\"\n",
      "\n",
      "    Step 1 (Entities & types — internal scratchpad, do not output):\n",
      "    - Identify candidate SUBJECT and OBJECT spans in the sentence.\n",
      "    - When possible, map each entity to a concept from ONTOLOGY CONCEPTS (via surface form and context).\n",
      "    - Keep spans verbatim except for light normalization noted above.\n",
      "    - Resolve simple coreference (e.g., pronouns or descriptors → the correct entity mention).\n",
      "    - Discard uncertain or implied facts.\n",
      "\n",
      "    Step 2 (Verify relations — internal scratchpad, do not output):\n",
      "    - For each (SUBJECT, OBJECT) pair, check for explicit trigger wording in the sentence.\n",
      "    - If a matching ONTOLOGY RELATION label fits the wording and argument types, use that label.\n",
      "    - Otherwise, if the relation is explicitly stated but not in ontology, emit a concise, lowercase phrase\n",
      "      copied from the trigger wording (e.g., \"compatible with\", \"announced by\", \"launched from\").\n",
      "    - Only include relations explicitly supported by the text; no inference.\n",
      "    - One supported fact → one JSON object line in FINAL TRIPLES.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"publication date\", \"obj\": \"1984\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"based on\", \"obj\": \"a video\"}\n",
      "{\"sub\": \"a video\", \"rel\": \"cast member\", \"obj\": \"Fatso\"}\n",
      "{\"sub\": \"a video\", \"rel\": \"director\", \"obj\": \"Charlie Schmidt\"}\n",
      "{\"sub\": \"a video\", \"rel\": \"musical supervision\", \"obj\": \"Charlie Schmidt\"}\n",
      "{\"sub\": \"a video\", \"rel\": \"narrative location\", \"obj\": \"off-camera\"}\n",
      "[GEN TIME] 7.77s\n",
      "\n",
      "================================================================================\n",
      "[3] ID=ont_1_movie_test_3\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Extract ALL SPO facts explicitly supported by the TEST SENTENCE.\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence and argument types.\n",
      "    - If a supported SPO fact has NO matching ontology relation, STILL INCLUDE IT:\n",
      "      use a concise, lowercase predicate phrase taken directly from the trigger wording in the sentence\n",
      "      (e.g., \"announced by\", \"located near\", \"compatible with\", \"launched from\"). Do NOT invent facts.\n",
      "    - Use the provided ONTOLOGY CONCEPTS for typing when possible; if none fits, type as \"other\" internally (do not print types).\n",
      "    - apply only light normalization (e.g., demonyms→country names; unambiguous years → 'YYYY'); keep names/titles/IDs verbatim unless trivial normalization applies.\n",
      "    - Avoid duplicates (treat case/spacing variants as identical).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple (internal reasoning only; do not output the steps).\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\":\"<Subject>\",\"rel\":\"<predicate label>\",\"obj\":\"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"series, and was directed by Hiroshi Negishi.\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Tenchi Forever! The Movie\", \"rel\": \"director\", \"obj\": \"Hiroshi Negishi\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"Entity B\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"2020\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<phrase from sentence>\",\"obj\":\"Entity C\"}  # out-of-ontology relation allowed\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"The series was directed by Mitsuko Kase (episodes 1-7) and Takashi Imanishi (episodes 8-13).\"\n",
      "\n",
      "    Step 1 (Entities & types — internal scratchpad, do not output):\n",
      "    - Identify candidate SUBJECT and OBJECT spans in the sentence.\n",
      "    - When possible, map each entity to a concept from ONTOLOGY CONCEPTS (via surface form and context).\n",
      "    - Keep spans verbatim except for light normalization noted above.\n",
      "    - Resolve simple coreference (e.g., pronouns or descriptors → the correct entity mention).\n",
      "    - Discard uncertain or implied facts.\n",
      "\n",
      "    Step 2 (Verify relations — internal scratchpad, do not output):\n",
      "    - For each (SUBJECT, OBJECT) pair, check for explicit trigger wording in the sentence.\n",
      "    - If a matching ONTOLOGY RELATION label fits the wording and argument types, use that label.\n",
      "    - Otherwise, if the relation is explicitly stated but not in ontology, emit a concise, lowercase phrase\n",
      "      copied from the trigger wording (e.g., \"compatible with\", \"announced by\", \"launched from\").\n",
      "    - Only include relations explicitly supported by the text; no inference.\n",
      "    - One supported fact → one JSON object line in FINAL TRIPLES.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "\n",
      "{\"sub\": \"The series\", \"rel\": \"director\", \"obj\": \"Mitsuko Kase\"}\n",
      "{\"sub\": \"The series\", \"rel\": \"director\", \"obj\": \"Takashi Imanishi\"}\n",
      "{\"sub\": \"episodes 1-7\", \"rel\": \"episode_range\", \"obj\": \"Mitsuko Kase\"}\n",
      "{\"sub\": \"episodes 8-13\", \"rel\": \"episode_range\", \"obj\": \"Takashi Imanishi\"}\n",
      "[GEN TIME] 6.23s\n",
      "\n",
      "================================================================================\n",
      "[4] ID=ont_1_movie_test_4\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Extract ALL SPO facts explicitly supported by the TEST SENTENCE.\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence and argument types.\n",
      "    - If a supported SPO fact has NO matching ontology relation, STILL INCLUDE IT:\n",
      "      use a concise, lowercase predicate phrase taken directly from the trigger wording in the sentence\n",
      "      (e.g., \"announced by\", \"located near\", \"compatible with\", \"launched from\"). Do NOT invent facts.\n",
      "    - Use the provided ONTOLOGY CONCEPTS for typing when possible; if none fits, type as \"other\" internally (do not print types).\n",
      "    - apply only light normalization (e.g., demonyms→country names; unambiguous years → 'YYYY'); keep names/titles/IDs verbatim unless trivial normalization applies.\n",
      "    - Avoid duplicates (treat case/spacing variants as identical).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple (internal reasoning only; do not output the steps).\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\":\"<Subject>\",\"rel\":\"<predicate label>\",\"obj\":\"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"Spirited Away was the co-recipient of the Golden Bear at the 2002 Berlin International Film Festival (shared with Bloody Sunday) and is in the top 10 on the British Film Institute's list of \\\"Top 50 films for children up to the age of 14\\\".\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"award_received\", \"obj\": \"Golden Bear\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"Entity B\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<ontology relation label>\",\"obj\":\"2020\"}\n",
      "{\"sub\":\"Entity A\",\"rel\":\"<phrase from sentence>\",\"obj\":\"Entity C\"}  # out-of-ontology relation allowed\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Spirited Away (Japanese: , Hepburn: Sen to Chihiro no Kamikakushi, \"Sen and Chihiros Spiriting Away\") is a 2001 Japanese animated fantasy film written and directed by Hayao Miyazaki, animated by Studio Ghibli for Tokuma Shoten, Nippon Television Network, Dentsu, Buena Vista Home Entertainment, Tohokushinsha Film and Mitsubishi, and distributed by Toho.\"\n",
      "\n",
      "    Step 1 (Entities & types — internal scratchpad, do not output):\n",
      "    - Identify candidate SUBJECT and OBJECT spans in the sentence.\n",
      "    - When possible, map each entity to a concept from ONTOLOGY CONCEPTS (via surface form and context).\n",
      "    - Keep spans verbatim except for light normalization noted above.\n",
      "    - Resolve simple coreference (e.g., pronouns or descriptors → the correct entity mention).\n",
      "    - Discard uncertain or implied facts.\n",
      "\n",
      "    Step 2 (Verify relations — internal scratchpad, do not output):\n",
      "    - For each (SUBJECT, OBJECT) pair, check for explicit trigger wording in the sentence.\n",
      "    - If a matching ONTOLOGY RELATION label fits the wording and argument types, use that label.\n",
      "    - Otherwise, if the relation is explicitly stated but not in ontology, emit a concise, lowercase phrase\n",
      "      copied from the trigger wording (e.g., \"compatible with\", \"announced by\", \"launched from\").\n",
      "    - Only include relations explicitly supported by the text; no inference.\n",
      "    - One supported fact → one JSON object line in FINAL TRIPLES.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"written_work\", \"obj\": \"Sen to Chihiro no Kamikakushi\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"director\", \"obj\": \"Hayao Miyazaki\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"animation_studio\", \"obj\": \"Studio Ghibli\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Tokuma Shoten\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Nippon Television Network\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Dentsu\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Buena Vista Home Entertainment\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Tohokushinsha Film\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"production_company\", \"obj\": \"Mitsubishi\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"distributor\", \"obj\": \"Toho\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"publication_date\", \"obj\": \"2001\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_genre\", \"obj\": \"animated fantasy\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_type\", \"obj\": \"Japanese\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"narrative_location\", \"obj\": \"Japan\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"filming_location\", \"obj\": \"Japan\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_production_financed_by\", \"obj\": \"Nippon Television Network\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_production_financed_by\", \"obj\": \"Dentsu\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_production_financed_by\", \"obj\": \"Buena Vista Home Entertainment\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_production_financed_by\", \"obj\": \"Tohokushinsha Film\"}\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"film_production_financed_by\", \"obj\": \"Mitsubishi\"}\n",
      "[GEN TIME] 28.72s\n",
      "\n",
      "✅ Done. Wrote 4 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl | Total time: 44.8s\n"
     ]
    }
   ],
   "source": [
    "# Block 7 — Example Run (paths unchanged)\n",
    "\n",
    "INPUT_JSONL    = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "ONTOLOGY_JSON  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "OUTPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "FEW_SHOT_JSONL = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\"\n",
    "MAX_ITEMS = 4  # or None\n",
    "\n",
    "out = run_pipeline(\n",
    "    input_jsonl_path=INPUT_JSONL,\n",
    "    ontology_json_path=ONTOLOGY_JSON,\n",
    "    output_jsonl_path=OUTPUT_JSONL,\n",
    "    max_items=MAX_ITEMS,\n",
    "    max_new_tokens=768,\n",
    "    temperature=0.25,\n",
    "    verbose=True,\n",
    "    few_shot_jsonl_path=FEW_SHOT_JSONL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08529b4-008a-4312-8017-14aece126094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded8fbd-2428-4448-8624-2d92256ec8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "144f0dc3-e90d-4ca8-b94e-a1041cc73cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] ont_12_monument\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_12_monument_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/12_monument_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_12_monument_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_12_monument_output.jsonl\n",
      "[FEW-SHOT] Loaded 19 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_12_monument_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d2f7b604634d3783eeefb734d9fa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 19 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_12_monument_output.jsonl | Total time: 116.7s\n",
      "[DONE] ont_12_monument\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_1_university\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_1_university_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/1_university_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_1_university_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_1_university_output.jsonl\n",
      "[FEW-SHOT] Loaded 71 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_1_university_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da7d90d4dec4ba3b0cc7342e714839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 71 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_1_university_output.jsonl | Total time: 512.5s\n",
      "[DONE] ont_1_university\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_10_comicscharacter\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_10_comicscharacter_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/10_comicscharacter_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_10_comicscharacter_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_10_comicscharacter_output.jsonl\n",
      "[FEW-SHOT] Loaded 36 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_10_comicscharacter_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937f9d3f474245d5ac2dbfeee04456f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 36 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_10_comicscharacter_output.jsonl | Total time: 134.8s\n",
      "[DONE] ont_10_comicscharacter\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_11_meanoftransportation\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_11_meanoftransportation_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/11_meanoftransportation_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_11_meanoftransportation_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_11_meanoftransportation_output.jsonl\n",
      "[FEW-SHOT] Loaded 92 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_11_meanoftransportation_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbec7f33fd74e4b982ce7cacef3b180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done. Wrote 92 lines to: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_11_meanoftransportation_output.jsonl | Total time: 407.5s\n",
      "[DONE] ont_11_meanoftransportation\n",
      "\n",
      "================================================================================\n",
      "[RUN] ont_13_food\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_13_food_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/13_food_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_13_food_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_13_food_output.jsonl\n",
      "[FEW-SHOT] Loaded 153 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_13_food_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f08539a597403897e3753315bed10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFEWS  :\u001b[39m\u001b[38;5;124m\"\u001b[39m, FEW_SHOT_JSONL)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT:\u001b[39m\u001b[38;5;124m\"\u001b[39m, OUTPUT_JSONL)\n\u001b[0;32m---> 63\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_jsonl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINPUT_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43montology_json_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mONTOLOGY_JSON\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_jsonl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOUTPUT_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# keep as-is; change if you want to limit\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfew_shot_jsonl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEW_SHOT_JSONL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[14], line 59\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(input_jsonl_path, ontology_json_path, output_jsonl_path, max_items, max_new_tokens, temperature, verbose, few_shot_jsonl_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[1;32m     58\u001b[0m t_gen0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 59\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_triples_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m gen_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_gen0\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m#print(\"[GEN OUTPUT]\\n\", model_output)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#print(f\"[GEN TIME] {gen_time:.2f}s\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mgenerate_triples_text\u001b[0;34m(generator, tokenizer, prompt_text, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m      8\u001b[0m chat \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      9\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a precise information-extraction model. Follow instructions carefully.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     10\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_text}\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     12\u001b[0m formatted \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(chat, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformatted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:302\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1431\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1424\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1425\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         )\n\u001b[1;32m   1429\u001b[0m     )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1438\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1437\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1438\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/base.py:1338\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1337\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1338\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1339\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:400\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    398\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 400\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    403\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/generation/utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3564\u001b[0m     outputs,\n\u001b[1;32m   3565\u001b[0m     model_kwargs,\n\u001b[1;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3567\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:690\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    686\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    687\u001b[0m )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:423\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    421\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 423\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:259\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    258\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 259\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    261\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/kg_pipeline/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:212\u001b[0m, in \u001b[0;36mMistralRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    210\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    211\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 212\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "\n",
    "# ---- Fixed base paths (unchanged) ----\n",
    "BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/\"\n",
    "BASE_FEW   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/\"\n",
    "\n",
    "# ---- The 19 filenames from your screenshot ----\n",
    "FILENAMES = [\n",
    "    \"ont_12_monument_test.jsonl\",\n",
    "    \"ont_1_university_test.jsonl\",\n",
    "    \"ont_10_comicscharacter_test.jsonl\",\n",
    "    \"ont_11_meanoftransportation_test.jsonl\",\n",
    "    \"ont_13_food_test.jsonl\",\n",
    "    \"ont_14_writtenwork_test.jsonl\",\n",
    "    \"ont_15_sportsteam_test.jsonl\",\n",
    "    \"ont_16_city_test.jsonl\",\n",
    "    \"ont_17_artist_test.jsonl\",\n",
    "    \"ont_18_scientist_test.jsonl\",\n",
    "    \"ont_19_film_test.jsonl\",\n",
    "    \"ont_2_musicalwork_test.jsonl\",\n",
    "    \"ont_3_airport_test.jsonl\",\n",
    "    \"ont_4_building_test.jsonl\",\n",
    "    \"ont_5_athlete_test.jsonl\",\n",
    "    \"ont_6_politician_test.jsonl\",\n",
    "    \"ont_7_company_test.jsonl\",\n",
    "    \"ont_8_celestialbody_test.jsonl\",\n",
    "    \"ont_9_astronaut_test.jsonl\",\n",
    "]\n",
    "\n",
    "# ont_{index}_{category}_test.jsonl  ->  {index}_{category}_ontology.json, ont_{index}_{category}_few_shot.jsonl, ont_{index}_{category}_output.jsonl\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "    few_shot_jsonl = os.path.join(BASE_FEW, f\"ont_{idx}_{cat}_few_shot.jsonl\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    return input_jsonl, ontology_json, few_shot_jsonl, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "# ---- Run all files ----\n",
    "for fname in FILENAMES:\n",
    "    try:\n",
    "        INPUT_JSONL, ONTOLOGY_JSON, FEW_SHOT_JSONL, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[RUN] {tag}\")\n",
    "        print(\"INPUT :\", INPUT_JSONL)\n",
    "        print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "        print(\"FEWS  :\", FEW_SHOT_JSONL)\n",
    "        print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "        out = run_pipeline(\n",
    "            input_jsonl_path=INPUT_JSONL,\n",
    "            ontology_json_path=ONTOLOGY_JSON,\n",
    "            output_jsonl_path=OUTPUT_JSONL,\n",
    "            max_items=None,          # keep as-is; change if you want to limit\n",
    "            max_new_tokens=768,\n",
    "            temperature=0.25,\n",
    "            verbose=True,\n",
    "            few_shot_jsonl_path=FEW_SHOT_JSONL,\n",
    "        )\n",
    "\n",
    "        print(f\"[DONE] {tag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5641150f-421c-487e-8eab-7dceace72750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6fc2d-0085-46ac-b236-df1664e21faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] ont_1_movie\n",
      "INPUT : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\n",
      "ONTO  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\n",
      "FEWS  : /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\n",
      "OUTPUT: /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output.jsonl\n",
      "[FEW-SHOT] Loaded 840 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/ont_1_movie_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9c87067122425aa33bd7818b715c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[1] ID=ont_1_movie_test_1\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
      "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    - Avoid duplicates; normalize only trivial cases (e.g., Japanese→Japan).\n",
      "    - For each distinct supported fact, write a separate JSON object on its own line.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\": \"<Subject>\", \"rel\": \"<predicate label>\", \"obj\": \"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"Resident Evil: Damnation, known as Biohazard: Damnation ( , BaiohazÄdo: DamunÄshon) in Japan, is a 2012 Japanese adult animated biopunk horror action film by Capcom and Sony Pictures Entertainment Japan, directed by Makoto Kamiya and produced by Hiroyuki Kobayashi.\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Resident Evil: Damnation\", \"rel\": \"director\", \"obj\": \"Makoto Kamiya\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Subject 1\",\"rel\":\"<predicate label 1>\",\"obj\":\"<Object 1>\"}\n",
      "{\"sub\":\"Subject 2\",\"rel\":\"<predicate label 2>\",\"obj\":\"<Object 2>\"}\n",
      "{\"sub\":\"Subject 3\",\"rel\":\" <predicate label 3>\",\"obj\":\"<Object 3>\"}\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\"\n",
      "\n",
      "    Step 1 (Entities & types):\n",
      "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "{\"sub\": \"Bleach: Hell Verse\", \"rel\": \"director\", \"obj\": \"Noriyuki Abe\"}\n",
      "[GEN TIME] 1.98s\n",
      "\n",
      "================================================================================\n",
      "[2] ID=ont_1_movie_test_2\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
      "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    - Avoid duplicates; normalize only trivial cases (e.g., Japanese→Japan).\n",
      "    - For each distinct supported fact, write a separate JSON object on its own line.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\": \"<Subject>\", \"rel\": \"<predicate label>\", \"obj\": \"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"The Cat Concerto was produced by Fred Quimby and directed by William Hanna and Joseph Barbera, with musical supervision by Scott Bradley, and animation by Kenneth Muse, Ed Barge and Irven Spence and additional animation by Richard Bickenbach (uncredited).\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"The Cat Concerto\", \"rel\": \"screenwriter\", \"obj\": \"William Hanna\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Subject 1\",\"rel\":\"<predicate label 1>\",\"obj\":\"<Object 1>\"}\n",
      "{\"sub\":\"Subject 2\",\"rel\":\"<predicate label 2>\",\"obj\":\"<Object 2>\"}\n",
      "{\"sub\":\"Subject 3\",\"rel\":\" <predicate label 3>\",\"obj\":\"<Object 3>\"}\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Keyboard Cat's original form was a video originally made in 1984 by Charlie Schmidt of his cat Fatso seemingly playing a piano (though manipulated by Schmidt off-camera) to a cheery tune.\"\n",
      "\n",
      "    Step 1 (Entities & types):\n",
      "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"based on\", \"obj\": \"a video\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"created by\", \"obj\": \"Charlie Schmidt\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"narrative location\", \"obj\": \"Charlie Schmidt's home\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"filming location\", \"obj\": \"Charlie Schmidt's home\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"cast member\", \"obj\": \"Fatso\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"music\", \"obj\": \"a cheery tune\"}\n",
      "{\"sub\": \"Keyboard Cat's original form\", \"rel\": \"manipulated by\", \"obj\": \"Charlie Schmidt\"}\n",
      "[GEN TIME] 10.22s\n",
      "\n",
      "================================================================================\n",
      "[3] ID=ont_1_movie_test_3\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
      "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    - Avoid duplicates; normalize only trivial cases (e.g., Japanese→Japan).\n",
      "    - For each distinct supported fact, write a separate JSON object on its own line.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\": \"<Subject>\", \"rel\": \"<predicate label>\", \"obj\": \"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"series, and was directed by Hiroshi Negishi.\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Tenchi Forever! The Movie\", \"rel\": \"director\", \"obj\": \"Hiroshi Negishi\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Subject 1\",\"rel\":\"<predicate label 1>\",\"obj\":\"<Object 1>\"}\n",
      "{\"sub\":\"Subject 2\",\"rel\":\"<predicate label 2>\",\"obj\":\"<Object 2>\"}\n",
      "{\"sub\":\"Subject 3\",\"rel\":\" <predicate label 3>\",\"obj\":\"<Object 3>\"}\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"The series was directed by Mitsuko Kase (episodes 1-7) and Takashi Imanishi (episodes 8-13).\"\n",
      "\n",
      "    Step 1 (Entities & types):\n",
      "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
      "\n",
      "    ### FINAL TRIPLES\n",
      "[GEN OUTPUT]\n",
      "  ### FINAL TRIPLES\n",
      "{\"sub\": \"The series\", \"rel\": \"director\", \"obj\": \"Mitsuko Kase\"}\n",
      "{\"sub\": \"The series\", \"rel\": \"director\", \"obj\": \"Takashi Imanishi\"}\n",
      "{\"sub\": \"Episodes 1-7\", \"rel\": \"director\", \"obj\": \"Mitsuko Kase\"}\n",
      "{\"sub\": \"Episodes 8-13\", \"rel\": \"director\", \"obj\": \"Takashi Imanishi\"}\n",
      "[GEN TIME] 5.93s\n",
      "\n",
      "================================================================================\n",
      "[4] ID=ont_1_movie_test_4\n",
      "prompt_text:\n",
      " TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - If a valid SPO fact has no matching ontology label, still output it in the same JSONL under FINAL TRIPLES (single section, multiple lines).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    RULES:\n",
      "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
      "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
      "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
      "    - Base decisions ONLY on the TEST SENTENCE.\n",
      "    - Avoid duplicates; normalize only trivial cases (e.g., Japanese→Japan).\n",
      "    - For each distinct supported fact, write a separate JSON object on its own line.\n",
      "    OUTPUT SHAPE (strict):\n",
      "- After Step 2, output the header exactly:\n",
      "### FINAL TRIPLES\n",
      "- Then output one JSON object per line (JSON Lines), each with exactly these keys:\n",
      "  {\"sub\": \"<Subject>\", \"rel\": \"<predicate label>\", \"obj\": \"<Object>\"}\n",
      "- Output ALL supported facts from the TEST SENTENCE; do NOT stop after the first triple.\n",
      "- Do not output brackets, arrays, bullets, or code fences. Only raw JSON lines.\n",
      "- No extra commentary after the JSON lines. If no triples, just output the header.\n",
      "\n",
      "\n",
      "\n",
      "    \"Example sentence\": \"Spirited Away was the co-recipient of the Golden Bear at the 2002 Berlin International Film Festival (shared with Bloody Sunday) and is in the top 10 on the British Film Institute's list of \\\"Top 50 films for children up to the age of 14\\\".\"\n",
      "\n",
      "\"Example FINAL TRIPLES\":\n",
      "{\"sub\": \"Spirited Away\", \"rel\": \"award_received\", \"obj\": \"Golden Bear\"}\n",
      "\n",
      "    OUTPUT FORMAT DEMO (JSON Lines, not part of reasoning):\n",
      "{\"sub\":\"Subject 1\",\"rel\":\"<predicate label 1>\",\"obj\":\"<Object 1>\"}\n",
      "{\"sub\":\"Subject 2\",\"rel\":\"<predicate label 2>\",\"obj\":\"<Object 2>\"}\n",
      "{\"sub\":\"Subject 3\",\"rel\":\" <predicate label 3>\",\"obj\":\"<Object 3>\"}\n",
      "\n",
      "    ### TEST SENTENCE\n",
      "    \"Spirited Away (Japanese: , Hepburn: Sen to Chihiro no Kamikakushi, \"Sen and Chihiros Spiriting Away\") is a 2001 Japanese animated fantasy film written and directed by Hayao Miyazaki, animated by Studio Ghibli for Tokuma Shoten, Nippon Television Network, Dentsu, Buena Vista Home Entertainment, Tohokushinsha Film and Mitsubishi, and distributed by Toho.\"\n",
      "\n",
      "    Step 1 (Entities & types):\n",
      "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
      "\n",
      "    ### FINAL TRIPLES\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "\n",
    "# ---- Fixed base paths (unchanged) ----\n",
    "BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/\"\n",
    "BASE_FEW   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/wikidata/\"\n",
    "\n",
    "# ---- The 19 filenames from your screenshot ----\n",
    "FILENAMES = [\n",
    "    \"ont_1_movie_test.jsonl\",\n",
    "    \"ont_2_music_test.jsonl\",\n",
    "    \"ont_3_sport_test.jsonl\",\n",
    "    \"ont_4_book_test.jsonl\",\n",
    "    \"ont_5_military_test.jsonl\",\n",
    "    \"ont_6_computer_test.jsonl\",\n",
    "    \"ont_7_space_test.jsonl\",\n",
    "    \"ont_8_politics_test.jsonl\",\n",
    "    \"ont_9_nature_test.jsonl\",\n",
    "    \"ont_10_culture_test.jsonl\",\n",
    "]\n",
    "\n",
    "# ont_{index}_{category}_test.jsonl  ->  {index}_{category}_ontology.json, ont_{index}_{category}_few_shot.jsonl, ont_{index}_{category}_output.jsonl\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "    few_shot_jsonl = os.path.join(BASE_FEW, f\"ont_{idx}_{cat}_few_shot.jsonl\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    return input_jsonl, ontology_json, few_shot_jsonl, output_jsonl, f\"ont_{idx}_{cat}\"\n",
    "\n",
    "# ---- Run all files ----\n",
    "for fname in FILENAMES:\n",
    "    try:\n",
    "        INPUT_JSONL, ONTOLOGY_JSON, FEW_SHOT_JSONL, OUTPUT_JSONL, tag = make_paths(fname)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"[RUN] {tag}\")\n",
    "        print(\"INPUT :\", INPUT_JSONL)\n",
    "        print(\"ONTO  :\", ONTOLOGY_JSON)\n",
    "        print(\"FEWS  :\", FEW_SHOT_JSONL)\n",
    "        print(\"OUTPUT:\", OUTPUT_JSONL)\n",
    "\n",
    "        out = run_pipeline(\n",
    "            input_jsonl_path=INPUT_JSONL,\n",
    "            ontology_json_path=ONTOLOGY_JSON,\n",
    "            output_jsonl_path=OUTPUT_JSONL,\n",
    "            max_items=None,          # keep as-is; change if you want to limit\n",
    "            max_new_tokens=768,\n",
    "            temperature=0.25,\n",
    "            verbose=True,\n",
    "            few_shot_jsonl_path=FEW_SHOT_JSONL,\n",
    "        )\n",
    "\n",
    "        print(f\"[DONE] {tag}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d946c-45fe-4e57-9bb4-ad26d6f81659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec02b4e-ad34-421b-8f61-2a832da978f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44869c2e-4b50-4484-ab09-55ef53cacec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e77c9-db1a-402e-a193-140d5d4841d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539340-9028-461a-b543-6448bfb359e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
