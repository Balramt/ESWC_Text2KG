{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5678a-4d4c-4bd3-8903-b4ef9fbbe4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c04de57-57ca-4606-8479-cc5eadc0ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  7 17:47:03 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8             31W /  370W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38324f-ffff-4417-9de5-16b9652b1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 584175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f488284-319c-4221-b2b2-eb45dd31f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/upb/users/b/balram/profiles/unix/cs/.conda/envs/gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x7fb212a47f10>\n"
     ]
    }
   ],
   "source": [
    "# Block 1Ô∏è‚É£ ‚Äì Model Setup\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load Mistral-7B-Instruct and prepare a text-generation pipeline.\n",
    "    \"\"\"\n",
    "    torch.backends.cudnn.benchmark = True  # GPU speed-up\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = False  # disable cache for long prompts\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    return generator, tokenizer\n",
    "\n",
    "# Initialize model & tokenizer\n",
    "text_pipe, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "# quick sanity check\n",
    "print(text_pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5539a-60c8-4964-8329-b9ce4097fe37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb252ec-605e-4aad-a2ff-ac398787a4c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d310-97b3-4d4f-804b-4a2c52cc3513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e15310d-45f4-44e3-82e6-3ffcfd70f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
      "    - Use the ontology to guide relation labeling when possible.\n",
      "    - normalize only trivial cases (e.g., Japanese‚ÜíJapan).\n",
      "    - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
      "\n",
      "    ONTOLOGY CONCEPTS:\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    ONTOLOGY RELATIONS (argument types):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "\n",
      "RULES:\n",
      "- Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
      "- If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sent\n"
     ]
    }
   ],
   "source": [
    "# Block 2Ô∏è‚É£ ‚Äì Prompt Creation (Reason ‚Üí Verify ‚Üí Final Triples; single block incl. ontology + extra)\n",
    "\n",
    "from textwrap import dedent\n",
    "\n",
    "def _concept_label(ontology_json, qid):\n",
    "    return next((c[\"label\"] for c in ontology_json[\"concepts\"] if c[\"qid\"] == qid), \"\")\n",
    "\n",
    "def format_ontology_concepts(ontology_json):\n",
    "    return \", \".join(c[\"label\"] for c in ontology_json[\"concepts\"])\n",
    "\n",
    "def format_ontology_relations(ontology_json):\n",
    "    lines = []\n",
    "    for r in ontology_json[\"relations\"]:\n",
    "        dom = _concept_label(ontology_json, r[\"domain\"])\n",
    "        rng = _concept_label(ontology_json, r[\"range\"])\n",
    "        lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_reason_then_extract_prompt(\n",
    "    ontology_json,\n",
    "    test_sentence,\n",
    "    worked_example=None,\n",
    "    allow_light_norm=True,\n",
    "    relation_cues_text=None  # optional Step 0 paraphrase cues (can be left None)\n",
    "):\n",
    "    concepts_line  = format_ontology_concepts(ontology_json)\n",
    "    relations_block = format_ontology_relations(ontology_json)\n",
    "    norm_note = \"normalize only trivial cases (e.g., Japanese‚ÜíJapan)\" if allow_light_norm else \"copy spans verbatim; no normalization\"\n",
    "\n",
    "    step0_block = \"\"\n",
    "    if relation_cues_text:\n",
    "        step0_block = dedent(f\"\"\"\n",
    "        Step 0 (Paraphrase expansion for this sentence):\n",
    "        {relation_cues_text.strip()}\n",
    "        \"\"\")\n",
    "\n",
    "    example_block = \"\"\n",
    "    if worked_example:\n",
    "        example_block = dedent(f\"\"\"\n",
    "        ### WORKED EXAMPLE\n",
    "        Sentence:\n",
    "        {worked_example[\"sentence\"]}\n",
    "\n",
    "        Step 1 (Entities & types):\n",
    "        {worked_example[\"step1\"]}\n",
    "\n",
    "        Step 2 (Verify relations):\n",
    "        {worked_example[\"step2\"]}\n",
    "\n",
    "        ### FINAL TRIPLES\n",
    "        {worked_example[\"final_triples\"].strip()}\n",
    "        \"\"\").strip()\n",
    "\n",
    "    rules = dedent(\"\"\"\n",
    "    RULES:\n",
    "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
    "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" ‚Üí the film title in this sentence).\n",
    "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "    - Output one triple per line, format: predicate(\"Subject\",\"Object\").\n",
    "    - Base decisions ONLY on the TEST SENTENCE.\n",
    "    - Avoid duplicates; apply light normalization only (e.g., Japanese‚ÜíJapan).\n",
    "    \"\"\")\n",
    "\n",
    "    prompt = dedent(f\"\"\"\n",
    "    TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "    - Use the ontology to guide relation labeling when possible.\n",
    "    - {norm_note}.\n",
    "    - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
    "\n",
    "    ONTOLOGY CONCEPTS:\n",
    "    {concepts_line}\n",
    "\n",
    "    ONTOLOGY RELATIONS (argument types):\n",
    "    {relations_block}\n",
    "\n",
    "    {rules}\n",
    "\n",
    "    {step0_block}\n",
    "\n",
    "    {example_block if example_block else \"\"}\n",
    "\n",
    "    ### TEST SENTENCE\n",
    "    \"{test_sentence}\"\n",
    "\n",
    "    Step 1 (Entities & types):\n",
    "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "    ### FINAL TRIPLES\n",
    "    # one triple per line; include ontology-mapped AND extra sentence-backed relations\n",
    "    \"\"\").strip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# --- Example ontology + text ---------------------------------------\n",
    "\n",
    "input_text = \"\"\"The Life on the Earth also known as Tracing the Gray Summer, is a 2001 Japanese anime drama film by Satoshi Dezaki, a joint production between the Magic Bus and the GoGo Visual Planning which recounts the true story of the Seveso disaster, a chemical incident occurred over the Italian town of Seveso in 1976, which is still considered one of the worst ecological disasters in history.\"\"\"\n",
    "\n",
    "ontology = {\n",
    "    \"title\": \"Movie Ontology\",\n",
    "    \"id\": \"ont_1_movie\",\n",
    "    \"concepts\": [\n",
    "        {\"qid\": \"Q5\", \"label\": \"human\"},\n",
    "        {\"qid\": \"Q515\", \"label\": \"city\"},\n",
    "        {\"qid\": \"Q6256\", \"label\": \"country\"},\n",
    "        {\"qid\": \"Q11424\", \"label\": \"film\"},\n",
    "        {\"qid\": \"Q201658\", \"label\": \"film genre\"},\n",
    "        {\"qid\": \"Q483394\", \"label\": \"genre\"},\n",
    "        {\"qid\": \"Q1762059\", \"label\": \"film production company\"},\n",
    "        {\"qid\": \"Q4220917\", \"label\": \"film award\"},\n",
    "        {\"qid\": \"Q618779\", \"label\": \"award\"},\n",
    "        {\"qid\": \"Q47461344\", \"label\": \"written work\"},\n",
    "        {\"qid\": \"Q15773347\", \"label\": \"film character\"},\n",
    "        {\"qid\": \"Q104649845\", \"label\": \"film organization\"}\n",
    "    ],\n",
    "    \"relations\": [\n",
    "        {\"pid\": \"P57\", \"label\": \"director\", \"domain\": \"Q11424\", \"range\": \"Q5\"},\n",
    "        {\"pid\": \"P58\", \"label\": \"screenwriter\", \"domain\": \"Q11424\", \"range\": \"Q5\"},\n",
    "        {\"pid\": \"P136\", \"label\": \"genre\", \"domain\": \"Q11424\", \"range\": \"Q483394\"},\n",
    "        {\"pid\": \"P144\", \"label\": \"based on\", \"domain\": \"Q11424\", \"range\": \"Q47461344\"},\n",
    "        {\"pid\": \"P161\", \"label\": \"cast member\", \"domain\": \"Q11424\", \"range\": \"Q5\"},\n",
    "        {\"pid\": \"P166\", \"label\": \"award received\", \"domain\": \"Q11424\", \"range\": \"Q618779\"},\n",
    "        {\"pid\": \"P272\", \"label\": \"production company\", \"domain\": \"Q11424\", \"range\": \"Q1762059\"},\n",
    "        {\"pid\": \"P495\", \"label\": \"country of origin\", \"domain\": \"Q11424\", \"range\": \"Q6256\"},\n",
    "        {\"pid\": \"P577\", \"label\": \"publication date\", \"domain\": \"Q11424\", \"range\": \"\"},\n",
    "        {\"pid\": \"P674\", \"label\": \"characters\", \"domain\": \"Q11424\", \"range\": \"Q15773347\"},\n",
    "        {\"pid\": \"P840\", \"label\": \"narrative location\", \"domain\": \"Q11424\", \"range\": \"Q515\"},\n",
    "        {\"pid\": \"P915\", \"label\": \"filming location\", \"domain\": \"Q11424\", \"range\": \"Q515\"},\n",
    "        {\"pid\": \"P921\", \"label\": \"main subject\", \"domain\": \"Q11424\", \"range\": \"\"},\n",
    "        {\"pid\": \"P1411\", \"label\": \"nominated for\", \"domain\": \"Q11424\", \"range\": \"Q618779\"},\n",
    "        {\"pid\": \"P2130\", \"label\": \"cost\", \"domain\": \"Q11424\", \"range\": \"\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "worked_example = {\n",
    "    \"sentence\": '\"Resident Evil: Damnation is a 2012 Japanese animated film directed by Makoto Kamiya.\"',\n",
    "    \"step1\": \"- Resident Evil: Damnation ‚Üí film\\n- Makoto Kamiya ‚Üí human\\n- 2012 ‚Üí year\\n- Japanese ‚Üí country ‚Üí Japan\\n- animated ‚Üí genre\",\n",
    "    \"step2\": \"- director(film,human): 'directed by Makoto Kamiya' ‚úì\\n- country of origin(film,country): 'Japanese' ‚Üí Japan ‚úì\\n- publication date(film,year): '2012' ‚úì\",\n",
    "    \"final_triples\": 'director(\"Resident Evil: Damnation\",\"Makoto Kamiya\")\\ncountry of origin(\"Resident Evil: Damnation\",\"Japan\")\\npublication date(\"Resident Evil: Damnation\",\"2012\")'\n",
    "}\n",
    "\n",
    "prompt_text = build_reason_then_extract_prompt(\n",
    "    ontology, input_text, worked_example=worked_example, allow_light_norm=True, relation_cues_text=None\n",
    ")\n",
    "\n",
    "print(prompt_text[:1200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5772611c-5e96-40dc-a2b6-89d81d036f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Sending chat-formatted prompt to model ...\n",
      "\n",
      "‚úÖ Model generation completed.\n",
      "\n",
      "----- OUTPUT HEAD -----\n",
      "  Step 1 (Entities & types):\n",
      "- The Life on the Earth (or Tracing the Gray Summer) ‚Üí film\n",
      "- Satoshi Dezaki ‚Üí human\n",
      "- 2001 ‚Üí year\n",
      "- Japanese ‚Üí country ‚Üí Japan\n",
      "- anime ‚Üí genre\n",
      "- Magic Bus ‚Üí film production company\n",
      "- GoGo Visual Planning ‚Üí film production company\n",
      "- Seveso disaster ‚Üí event\n",
      "- Italian town of Seveso ‚Üí city\n",
      "- 1976 ‚Üí year\n",
      "\n",
      "Step 2 (Verify relations):\n",
      "- director(film,human): 'by Satoshi Dezaki' ‚úì\n",
      "- production company(film,film production company): 'a joint production between the Magic Bus and the GoGo Visual Planning' ‚úì\n",
      "- based on(film,written work): 'which recounts the true story of the Seveso disaster' (implied, no direct mention)\n",
      "- narrative location(film,city): 'Italian town of Seveso' ‚úì\n",
      "- event occurred in(event,city): 'occurred over the Italian town of Seveso' ‚úì\n",
      "- year of event(\n",
      "\n",
      "----- OUTPUT TAIL -----\n",
      "  Seveso' ‚úì\n",
      "- event occurred in(event,city): 'occurred over the Italian town of Seveso' ‚úì\n",
      "- year of event(event,year): '1976' ‚úì\n",
      "- country of event(event,country): 'Italy' (implied, no direct mention)\n",
      "\n",
      "### FINAL TRIPLES\n",
      "director(\"The Life on the Earth (or Tracing the Gray Summer)\",\"Satoshi Dezaki\")\n",
      "production company(\"The Life on the Earth (or Tracing the Gray Summer)\",\"Magic Bus\")\n",
      "production company(\"The Life on the Earth (or Tracing the Gray Summer)\",\"GoGo Visual Planning\")\n",
      "based on(\"The Life on the Earth (or Tracing the Gray Summer)\",\"Seveso disaster\")\n",
      "narrative location(\"The Life on the Earth (or Tracing the Gray Summer)\",\"Italian town of Seveso\")\n",
      "event occurred in(Seveso disaster\",\"Italian town of Seveso\")\n",
      "year of event(Seveso disaster\",\"1976\")\n",
      "country of event(Seveso disaster\",\"Italy\")\n"
     ]
    }
   ],
   "source": [
    "# Block 3Ô∏è‚É£ ‚Äì Run Single Inference (chat template; continuation only)\n",
    "\n",
    "def run_single_inference_chat(generator, tokenizer, base_prompt, max_new_tokens=1024):\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": base_prompt}\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    print(\"üîπ Sending chat-formatted prompt to model ...\")\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    text = out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n",
    "    print(\"\\n‚úÖ Model generation completed.\\n\")\n",
    "    return text\n",
    "\n",
    "# Run once\n",
    "model_output = run_single_inference_chat(text_pipe, tokenizer, prompt_text, max_new_tokens=1024)\n",
    "\n",
    "print(\"----- OUTPUT HEAD -----\\n\", model_output[:800])\n",
    "print(\"\\n----- OUTPUT TAIL -----\\n\", model_output[-800:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e088e2-943b-4b5e-bfa0-0fa37c34569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c8ea7-63eb-4277-be07-fe6cd8813e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586dd9b-9277-4e22-9dd8-facc39effc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1994b-783c-47d3-b184-71e38f4f7ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89ee08-c828-40f6-b81c-c156e1612052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614885db-263a-440c-9ca8-e7565dc577b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d1faf-94d9-4955-9bba-647dbb06347e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a66f6-1243-4a94-ac19-a193850b56a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd2c6c-199f-4980-b1a6-eacd07d62fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241fa9be-ab45-4590-986a-d4569426d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # Triple Extraction Pipeline (Steps 1‚Äì4)\n",
    "# # - Single-call per input text\n",
    "# # - Ontology-guided + open-set (include sentence-backed triples even if not in ontology)\n",
    "# # - Processes first N inputs for fast debugging\n",
    "# # =========================\n",
    "\n",
    "# import os, json, re, time\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# # -------- Block 1: Model Setup --------\n",
    "# def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         device_map=\"auto\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#     )\n",
    "#     model.config.use_cache = False\n",
    "#     generator = pipeline(\n",
    "#         \"text-generation\",\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "#     return generator, tokenizer\n",
    "\n",
    "\n",
    "# # -------- Block 2: Prompt Creation (Reason ‚Üí Verify ‚Üí Final Triples) --------\n",
    "# from textwrap import dedent\n",
    "\n",
    "# def _concept_label(ontology_json, qid):\n",
    "#     return next((c[\"label\"] for c in ontology_json.get(\"concepts\", []) if c.get(\"qid\") == qid), \"\")\n",
    "\n",
    "# def format_ontology_concepts(ontology_json):\n",
    "#     return \", \".join(c[\"label\"] for c in ontology_json.get(\"concepts\", []))\n",
    "\n",
    "# def format_ontology_relations(ontology_json):\n",
    "#     lines = []\n",
    "#     for r in ontology_json.get(\"relations\", []):\n",
    "#         dom = _concept_label(ontology_json, r.get(\"domain\"))\n",
    "#         rng = _concept_label(ontology_json, r.get(\"range\"))\n",
    "#         lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "#     return \"\\n\".join(lines)\n",
    "\n",
    "# def build_reason_then_extract_prompt(\n",
    "#     ontology_json,\n",
    "#     test_sentence,\n",
    "#     worked_example=None,\n",
    "#     allow_light_norm=True,\n",
    "#     relation_cues_text=None  # optional Step 0 paraphrase cues (keep None for now)\n",
    "# ):\n",
    "#     concepts_line   = format_ontology_concepts(ontology_json)\n",
    "#     relations_block = format_ontology_relations(ontology_json)\n",
    "#     norm_note = \"normalize only trivial cases (e.g., Japanese‚ÜíJapan)\" if allow_light_norm else \"copy spans verbatim; no normalization\"\n",
    "\n",
    "#     step0_block = \"\"\n",
    "#     if relation_cues_text:\n",
    "#         step0_block = dedent(f\"\"\"\n",
    "#         Step 0 (Paraphrase expansion for this sentence):\n",
    "#         {relation_cues_text.strip()}\n",
    "#         \"\"\")\n",
    "\n",
    "#     example_block = \"\"\n",
    "#     if worked_example:\n",
    "#         example_block = dedent(f\"\"\"\n",
    "#         ### WORKED EXAMPLE\n",
    "#         Sentence:\n",
    "#         {worked_example[\"sentence\"]}\n",
    "\n",
    "#         Step 1 (Entities & types):\n",
    "#         {worked_example[\"step1\"]}\n",
    "\n",
    "#         Step 2 (Verify relations):\n",
    "#         {worked_example[\"step2\"]}\n",
    "\n",
    "#         ### FINAL TRIPLES\n",
    "#         {worked_example[\"final_triples\"].strip()}\n",
    "#         \"\"\").strip()\n",
    "\n",
    "#     rules = dedent(\"\"\"\n",
    "#     RULES:\n",
    "#     - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "#     - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
    "#     - Resolve simple coreference (e.g., \"the university\", \"it\", \"this college\" ‚Üí the named university in this sentence).\n",
    "#     - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "#     - Output one triple per line, format: predicate(\"Subject\",\"Object\").\n",
    "#     - Base decisions ONLY on the TEST SENTENCE.\n",
    "#     - Avoid duplicates; apply light normalization only where unambiguous.\n",
    "#     \"\"\")\n",
    "\n",
    "#     prompt = dedent(f\"\"\"\n",
    "#     TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "#     - Use the ontology to guide relation labeling when possible.\n",
    "#     - {norm_note}.\n",
    "#     - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
    "\n",
    "#     ONTOLOGY CONCEPTS:\n",
    "#     {concepts_line}\n",
    "\n",
    "#     ONTOLOGY RELATIONS (argument types):\n",
    "#     {relations_block}\n",
    "\n",
    "#     {rules}\n",
    "\n",
    "#     {step0_block}\n",
    "\n",
    "#     {example_block if example_block else \"\"}\n",
    "\n",
    "#     ### TEST SENTENCE\n",
    "#     \"{test_sentence}\"\n",
    "\n",
    "#     Step 1 (Entities & types):\n",
    "#     Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "#     ### FINAL TRIPLES\n",
    "#     # one triple per line; include ontology-mapped AND extra sentence-backed relations\n",
    "#     \"\"\").strip()\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "\n",
    "# # -------- Block 3: Single Inference (chat template; continuation-only) --------\n",
    "# def run_single_inference_chat(generator, tokenizer, base_prompt, max_new_tokens=768, temperature=0.3):\n",
    "#     chat = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "#         {\"role\": \"user\", \"content\": base_prompt}\n",
    "#     ]\n",
    "#     formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     out = generator(\n",
    "#         formatted,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         temperature=temperature,\n",
    "#         top_p=0.9,\n",
    "#         do_sample=True,\n",
    "#         return_full_text=False,\n",
    "#         truncation=False,\n",
    "#         eos_token_id=tokenizer.eos_token_id,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n",
    "\n",
    "\n",
    "# # -------- Block 4: Extract & Parse FINAL TRIPLES --------\n",
    "# FINAL_HEADER_REGEX = r'^\\s*#{2,}\\s*FINAL\\s+TRIPLES\\s*$'  # matches \"## FINAL TRIPLES\" or \"### FINAL TRIPLES\"\n",
    "\n",
    "# def extract_final_triples_block(text: str) -> str:\n",
    "#     # 1) primary: markdown header style\n",
    "#     header = re.search(FINAL_HEADER_REGEX, text, re.IGNORECASE | re.MULTILINE)\n",
    "#     if not header:\n",
    "#         # 2) relaxed fallback: line containing \"FINAL TRIPLES\" at end\n",
    "#         header = re.search(r'FINAL\\s+TRIPLES\\s*[:\\-]*\\s*$', text, re.IGNORECASE | re.MULTILINE)\n",
    "#         if not header:\n",
    "#             return \"\"\n",
    "#     start = header.end()\n",
    "#     tail = text[start:].strip()\n",
    "#     # stop at next markdown header or end\n",
    "#     nxt = re.search(r'^\\s*#{2,}\\s+[A-Z].*$', tail, re.MULTILINE)\n",
    "#     if nxt:\n",
    "#         tail = tail[:nxt.start()].strip()\n",
    "#     return tail\n",
    "\n",
    "# def parse_triples_block(block_text: str):\n",
    "#     \"\"\"\n",
    "#     Parse lines: relation(\"Subject\",\"Object\")\n",
    "#     - relation may include spaces/underscores\n",
    "#     - subject/object must be double-quoted (handles commas safely)\n",
    "#     \"\"\"\n",
    "#     triples = []\n",
    "#     line_re = re.compile(r'^\\s*([A-Za-z][A-Za-z0-9_ ]*?)\\s*\\(\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\)\\s*$')\n",
    "#     for raw in block_text.splitlines():\n",
    "#         line = raw.strip()\n",
    "#         if not line or line.startswith(\"#\"):\n",
    "#             continue\n",
    "#         m = line_re.match(line)\n",
    "#         if not m:\n",
    "#             # skip non-conforming lines\n",
    "#             continue\n",
    "#         rel, sub, obj = (m.group(1).strip(), m.group(2).strip(), m.group(3).strip())\n",
    "#         triples.append({\"sub\": sub, \"rel\": rel, \"obj\": obj})\n",
    "#     return triples\n",
    "\n",
    "\n",
    "# # -------- Utilities: JSONL I/O --------\n",
    "# def read_jsonl(path, max_items=None):\n",
    "#     count = 0\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             yield json.loads(line)\n",
    "#             count += 1\n",
    "#             if max_items is not None and count >= max_items:\n",
    "#                 break\n",
    "\n",
    "# def write_jsonl(path, records):\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for rec in records:\n",
    "#             f.write(json.dumps(rec, ensure_ascii=False))\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# # -------- Orchestration: process file --------\n",
    "# def run_pipeline(\n",
    "#     input_jsonl_path: str,\n",
    "#     ontology_json_path: str,\n",
    "#     output_jsonl_path: str,\n",
    "#     max_items: int = 4,\n",
    "#     max_new_tokens: int = 768,\n",
    "#     temperature: float = 0.3,\n",
    "#     verbose: bool = True\n",
    "# ):\n",
    "#     # Load ontology once\n",
    "#     with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         ontology = json.load(f)\n",
    "\n",
    "#     # Load model\n",
    "#     generator, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "#     out_records = []\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     for idx, item in enumerate(read_jsonl(input_jsonl_path, max_items=max_items), start=1):\n",
    "#         sent_id = item.get(\"id\")\n",
    "#         sent    = item.get(\"sent\", \"\")\n",
    "\n",
    "#         # Build prompt\n",
    "#         prompt_text = build_reason_then_extract_prompt(\n",
    "#             ontology_json=ontology,\n",
    "#             test_sentence=sent,\n",
    "#             worked_example=None,       # keep None; you can inject a worked example if you want\n",
    "#             allow_light_norm=True,\n",
    "#             relation_cues_text=None    # keep None; can inject paraphrase cues later\n",
    "#         )\n",
    "\n",
    "#         # Inference\n",
    "#         gen_t0 = time.time()\n",
    "#         model_output = run_single_inference_chat(generator, tokenizer, prompt_text, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "#         gen_dt = time.time() - gen_t0\n",
    "\n",
    "#         # Extract & parse\n",
    "#         final_block = extract_final_triples_block(model_output)\n",
    "#         triples = parse_triples_block(final_block)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"\\n[{idx}] ID={sent_id}\")\n",
    "#             print(f\"Sentence: {sent}\")\n",
    "#             print(f\"Generated in {gen_dt:.2f}s | Triples: {len(triples)}\")\n",
    "#             if triples:\n",
    "#                 print(\"Sample:\", triples[0])\n",
    "\n",
    "#         out_records.append({\"id\": sent_id, \"triples\": triples})\n",
    "\n",
    "#     # Write output JSONL\n",
    "#     write_jsonl(output_jsonl_path, out_records)\n",
    "#     print(f\"\\n‚úÖ Done. Wrote {len(out_records)} lines to: {output_jsonl_path} | Total time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "\n",
    "# # =========================\n",
    "# # Configure your paths here\n",
    "# # =========================\n",
    "\n",
    "# # Example paths (edit these to your files)\n",
    "# INPUT_JSONL  = \"/path/to/your/input_texts.jsonl\"   # the file with your 3 test lines\n",
    "# ONTOLOGY_JSON = \"/path/to/your/university_ontology.json\"\n",
    "# OUTPUT_JSONL  = \"/path/to/output_triples.jsonl\"\n",
    "\n",
    "# # Process only first N lines for debugging\n",
    "# MAX_ITEMS = 4   # set to 20 for a larger preview, or None for full run\n",
    "\n",
    "# # Run the pipeline\n",
    "# # (Uncomment the line below after setting valid paths)\n",
    "# # run_pipeline(INPUT_JSONL, ONTOLOGY_JSON, OUTPUT_JSONL, max_items=MAX_ITEMS, max_new_tokens=768, temperature=0.25, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8844b65-e544-49f4-9051-bf4ffb9cc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# love it. here‚Äôs a **clean, modular pipeline**‚Äîeach step in its own block/function so you can debug or swap parts easily.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 0 ‚Äî Imports & Config\n",
    "\n",
    "# ```python\n",
    "# # Block 0 ‚Äî Imports & Config\n",
    "\n",
    "# import os, json, re, time\n",
    "# import torch\n",
    "# from textwrap import dedent\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 1 ‚Äî Model Setup\n",
    "\n",
    "# ```python\n",
    "# # Block 1 ‚Äî Model Setup\n",
    "\n",
    "# def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "#     \"\"\"\n",
    "#     Returns (generator, tokenizer) ready for inference with chat template.\n",
    "#     \"\"\"\n",
    "#     torch.backends.cudnn.benchmark = True\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_id,\n",
    "#         device_map=\"auto\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#     )\n",
    "#     model.config.use_cache = False\n",
    "\n",
    "#     generator = pipeline(\n",
    "#         \"text-generation\",\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         device_map=\"auto\"\n",
    "#     )\n",
    "#     return generator, tokenizer\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 2 ‚Äî Prompt Builder (Reason ‚Üí Verify ‚Üí Final Triples)\n",
    "\n",
    "# ```python\n",
    "# # Block 2 ‚Äî Prompt Builder (Reason ‚Üí Verify ‚Üí Final Triples)\n",
    "\n",
    "# def _concept_label(ontology_json, qid):\n",
    "#     return next((c[\"label\"] for c in ontology_json.get(\"concepts\", []) if c.get(\"qid\") == qid), \"\")\n",
    "\n",
    "# def format_ontology_concepts(ontology_json):\n",
    "#     return \", \".join(c[\"label\"] for c in ontology_json.get(\"concepts\", []))\n",
    "\n",
    "# def format_ontology_relations(ontology_json):\n",
    "#     lines = []\n",
    "#     for r in ontology_json.get(\"relations\", []):\n",
    "#         dom = _concept_label(ontology_json, r.get(\"domain\"))\n",
    "#         rng = _concept_label(ontology_json, r.get(\"range\"))\n",
    "#         lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "#     return \"\\n\".join(lines)\n",
    "\n",
    "# def build_reason_then_extract_prompt(\n",
    "#     ontology_json,\n",
    "#     test_sentence: str,\n",
    "#     worked_example: dict | None = None,\n",
    "#     allow_light_norm: bool = True,\n",
    "#     relation_cues_text: str | None = None,   # optional Step 0 paraphrase cues\n",
    "# ) -> str:\n",
    "#     \"\"\"\n",
    "#     Single-output-block design: ### FINAL TRIPLES includes\n",
    "#     - ontology-mapped relations (preferred)\n",
    "#     - plus any other sentence-backed SPO relations (open-set)\n",
    "#     \"\"\"\n",
    "#     concepts_line   = format_ontology_concepts(ontology_json)\n",
    "#     relations_block = format_ontology_relations(ontology_json)\n",
    "#     norm_note = \"normalize only trivial cases (e.g., Japanese‚ÜíJapan)\" if allow_light_norm else \"copy spans verbatim; no normalization\"\n",
    "\n",
    "#     step0_block = \"\"\n",
    "#     if relation_cues_text:\n",
    "#         step0_block = dedent(f\"\"\"\n",
    "#         Step 0 (Paraphrase expansion for this sentence):\n",
    "#         {relation_cues_text.strip()}\n",
    "#         \"\"\")\n",
    "\n",
    "#     example_block = \"\"\n",
    "#     if worked_example:\n",
    "#         example_block = dedent(f\"\"\"\n",
    "#         ### WORKED EXAMPLE\n",
    "#         Sentence:\n",
    "#         {worked_example[\"sentence\"]}\n",
    "\n",
    "#         Step 1 (Entities & types):\n",
    "#         {worked_example[\"step1\"]}\n",
    "\n",
    "#         Step 2 (Verify relations):\n",
    "#         {worked_example[\"step2\"]}\n",
    "\n",
    "#         ### FINAL TRIPLES\n",
    "#         {worked_example[\"final_triples\"].strip()}\n",
    "#         \"\"\").strip()\n",
    "\n",
    "#     rules = dedent(\"\"\"\n",
    "#     RULES:\n",
    "#     - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "#     - If the sentence clearly states another SPO relation NOT in the ontology, still include it using a concise predicate phrase from the sentence (do NOT invent facts).\n",
    "#     - Resolve simple coreference (e.g., \"the university\", \"it\", \"this college\" ‚Üí the named university in this sentence).\n",
    "#     - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "#     - Output one triple per line, format: predicate(\"Subject\",\"Object\").\n",
    "#     - Base decisions ONLY on the TEST SENTENCE.\n",
    "#     - Avoid duplicates; apply light normalization only where unambiguous.\n",
    "#     \"\"\")\n",
    "\n",
    "#     prompt = dedent(f\"\"\"\n",
    "#     TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "#     - Use the ontology to guide relation labeling when possible.\n",
    "#     - {norm_note}.\n",
    "#     - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
    "\n",
    "#     ONTOLOGY CONCEPTS:\n",
    "#     {concepts_line}\n",
    "\n",
    "#     ONTOLOGY RELATIONS (argument types):\n",
    "#     {relations_block}\n",
    "\n",
    "#     {rules}\n",
    "\n",
    "#     {step0_block}\n",
    "\n",
    "#     {example_block if example_block else \"\"}\n",
    "\n",
    "#     ### TEST SENTENCE\n",
    "#     \"{test_sentence}\"\n",
    "\n",
    "#     Step 1 (Entities & types):\n",
    "#     Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "#     ### FINAL TRIPLES\n",
    "#     # one triple per line; include ontology-mapped AND extra sentence-backed relations\n",
    "#     \"\"\").strip()\n",
    "\n",
    "#     return prompt\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 3 ‚Äî Single Inference (chat template; continuation-only)\n",
    "\n",
    "# ```python\n",
    "# # Block 3 ‚Äî Single Inference (chat template; continuation-only)\n",
    "\n",
    "# def generate_triples_text(generator, tokenizer, prompt_text: str,\n",
    "#                           max_new_tokens: int = 768, temperature: float = 0.3) -> str:\n",
    "#     \"\"\"\n",
    "#     Calls the model once. Returns the full generated continuation (reasoning + FINAL TRIPLES).\n",
    "#     \"\"\"\n",
    "#     chat = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "#         {\"role\": \"user\", \"content\": prompt_text}\n",
    "#     ]\n",
    "#     formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     out = generator(\n",
    "#         formatted,\n",
    "#         max_new_tokens=max_new_tokens,\n",
    "#         temperature=temperature,\n",
    "#         top_p=0.9,\n",
    "#         do_sample=True,\n",
    "#         return_full_text=False,\n",
    "#         truncation=False,\n",
    "#         eos_token_id=tokenizer.eos_token_id,\n",
    "#         pad_token_id=tokenizer.eos_token_id,\n",
    "#     )\n",
    "#     return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 4 ‚Äî Extract & Parse `FINAL TRIPLES`\n",
    "\n",
    "# ```python\n",
    "# # Block 4 ‚Äî Extract & Parse `FINAL TRIPLES`\n",
    "\n",
    "# FINAL_HEADER_REGEX = r'^\\s*#{2,}\\s*FINAL\\s+TRIPLES\\s*$'  # matches ## or ### FINAL TRIPLES\n",
    "\n",
    "# def extract_final_triples_block(model_output_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Return the text after the '##/### FINAL TRIPLES' header (until next header or end).\n",
    "#     \"\"\"\n",
    "#     header = re.search(FINAL_HEADER_REGEX, model_output_text, re.IGNORECASE | re.MULTILINE)\n",
    "#     if not header:\n",
    "#         header = re.search(r'FINAL\\s+TRIPLES\\s*[:\\-]*\\s*$', model_output_text, re.IGNORECASE | re.MULTILINE)\n",
    "#         if not header:\n",
    "#             return \"\"\n",
    "#     start = header.end()\n",
    "#     tail = model_output_text[start:].strip()\n",
    "#     nxt = re.search(r'^\\s*#{2,}\\s+[A-Z].*$', tail, re.MULTILINE)\n",
    "#     if nxt:\n",
    "#         tail = tail[:nxt.start()].strip()\n",
    "#     return tail\n",
    "\n",
    "# def parse_triples_block(block_text: str):\n",
    "#     \"\"\"\n",
    "#     Parse lines: relation(\"Subject\",\"Object\")\n",
    "#     Keeps both ontology and non-ontology predicates (open-set allowed).\n",
    "#     \"\"\"\n",
    "#     triples = []\n",
    "#     line_re = re.compile(r'^\\s*([A-Za-z][A-Za-z0-9_ ]*?)\\s*\\(\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\)\\s*$')\n",
    "#     for raw in block_text.splitlines():\n",
    "#         line = raw.strip()\n",
    "#         if not line or line.startswith(\"#\"):\n",
    "#             continue\n",
    "#         m = line_re.match(line)\n",
    "#         if not m:\n",
    "#             continue\n",
    "#         rel, sub, obj = (m.group(1).strip(), m.group(2).strip(), m.group(3).strip())\n",
    "#         triples.append({\"sub\": sub, \"rel\": rel, \"obj\": obj})\n",
    "#     return triples\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 5 ‚Äî JSONL I/O Helpers\n",
    "\n",
    "# ```python\n",
    "# # Block 5 ‚Äî JSONL I/O Helpers\n",
    "\n",
    "# def read_jsonl(path, max_items: int | None = None):\n",
    "#     \"\"\"\n",
    "#     Yields JSON objects from a .jsonl file.\n",
    "#     If max_items is set, stops after that many records (for debugging).\n",
    "#     \"\"\"\n",
    "#     count = 0\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             yield json.loads(line)\n",
    "#             count += 1\n",
    "#             if max_items is not None and count >= max_items:\n",
    "#                 break\n",
    "\n",
    "# def write_jsonl(path, records):\n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         for rec in records:\n",
    "#             f.write(json.dumps(rec, ensure_ascii=False))\n",
    "#             f.write(\"\\n\")\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 6 ‚Äî Orchestrator (loop inputs ‚Üí Steps 1‚Äì4 ‚Üí output)\n",
    "\n",
    "# ```python\n",
    "# # Block 6 ‚Äî Orchestrator (loop inputs ‚Üí Steps 1‚Äì4 ‚Üí output)\n",
    "\n",
    "# def run_pipeline(\n",
    "#     input_jsonl_path: str,\n",
    "#     ontology_json_path: str,\n",
    "#     output_jsonl_path: str,\n",
    "#     max_items: int = 4,           # ‚Üê process only first N rows (easy debugging)\n",
    "#     max_new_tokens: int = 768,\n",
    "#     temperature: float = 0.3,\n",
    "#     verbose: bool = True\n",
    "# ):\n",
    "#     # Load ontology once\n",
    "#     with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         ontology = json.load(f)\n",
    "\n",
    "#     # Setup model once\n",
    "#     generator, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "#     out_records = []\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     for idx, item in enumerate(read_jsonl(input_jsonl_path, max_items=max_items), start=1):\n",
    "#         sent_id = item.get(\"id\")\n",
    "#         sent    = item.get(\"sent\", \"\")\n",
    "\n",
    "#         # Step 2 ‚Äî Build prompt\n",
    "#         prompt_text = build_reason_then_extract_prompt(\n",
    "#             ontology_json=ontology,\n",
    "#             test_sentence=sent,\n",
    "#             worked_example=None,\n",
    "#             allow_light_norm=True,\n",
    "#             relation_cues_text=None\n",
    "#         )\n",
    "\n",
    "#         # Step 3 ‚Äî Generate\n",
    "#         t_gen0 = time.time()\n",
    "#         model_output = generate_triples_text(\n",
    "#             generator, tokenizer, prompt_text,\n",
    "#             max_new_tokens=max_new_tokens, temperature=temperature\n",
    "#         )\n",
    "#         gen_time = time.time() - t_gen0\n",
    "\n",
    "#         # Step 4 ‚Äî Extract + Parse\n",
    "#         final_block = extract_final_triples_block(model_output)\n",
    "#         triples = parse_triples_block(final_block)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"\\n[{idx}] ID={sent_id}\")\n",
    "#             print(f\"Sentence: {sent}\")\n",
    "#             print(f\"Generated in {gen_time:.2f}s | Triples: {len(triples)}\")\n",
    "#             if triples:\n",
    "#                 print(\"Example triple:\", triples[0])\n",
    "\n",
    "#         out_records.append({\"id\": sent_id, \"triples\": triples})\n",
    "\n",
    "#     # Write output\n",
    "#     write_jsonl(output_jsonl_path, out_records)\n",
    "#     print(f\"\\n‚úÖ Done. Wrote {len(out_records)} lines to: {output_jsonl_path} | Total time: {time.time()-t0:.1f}s\")\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# # Block 7 ‚Äî Example Run (set paths & N)\n",
    "\n",
    "# ```python\n",
    "# # Block 7 ‚Äî Example Run (edit these paths)\n",
    "\n",
    "# INPUT_JSONL   = \"/path/to/input_university_texts.jsonl\"     # your 3 sample lines as JSONL\n",
    "# ONTOLOGY_JSON = \"/path/to/university_ontology.json\"         # your ontology JSON\n",
    "# OUTPUT_JSONL  = \"/path/to/output_triples.jsonl\"             # where to write results\n",
    "\n",
    "# MAX_ITEMS = 4   # change to 20 (or None for full file) to control batches during debugging\n",
    "\n",
    "# # Uncomment to execute:\n",
    "# # run_pipeline(INPUT_JSONL, ONTOLOGY_JSON, OUTPUT_JSONL,\n",
    "# #              max_items=MAX_ITEMS, max_new_tokens=768, temperature=0.25, verbose=True)\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### why this layout is nice to debug\n",
    "\n",
    "# * You can **unit-test** each block separately:\n",
    "\n",
    "#   * Block 2: print `prompt_text`\n",
    "#   * Block 3: print the **raw model output**\n",
    "#   * Block 4: print the **extracted block** and **parsed triples**\n",
    "# * **Swap** the model easily (change the `setup_model` param).\n",
    "# * **Scale** later: add fusion/validation after Block 4 without touching earlier blocks.\n",
    "\n",
    "# want me to add a tiny **validation Block 8** next (ontology-conformance flag + entity-present-in-sentence check)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5c17d-3b62-46d9-b357-af7d2f986635",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
