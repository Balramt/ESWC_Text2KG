{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922ee2a3-5a35-44b4-a2d6-caece5e0d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 10 10:57:32 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   41C    P8             32W /  370W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a9a718-f1fa-4726-ad9c-d0ababffa531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (824205) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 824205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "770f49a2-a0f8-40c1-9e68-a60218401786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "Executable: /upb/users/b/balram/profiles/unix/cs/.conda/envs/kg_pipeline/bin/python3\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Executable:\", sys.executable)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4155b4b7-09b5-4fc5-b5c8-6add97eaa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0 — Imports & Config\n",
    "\n",
    "import os, json, re, time\n",
    "import torch\n",
    "from textwrap import dedent\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9eba531-8734-4269-95a6-5cb6ca325dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_id=\"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True  # KV caching is fine; doesn't cause \"same output\"\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    #print(\"✅ Model loaded successfully.\")\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e289ef-514b-464d-adce-db8acbdd1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utilities ----------\n",
    "\n",
    "def _quote(v: object) -> str:\n",
    "    \"\"\"\n",
    "    Convert any value to a safe, double-quoted string for the triple format.\n",
    "    Escapes backslashes and double quotes.\n",
    "    \"\"\"\n",
    "    s = str(v)\n",
    "    s = s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n",
    "    return f'\"{s}\"'\n",
    "\n",
    "\n",
    "def _coerce_triples_to_str(triples_val):\n",
    "    \"\"\"\n",
    "    Normalize triples into newline-separated lines: rel(\"Subject\",\"Object\")\n",
    "\n",
    "    Accepts:\n",
    "      - str: a preformatted block\n",
    "      - list[str]: each a preformatted line\n",
    "      - list[dict]: each with keys {sub, rel, obj}\n",
    "    \"\"\"\n",
    "    if triples_val is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Already a full block string\n",
    "    if isinstance(triples_val, str):\n",
    "        return triples_val.strip()\n",
    "\n",
    "    # List of preformatted strings\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, str) for x in triples_val):\n",
    "        return \"\\n\".join(x.strip() for x in triples_val if x and x.strip())\n",
    "\n",
    "    # List of dicts -> format to rel(\"sub\",\"obj\")\n",
    "    if isinstance(triples_val, list) and all(isinstance(x, dict) for x in triples_val):\n",
    "        lines = []\n",
    "        for t in triples_val:\n",
    "            rel = (t.get(\"rel\") or \"\").strip()\n",
    "            sub = t.get(\"sub\")\n",
    "            obj = t.get(\"obj\")\n",
    "            if rel and (sub is not None) and (obj is not None):\n",
    "                lines.append(f'{rel}({_quote(sub)},{_quote(obj)})')\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    # Fallback\n",
    "    return str(triples_val).strip()\n",
    "\n",
    "\n",
    "# ---------- Few-shot loader for FIXED schema ----------\n",
    "\n",
    "def load_one_shot_examples(few_shot_jsonl_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads {id: {\"example_sentence\": <str>, \"example_triples_output\": <any>}}.\n",
    "    - Reads fields exactly as written in your file:\n",
    "        \"Example sentence\"\n",
    "        \"Example triples output\"   (and falls back to \"Example triples\" if the former is absent)\n",
    "    - No validation / coercion.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for rec in read_jsonl(few_shot_jsonl_path, max_items=None):\n",
    "        rid = rec.get(\"id\")\n",
    "        if rid is None:\n",
    "            continue  # keep this tiny guard so the dict key isn't None\n",
    "        out[str(rid).strip()] = {\n",
    "            \"example_sentence\": rec.get(\"Example sentence\"),\n",
    "            # prefer \"Example triples output\", else fallback to \"Example triples\"\n",
    "            \"example_triples_output\": rec.get(\"Example triples output\", rec.get(\"Example triples\")),\n",
    "        }\n",
    "    print(f\"[FEW-SHOT] Loaded {len(out)} examples from: {few_shot_jsonl_path}\")\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1de81bde-9a09-40e9-8b19-f50ee2974b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Prompt Builder (Reason → Verify → Final Triples)\n",
    "\n",
    "from textwrap import dedent\n",
    "import json\n",
    "\n",
    "# ---------- Block 2 — Prompt Builder (Reason → Verify → Final Triples) ----------\n",
    "\n",
    "# Helpers (strict access; fail fast if ontology keys are missing)\n",
    "def _concept_label(ontology_json, qid):\n",
    "    return next((c[\"label\"] for c in ontology_json[\"concepts\"] if c[\"qid\"] == qid), \"\")\n",
    "\n",
    "def format_ontology_concepts(ontology_json):\n",
    "    return \", \".join(c[\"label\"] for c in ontology_json[\"concepts\"])\n",
    "\n",
    "def format_ontology_relations(ontology_json):\n",
    "    lines = []\n",
    "    for r in ontology_json[\"relations\"]:\n",
    "        dom = _concept_label(ontology_json, r[\"domain\"])\n",
    "        rng = _concept_label(ontology_json, r[\"range\"])\n",
    "        lines.append(f'- {r[\"label\"]}({dom},{rng})')\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_reason_then_extract_prompt(\n",
    "    ontology_json,\n",
    "    test_sentence,\n",
    "    worked_example=None,\n",
    "    allow_light_norm=True,\n",
    "    relation_cues_text=None,   # optional Step 0 paraphrase cues\n",
    "    strict_format=True,        # enforce exact output shape\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a prompt that ends with the header `### FINAL TRIPLES` and *no trailing guidance*,\n",
    "    so generations appear directly under the header in the required line format.\n",
    "\n",
    "    This version matches the downstream extractor which searches for `##/### FINAL TRIPLES`.\n",
    "    \"\"\"\n",
    "\n",
    "    concepts_line   = format_ontology_concepts(ontology_json)\n",
    "    relations_block = format_ontology_relations(ontology_json)\n",
    "    norm_note = (\n",
    "        \"normalize only trivial cases (e.g., Japanese→Japan)\"\n",
    "        if allow_light_norm else\n",
    "        \"copy spans verbatim; no normalization\"\n",
    "    )\n",
    "\n",
    "    # Optional Step 0 cues\n",
    "    step0_block = \"\"\n",
    "    if relation_cues_text:\n",
    "        step0_block = dedent(f\"\"\"\n",
    "        Step 0 (Paraphrase expansion for this sentence):\n",
    "        {relation_cues_text.strip()}\n",
    "        \"\"\")\n",
    "\n",
    "    # Optional worked example — supports either the new shape or the older example fields\n",
    "    example_block = \"\"\n",
    "    if worked_example:\n",
    "        ex_sent = worked_example.get(\"example_sentence\")\n",
    "        ex_out  = worked_example.get(\"example_triples_output\")\n",
    "        #         # Use JSON dumps to preserve quotes/escapes for the sentence,\n",
    "        # and to serialize lists/dicts as-is for the output block.\n",
    "        ex_sent_json = json.dumps(ex_sent, ensure_ascii=False)  # -> \"The number of staff...\"\n",
    "        ex_out_json  = json.dumps(ex_out, ensure_ascii=False)   # -> [{\"sub\":...,\"rel\":...,\"obj\":...}, ...]\n",
    "        example_block = dedent(f'''\n",
    "        \"Example sentence\": {ex_sent_json}\n",
    "\n",
    "        \"Example FINAL TRIPLES\": {ex_out_json}\n",
    "        ''').strip()\n",
    "\n",
    "    # Strict output notes merged into RULES (no trailing text after the header in the final prompt)\n",
    "    strict_notes = \"\"\n",
    "    if strict_format:\n",
    "        strict_notes = dedent(\"\"\"\n",
    "        OUTPUT SHAPE (strict):\n",
    "        - After Step 2, output the header exactly:\n",
    "        ### FINAL TRIPLES\n",
    "        - Then output one triple per line as: predicate(\"Subject\",\"Object\")\n",
    "        - No extra commentary after the triples. If no triples, just output the header.\n",
    "        - Do NOT output JSON, code fences, or any other sections.\n",
    "        \"\"\").strip()\n",
    "\n",
    "    rules = dedent(f\"\"\"\n",
    "    RULES:\n",
    "    - Prefer relation labels from ONTOLOGY RELATIONS when they match the evidence.\n",
    "    - If the sentence clearly states another SPO relation that is NOT in the ontology, still include it using a concise predicate phrase derived from the trigger words in the sentence (do NOT invent facts).\n",
    "    - Resolve simple coreference (e.g., \"the film\", \"this movie\", \"it\" → the film title in this sentence).\n",
    "    - Quote/mention evidence in Step 2 for each emitted triple.\n",
    "    - Output one triple per line, format: predicate(\"Subject\",\"Object\").\n",
    "    - Base decisions ONLY on the TEST SENTENCE.\n",
    "    - Avoid duplicates; apply light normalization only (e.g., Japanese→Japan).\n",
    "    {strict_notes}\n",
    "    \"\"\").strip()\n",
    "\n",
    "    # IMPORTANT: end the prompt with the header and NOTHING after it.\n",
    "    prompt = dedent(f\"\"\"\n",
    "    TASK: Extract all SPO triples that the TEST SENTENCE clearly supports.\n",
    "    - Use the ontology to guide relation labeling when possible.\n",
    "    - {norm_note}.\n",
    "    - If a valid SPO fact has no matching ontology label, still output it (single FINAL TRIPLES list).\n",
    "\n",
    "    ONTOLOGY CONCEPTS:\n",
    "    {concepts_line}\n",
    "\n",
    "    ONTOLOGY RELATIONS (argument types):\n",
    "    {relations_block}\n",
    "\n",
    "    {rules}\n",
    "\n",
    "    {step0_block}\n",
    "\n",
    "    {example_block if example_block else \"\"}\n",
    "\n",
    "    ### TEST SENTENCE\n",
    "    \"{test_sentence}\"\n",
    "\n",
    "    Step 1 (Entities & types):\n",
    "    Step 2 (Verify relations): For each candidate triple, quote or mention the trigger phrase and spans.\n",
    "\n",
    "    ### FINAL TRIPLES\n",
    "    \"\"\").strip()\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838bbb8e-79ec-4ca3-8a7a-58c6cead4bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 — Single Inference (chat template; continuation-only)\n",
    "\n",
    "def generate_triples_text(generator, tokenizer, prompt_text: str,\n",
    "                          max_new_tokens: int = 768, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Calls the model once. Returns the full generated continuation (reasoning + FINAL TRIPLES).\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise information-extraction model. Follow instructions carefully.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544caf55-fb2f-433b-8c83-435333b65817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 — Extract & Parse `FINAL TRIPLES`\n",
    "\n",
    "FINAL_HEADER_REGEX = r'^\\s*#{2,}\\s*FINAL\\s+TRIPLES\\s*$'  # matches ## or ### FINAL TRIPLES\n",
    "\n",
    "def extract_final_triples_block(model_output_text: str) -> str:\n",
    "    # try normal header first\n",
    "    #print(\"model_output_text: \",model_output_text)\n",
    "    \n",
    "    header = re.search(FINAL_HEADER_REGEX, model_output_text, re.IGNORECASE | re.MULTILINE)\n",
    "    if header:\n",
    "        start = header.end()\n",
    "        tail = model_output_text[start:].strip()\n",
    "        nxt = re.search(r'^\\s*#{2,}\\s+[A-Z].*$', tail, re.MULTILINE)\n",
    "        return tail[:nxt.start()].strip() if nxt else tail\n",
    "\n",
    "    # fallback: take everything and let the parser fish valid lines out\n",
    "    return model_output_text\n",
    "\n",
    "def parse_triples_block(block_text: str):\n",
    "    \"\"\"\n",
    "    Parse lines of the form:\n",
    "        predicate(\"Subject\",\"Object\")\n",
    "        predicate('Subject','Object')\n",
    "        predicate(Subject,Object)\n",
    "        predicate(Subject, 250)\n",
    "    Keeps both ontology and non-ontology predicates (open-set allowed).\n",
    "    Returns a list of dicts: {\"sub\": ..., \"rel\": ..., \"obj\": ...}\n",
    "    \"\"\"\n",
    "    triples = []\n",
    "\n",
    "    line_re = re.compile(r\"\"\"\n",
    "    ^\\s*(?:[-*•]\\s*)?                 # optional markdown bullet\n",
    "    (?P<rel>[A-Za-z][A-Za-z0-9_ ]*?)  # predicate name\n",
    "    \\s*\\(\\s*\n",
    "    (?:                               # SUBJECT (quoted or unquoted)\n",
    "        [\"'](?P<sub_q>[^\"']+?)[\"']    # \"Subject\" or 'Subject'\n",
    "        | (?P<sub_u>[^,)\\n]+?)        # Subject (unquoted)\n",
    "    )\n",
    "    \\s*,\\s*\n",
    "    (?:                               # OBJECT (quoted or unquoted)\n",
    "        [\"'](?P<obj_q>[^\"']+?)[\"']    # \"Object\" or 'Object'\n",
    "        | (?P<obj_u>[^)\\n]+?)         # Object (unquoted)\n",
    "    )\n",
    "    \\s*\\)\\s*\\.?\\s*$                   # optional trailing period\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "    for raw in block_text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "\n",
    "        m = line_re.match(line)\n",
    "        if not m:\n",
    "            # skip non-matching lines quietly\n",
    "            continue\n",
    "\n",
    "        rel = m.group(\"rel\").strip()\n",
    "        sub = (m.group(\"sub_q\") or m.group(\"sub_u\") or \"\").strip()\n",
    "        obj = (m.group(\"obj_q\") or m.group(\"obj_u\") or \"\").strip()\n",
    "\n",
    "        # Optional: guard against empties\n",
    "        if not rel or not sub or not obj:\n",
    "            continue\n",
    "\n",
    "        triples.append({\"sub\": sub, \"rel\": rel, \"obj\": obj})\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fccda78-acb3-4fdb-9b9d-86738232623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 — JSONL I/O Helpers\n",
    "\n",
    "def read_jsonl(path, max_items: int | None = None):\n",
    "    \"\"\"\n",
    "    Yields JSON objects from a .jsonl file.\n",
    "    If max_items is set, stops after that many records (for debugging).\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "def write_jsonl(path, records):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c58533f-4884-4af5-bdd2-6c8bc471060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 — Orchestrator (loop inputs → Steps 1–4 → output)\n",
    "\n",
    "def run_pipeline(\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    max_items: int = 4,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = True,\n",
    "    few_shot_jsonl_path: str | None = None,   # pass as keyword in the call below\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the extraction pipeline.\n",
    "    Required: input_jsonl_path, ontology_json_path, output_jsonl_path.\n",
    "    Optional: max_items, max_new_tokens, temperature, verbose, few_shot_jsonl_path.\n",
    "    \"\"\"\n",
    "    # ✅ Load ontology once\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology = json.load(f)\n",
    "\n",
    "    # ✅ Load one-shot examples (if provided)\n",
    "    if few_shot_jsonl_path:\n",
    "        one_shot_by_id = load_one_shot_examples(few_shot_jsonl_path)\n",
    "    else:\n",
    "        one_shot_by_id = {}\n",
    "        print(\"[FEW-SHOT] No few_shot_jsonl_path provided — proceeding without examples.\")\n",
    "\n",
    "    # ✅ Setup model once\n",
    "    generator, tokenizer = setup_model(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "\n",
    "    out_records = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ✅ Iterate inputs\n",
    "    for idx, item in enumerate(read_jsonl(input_jsonl_path, max_items=max_items), start=1):\n",
    "        sent_id = item.get(\"id\")\n",
    "        sent    = item.get(\"sent\", \"\")\n",
    "\n",
    "        # Match example by id\n",
    "        worked_example = one_shot_by_id.get(sent_id)\n",
    "        # Build prompt\n",
    "        prompt_text = build_reason_then_extract_prompt(\n",
    "            ontology_json=ontology,\n",
    "            test_sentence=sent,\n",
    "            worked_example=worked_example,\n",
    "            allow_light_norm=True,\n",
    "            relation_cues_text=None\n",
    "        )\n",
    "\n",
    "\n",
    "        # Generate\n",
    "        t_gen0 = time.time()\n",
    "        model_output = generate_triples_text(\n",
    "            generator, tokenizer, prompt_text,\n",
    "            max_new_tokens=max_new_tokens, temperature=temperature\n",
    "        )\n",
    "        gen_time = time.time() - t_gen0\n",
    "\n",
    "        # Extract & Parse\n",
    "        final_block = extract_final_triples_block(model_output)\n",
    "\n",
    "        triples = parse_triples_block(final_block)\n",
    "\n",
    "        out_records.append({\"id\": sent_id, \"triples\": triples})\n",
    "\n",
    "    # Write output\n",
    "    write_jsonl(output_jsonl_path, out_records)\n",
    "    print(f\"\\n✅ Done. Wrote {len(out_records)} lines to: {output_jsonl_path} | Total time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    return out_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b8662f-aaf7-4f62-ac06-13d8d2db2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 — Example Run (edit these paths)\n",
    "\n",
    "INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_1_university_test.jsonl\"     # your 3 sample lines as JSONL\n",
    "ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/1_university_ontology.json\"         # your ontology JSON\n",
    "OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_1_university.jsonl\"             # where to write results\n",
    "FEW_SHOT_JSONL = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_1_university_few_shot.jsonl\"\n",
    "MAX_ITEMS =None   # change to 20 (or None for full file) to control batches during debugging\n",
    "\n",
    "# Uncomment to execute:\n",
    "# run_pipeline(INPUT_JSONL, ONTOLOGY_JSON, OUTPUT_JSONL,\n",
    "#              max_items=MAX_ITEMS, max_new_tokens=768, temperature=0.25, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2f2ab7-f568-4e97-9553-83b5d385bef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FEW-SHOT] Loaded 71 examples from: /upb/users/b/balram/profiles/unix/cs/promptKG/data/fewshots_example/dbpedia/ont_1_university_few_shot.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e313d99fd7ef4135b1e3db09ec1008ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "out = run_pipeline(\n",
    "    input_jsonl_path=INPUT_JSONL,\n",
    "    ontology_json_path=ONTOLOGY_JSON,\n",
    "    output_jsonl_path=OUTPUT_JSONL,\n",
    "    max_items=MAX_ITEMS,\n",
    "    max_new_tokens=768,\n",
    "    temperature=0.25,\n",
    "    verbose=True,\n",
    "    few_shot_jsonl_path=FEW_SHOT_JSONL,  # <-- pass by keyword\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c539340-9028-461a-b543-6448bfb359e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101b42f-5f45-4f59-9fad-fd18f41d792d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea0092-8e32-455e-9418-c736677bc1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KG Pipeline (GPU)",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
