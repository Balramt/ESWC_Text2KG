{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb3b356-268d-4e56-8719-e43e2c1091e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 21 12:32:28 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8             31W /  370W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8602ccbb-26c9-4e0a-b6c9-21add5d256eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (26205) - No such process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!kill 26205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9d2fc9-0ff8-4ff6-8265-e2b89cfd0dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limbo\n",
      "/opt/miniforge3/envs/jupyterhub/bin/python\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!which python\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de00b16-cfd6-4071-978f-4b2e235bb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 0 — Imports & Config\n",
    "import os, json, time\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3adac136-eb87-4ae1-980a-d3a817e726f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — IO Utilities\n",
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "def write_jsonl(path: str, records):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e6e135-6bd7-4491-9a14-7a61d77e73db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Ontology Helpers (label rendering without Q-IDs)\n",
    "\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a lookup that maps any known identifier to its human-readable label.\n",
    "    Keys include: concept['qid'], concept['id'], and concept['label'] (all as strings).\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "        # store by 'qid' (e.g., Q5), 'id' (if present), and the label itself\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            val = c.get(keyname)\n",
    "            if val is None:\n",
    "                continue\n",
    "            sval = str(val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "def _label_for(value: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Given a domain/range value (could be 'Q5', 'human', etc.),\n",
    "    return the human-readable label. Falls back to str(value) if unknown.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    sval = str(value).strip()\n",
    "    return cindex.get(sval, sval)\n",
    "\n",
    "def format_ontology_concepts(ontology_json: Dict[str, Any]) -> str:\n",
    "    # Just list labels; no Q-IDs\n",
    "    labels: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        lab = str(c.get(\"label\", \"\")).strip()\n",
    "        if lab:\n",
    "            labels.append(lab)\n",
    "    return \", \".join(labels)\n",
    "\n",
    "def format_ontology_relations(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render as:\n",
    "      - director(film,human)\n",
    "      - country of origin(film,country)\n",
    "    Never show Q-IDs.\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"),  cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a497dc3-2c5f-4e80-a040-1b6ee7be1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 — Prompt 1 builders\n",
    "def build_p1_system() -> str:\n",
    "    return (\n",
    "        \"You are a KG triple proposer in a Tree-of-Thoughts loop. \"\n",
    "        \"First detect entity mentions and assign tentative ontology types. \"\n",
    "        \"Then, using those mentions, propose candidate triples that are valid under the ontology (domain→range). \"\n",
    "        \"Return only JSON.\"\n",
    "    )\n",
    "\n",
    "def build_p1_user(TEXT: str, ONTO: Dict[str, Any], k: int) -> str:\n",
    "    return dedent(f'''\n",
    "    Task: From the text, 1) list detected mentions with tentative types, 2) propose up to k={k} candidate triples [subject, relation, object]. Use only relations whose domain/range match the types you inferred. For each triple, include confidence ∈ [0,1] and cite the exact supporting span(s).\n",
    "\n",
    "    Text\n",
    "    \"{_escape_multiline(TEXT)}\"\n",
    "\n",
    "    Ontology concepts\n",
    "    {format_ontology_concepts(ONTO)}\n",
    "\n",
    "    Ontology relations (domain → range)\n",
    "    {format_ontology_relations(ONTO)}\n",
    "\n",
    "    Output format (JSON only)\n",
    "    {{\n",
    "      \"mentions\": [\n",
    "        {{\"surface\": \"...\", \"type_candidates\": [\"ConceptA\",\"ConceptB\"], \"span\": [start,end]}}\n",
    "      ],\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,\n",
    "          \"support\": \"exact quote from text\",\n",
    "          \"notes\": \"why domain/range fits\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Constraints\n",
    "    Only output domain/range-valid triples.\n",
    "    Normalize dates to YYYY-MM-DD when possible.\n",
    "    If a pronoun is required, resolve it to the nearest valid antecedent and state that in notes.\n",
    "    Do not invent entities not in the text.\n",
    "    ''').strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a22472-398b-4cdc-bf0f-2e1c47b0abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 — Generation\n",
    "def generate_raw_json(generator, tokenizer, system_text: str, user_text: str,\n",
    "                      max_new_tokens: int = 768, temperature: float = 0.25) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_text},\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2fba5d7-dacd-4c2b-b12e-27381ee67bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Block 5 — Orchestrator (Prompt 1, RAW-only + readable JSONL)\n",
    "\n",
    "    from typing import Dict, Any, List, Tuple, Optional\n",
    "    import json, time\n",
    "\n",
    "    # Prefer your input's 'sent' field, but fall back gracefully\n",
    "    _TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "    def _extract_text(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "        \"\"\"Return (text, key_used) from an input record.\"\"\"\n",
    "        for k in _TEXT_KEYS_PRIORITY:\n",
    "            v = rec.get(k)\n",
    "            if isinstance(v, str) and v.strip():\n",
    "                return v.strip(), k\n",
    "        # Fallback: choose the longest string-valued field\n",
    "        best_k, best_v = \"\", \"\"\n",
    "        for k, v in rec.items():\n",
    "            if isinstance(v, str) and len(v) > len(best_v):\n",
    "                best_k, best_v = k, v\n",
    "        return best_v, best_k\n",
    "\n",
    "    def run_pipeline_prompt1(\n",
    "        input_jsonl_path: str,\n",
    "        ontology_json_path: str,\n",
    "        output_jsonl_path: str,\n",
    "        max_items: Optional[int] = None,\n",
    "        max_new_tokens: int = 768,\n",
    "        temperature: float = 0.25,\n",
    "        verbose: bool = True,\n",
    "        k: int = 6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RAW-only pipeline for Prompt 1 (no filtering / no validation).\n",
    "        Writes a JSONL where each line contains:\n",
    "        - id, prompt tag\n",
    "        - input (echo of text/k/ontology path)\n",
    "        - prompts (system/user exactly as sent)\n",
    "        - model + gen_params\n",
    "        - response: \n",
    "            * text  (verbatim raw LLM output)\n",
    "            * json  (parsed copy if valid; else null)\n",
    "            * json_valid (bool)\n",
    "            * parse_error (string or null)\n",
    "            * summary (tiny counts if json is valid)\n",
    "        - timestamp\n",
    "        \"\"\"\n",
    "        # Load ontology\n",
    "        with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            ontology_json = json.load(f)\n",
    "\n",
    "        # Load dataset\n",
    "        items = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "        if verbose:\n",
    "            print(f\"[RUN] Loaded {len(items)} input items from {input_jsonl_path}\")\n",
    "\n",
    "        # Model\n",
    "        generator, tokenizer = setup_model()\n",
    "\n",
    "        outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "        for i, rec in enumerate(items, start=1):\n",
    "            rid = str(rec.get(\"id\") or f\"item_{i}\")\n",
    "            text, key_used = _extract_text(rec)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"\\n[RUN] === ID={rid} ===\")\n",
    "                print(f\"[INFO] text key: {key_used!r}\")\n",
    "\n",
    "            # Build Prompt 1\n",
    "            sys_prompt = build_p1_system()\n",
    "            usr_prompt = build_p1_user(text, ontology_json, k)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\n==== [DEBUG] SYSTEM PROMPT ====\\n\", sys_prompt)\n",
    "                print(\"\\n==== [DEBUG] USER PROMPT ====\\n\", usr_prompt)\n",
    "\n",
    "            # Generate (RAW text only)\n",
    "            try:\n",
    "                raw = generate_raw_json(\n",
    "                    generator, tokenizer, sys_prompt, usr_prompt,\n",
    "                    max_new_tokens=max_new_tokens, temperature=temperature\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Generation failed for {rid}: {e}\")\n",
    "                raw = \"\"\n",
    "\n",
    "            if verbose:\n",
    "                print(\"\\n==== [DEBUG] RAW MODEL OUTPUT ====\\n\", raw)\n",
    "\n",
    "            # Try to parse JSON for convenience ONLY (no filtering). Keep raw text regardless.\n",
    "            parsed = None\n",
    "            parse_error = None\n",
    "            if isinstance(raw, str) and raw.strip():\n",
    "                try:\n",
    "                    parsed = json.loads(raw)\n",
    "                except Exception as e:\n",
    "                    # Optional: minimal recovery – try the first {...} block\n",
    "                    try:\n",
    "                        import re\n",
    "                        m = re.search(r\"\\{[\\s\\S]*\\}\", raw)\n",
    "                        if m:\n",
    "                            parsed = json.loads(m.group(0))\n",
    "                        else:\n",
    "                            parse_error = str(e)\n",
    "                    except Exception as e2:\n",
    "                        parse_error = f\"{e} | recovery: {e2}\"\n",
    "\n",
    "            # Tiny convenience summary (just counts; doesn’t modify content)\n",
    "            summary = None\n",
    "            if isinstance(parsed, dict):\n",
    "                summary = {\n",
    "                    \"mentions\": len(parsed.get(\"mentions\", [])) if isinstance(parsed.get(\"mentions\"), list) else None,\n",
    "                    \"triples\":  len(parsed.get(\"triples\", []))  if isinstance(parsed.get(\"triples\"), list)  else None,\n",
    "                }\n",
    "            out_rec = {\n",
    "                \"id\": rid,\n",
    "                \"input\": text,\n",
    "                \"prompts\": {\n",
    "                    \"system\": sys_prompt,\n",
    "                    \"user\": usr_prompt\n",
    "                },\n",
    "                \"response\": {\n",
    "                    \"text\": raw,                 # verbatim raw string\n",
    "                    \"json\": parsed,              # parsed copy if valid; else null\n",
    "                    \"json_valid\": parsed is not None,\n",
    "                    \"parse_error\": parse_error,  # why parsing failed (if it did)\n",
    "                    \"summary\": summary           # mentions/triples counts\n",
    "                },\n",
    "                \"timestamp\": int(time.time())\n",
    "            }\n",
    "\n",
    "            outputs.append(out_rec)\n",
    "\n",
    "        write_jsonl(output_jsonl_path, outputs)\n",
    "        if verbose:\n",
    "            print(f\"\\n[RUN] Wrote {len(outputs)} records to {output_jsonl_path}\")\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed19c53d-46ec-47a4-99e9-7ec2d89fdaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] First record keys: ['id', 'sent']\n",
      " id: ont_1_movie_test_1\n",
      " sent: Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\n",
      "[RUN] Loaded 1 input items from /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\n",
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239861b3466344da983be4c36f22dd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[RUN] === ID=ont_1_movie_test_1 ===\n",
      "[INFO] text key: 'sent'\n",
      "\n",
      "==== [DEBUG] SYSTEM PROMPT ====\n",
      " You are a KG triple proposer in a Tree-of-Thoughts loop. First detect entity mentions and assign tentative ontology types. Then, using those mentions, propose candidate triples that are valid under the ontology (domain→range). Return only JSON.\n",
      "\n",
      "==== [DEBUG] USER PROMPT ====\n",
      " Task: From the text, 1) list detected mentions with tentative types, 2) propose up to k=6 candidate triples [subject, relation, object]. Use only relations whose domain/range match the types you inferred. For each triple, include confidence ∈ [0,1] and cite the exact supporting span(s).\n",
      "\n",
      "    Text\n",
      "    \"Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\"\n",
      "\n",
      "    Ontology concepts\n",
      "    human, city, country, film, film genre, genre, film production company, film award, award, written work, film character, film organization\n",
      "\n",
      "    Ontology relations (domain → range)\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    Output format (JSON only)\n",
      "    {\n",
      "      \"mentions\": [\n",
      "        {\"surface\": \"...\", \"type_candidates\": [\"ConceptA\",\"ConceptB\"], \"span\": [start,end]}\n",
      "      ],\n",
      "      \"triples\": [\n",
      "        {\n",
      "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
      "          \"confidence\": 0.0,\n",
      "          \"support\": \"exact quote from text\",\n",
      "          \"notes\": \"why domain/range fits\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "\n",
      "    Constraints\n",
      "    Only output domain/range-valid triples.\n",
      "    Normalize dates to YYYY-MM-DD when possible.\n",
      "    If a pronoun is required, resolve it to the nearest valid antecedent and state that in notes.\n",
      "    Do not invent entities not in the text.\n",
      "\n",
      "==== [DEBUG] RAW MODEL OUTPUT ====\n",
      "  {\n",
      "  \"mentions\": [\n",
      "    {\n",
      "      \"surface\": \"Bleach: Hell Verse\",\n",
      "      \"type_candidates\": [\"Film\"],\n",
      "      \"span\": [0, 18]\n",
      "    },\n",
      "    {\n",
      "      \"surface\": \"Noriyuki Abe\",\n",
      "      \"type_candidates\": [\"Human\"],\n",
      "      \"span\": [34, 39]\n",
      "    },\n",
      "    {\n",
      "      \"surface\": \"Japanese\",\n",
      "      \"type_candidates\": [\"Country\"],\n",
      "      \"span\": [19, 24]\n",
      "    }\n",
      "  ],\n",
      "  \"triples\": [\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"director\", \"Noriyuki Abe\"],\n",
      "      \"confidence\": 1.0,\n",
      "      \"support\": \"directed by Noriyuki Abe\",\n",
      "      \"notes\": \"domain: film, range: human\"\n",
      "    },\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"country of origin\", \"Japanese\"],\n",
      "      \"confidence\": 1.0,\n",
      "      \"support\": \"Japanese\",\n",
      "      \"notes\": \"domain: film, range: country\"\n",
      "    },\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"genre\", \"Hell Verse\"],\n",
      "      \"confidence\": 0.5,\n",
      "      \"support\": \"no explicit genre mentioned, but the title suggests a genre related to hell\",\n",
      "      \"notes\": \"domain: film, range: genre (inferred)\"\n",
      "    },\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"production company\", \"not specified\"],\n",
      "      \"confidence\": 0.5,\n",
      "      \"support\": \"no production company mentioned\",\n",
      "      \"notes\": \"domain: film, range: film production company (implied)\"\n",
      "    },\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"publication date\", \"not specified\"],\n",
      "      \"confidence\": 0.5,\n",
      "      \"support\": \"no publication date mentioned\",\n",
      "      \"notes\": \"domain: film, range: date (implied)\"\n",
      "    },\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"characters\", \"not specified\"],\n",
      "      \"confidence\": 0.5,\n",
      "      \"support\": \"no characters mentioned\",\n",
      "      \"notes\": \"domain: film, range: film character (implied)\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[RUN] Wrote 1 records to /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Block 6 — Example Run\n",
    "\n",
    "ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "INPUT_JSONL   =  \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\" \n",
    "OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test.jsonl\" \n",
    "\n",
    "MAX_ITEMS      = 1 # None   # set 1 for quick smoke test\n",
    "MAX_NEW_TOKENS = 768\n",
    "TEMPERATURE    = 0.25\n",
    "VERBOSE        = True\n",
    "K_CANDIDATES   = 6\n",
    "\n",
    "first = next(read_jsonl(INPUT_JSONL, max_items=1), None)\n",
    "if first is None:\n",
    "    print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "else:\n",
    "    print(\"[DEBUG] First record keys:\", list(first.keys()))\n",
    "    print(\" id:\", first.get(\"id\"))\n",
    "    print(\" sent:\", (first.get(\"sent\") or \"\")[:160] + (\"...\" if first.get(\"sent\") and len(first[\"sent\"])>160 else \"\"))\n",
    "\n",
    "_ = run_pipeline_prompt1(\n",
    "    input_jsonl_path=INPUT_JSONL,\n",
    "    ontology_json_path=ONTOLOGY_JSON,\n",
    "    output_jsonl_path=OUTPUT_JSONL,\n",
    "    max_items=MAX_ITEMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    verbose=VERBOSE,\n",
    "    k=K_CANDIDATES,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d0245-05a9-488a-a883-8a64e7b0b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
