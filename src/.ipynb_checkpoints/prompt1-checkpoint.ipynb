{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871136a3-c3bc-470f-8197-ef09bca974ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 29 10:16:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8             31W /  370W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebf7ed9-365a-469c-958a-6025d355d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 925008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e23c67c-0223-4794-8fed-168b90b82fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limbo\n",
      "/opt/miniforge3/envs/jupyterhub/bin/python\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!which python\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053b5b96-8ffd-4a88-a0f6-213587cb6828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load the chat model + tokenizer and return a text-generation pipeline.\n",
    "    Uses half precision + device_map='auto' for efficiency.\n",
    "    \"\"\"\n",
    "    print(f\"[LOAD] model={model_id}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2bbfa7-bb4b-4cd7-81ad-c1ada06df7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Stream records from a .jsonl file.\n",
    "    Stops early if max_items is provided.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dicts as JSON lines.\n",
    "    Creates parent directory if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def extract_text_field(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the text field from an input record.\n",
    "    Returns (text_value, key_used).\n",
    "    Falls back to the longest string field if none of the preferred keys exist.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "\n",
    "    # fallback: choose longest string in record\n",
    "    best_key, best_val = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_val):\n",
    "            best_key, best_val = k, v\n",
    "    return best_val.strip(), best_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfd864e9-b799-481b-86f2-09da699b1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map any known identifier (qid/id/label) -> canonical label string.\n",
    "    This lets us convert domain/range IDs into human-readable names.\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for concept in ontology_json.get(\"concepts\", []):\n",
    "        label = str(concept.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            raw_val = concept.get(keyname)\n",
    "            if raw_val is None:\n",
    "                continue\n",
    "\n",
    "            sval = str(raw_val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _label_for(raw_val: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Convert domain/range IDs to readable labels.\n",
    "    Fallback to string form of raw_val.\n",
    "    \"\"\"\n",
    "    if raw_val is None:\n",
    "        return \"\"\n",
    "    rval = str(raw_val).strip()\n",
    "    return cindex.get(rval, rval)\n",
    "\n",
    "\n",
    "def render_concept_list(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of ontology concepts by label.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if label:\n",
    "            lines.append(f\"- {label}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def render_relation_list(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of relations with (domain, range) in human-readable form.\n",
    "    Format: - relationLabel(domainLabel,rangeLabel)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes so we can safely embed text\n",
    "    inside quoted blocks in the USER prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6803059-ebe5-4527-9f5d-40719f7d986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt1_system() -> str:\n",
    "    \"\"\"\n",
    "    SYSTEM message for Prompt 1.\n",
    "    Allows both ontology-aligned and non-ontology triples,\n",
    "    still returns strict JSON only.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are a KG triple proposer in a Tree-of-Thoughts loop. \"\n",
    "        \"First detect entity mentions and assign tentative ontology types. \"\n",
    "        \"Then propose candidate triples [subject, relation, object] that express factual statements in the text. \"\n",
    "        \"You MUST include:\\n\"\n",
    "        \"1) triples whose relation/domain/range matches the ontology, AND\\n\"\n",
    "        \"2) any other clearly stated factual triples in the text even if the relation or types are not present in the ontology.\\n\"\n",
    "        \"Return only JSON. Do not include any natural language outside JSON.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_prompt1_user(\n",
    "    TEXT: str,\n",
    "    ontology_json: Dict[str, Any],\n",
    "    k: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    USER message for Prompt 1.\n",
    "    Updated so the model will not suppress non-ontology triples.\n",
    "    We keep the same JSON schema (mentions[], triples[]),\n",
    "    but adjust the task + constraints language.\n",
    "    \"\"\"\n",
    "    concept_block = render_concept_list(ontology_json)\n",
    "    relation_block = render_relation_list(ontology_json)\n",
    "\n",
    "    return dedent(f\"\"\"\n",
    "    Task:\n",
    "    1) From the text, list detected entity mentions with tentative ontology types.\n",
    "    2) Propose up to k={k} candidate triples [subject, relation, object].\n",
    "\n",
    "    VERY IMPORTANT:\n",
    "    - You MUST include all explicit factual triples stated in the text, even if the relation,\n",
    "      subject type, or object type is not listed in the ontology.\n",
    "    - ALSO include ontology-valid triples whose domain/range matches the ontology relations.\n",
    "\n",
    "    For each triple, include confidence ∈ [0,1] and cite the exact supporting span(s).\n",
    "\n",
    "    Text\n",
    "    \"{_escape_multiline(TEXT)}\"\n",
    "\n",
    "    Ontology concepts\n",
    "    {concept_block}\n",
    "\n",
    "    Ontology relations (domain → range)\n",
    "    {relation_block}\n",
    "\n",
    "    Output format (JSON only)\n",
    "    {{\n",
    "      \"mentions\": [\n",
    "        {{\"surface\": \"...\", \"type_candidates\": [\"ConceptA\",\"ConceptB\"], \"span\": [start,end]}}\n",
    "      ],\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,\n",
    "          \"support\": \"exact quote from text\",\n",
    "          \"notes\": \"why this triple is supported; if ontology applies, explain domain/range fit. If not in ontology, say 'not in ontology but supported by text'.\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Constraints\n",
    "    - Extract ALL clearly stated factual triples in the text.\n",
    "    - If a triple matches an ontology relation, enforce domain→range consistency and mention that in notes.\n",
    "    - If a triple does NOT match any ontology relation, you MUST STILL include it (do not discard it).\n",
    "    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n",
    "    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n",
    "    - Do not invent entities that are not mentioned in the text.\n",
    "    - Output MUST be valid JSON and nothing else.\n",
    "    \"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e63193-fb71-4ca6-abaa-9cc4df6412d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    system_msg: str,\n",
    "    user_msg: str,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run the chat-style generation and return the raw generated text.\n",
    "    We assume the model will output ONLY the JSON object.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    out = generator(\n",
    "        prompt_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # HF pipeline can return dicts or strings depending on version\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a20fdf-9b3e-4ab3-9d95-0bdce3698264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "def robust_parse_model_output(raw_response: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try very hard to get structured data out of the model response.\n",
    "    Returns a dict with at least:\n",
    "      {\n",
    "        \"triples\": [ [head, rel, tail], ... ],\n",
    "        \"mentions\": [...] or None,\n",
    "        \"raw_json_obj\": ... or None\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Try direct parse\n",
    "    try:\n",
    "        obj = json.loads(raw_response)\n",
    "        return {\n",
    "            \"triples\": extract_triples_from_obj(obj),\n",
    "            \"mentions\": obj.get(\"mentions\"),\n",
    "            \"raw_json_obj\": obj,\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2. Try slice from first { to last }\n",
    "    try:\n",
    "        start_i = raw_response.find(\"{\")\n",
    "        end_i = raw_response.rfind(\"}\")\n",
    "        if start_i != -1 and end_i != -1 and end_i > start_i:\n",
    "            candidate = raw_response[start_i:end_i+1]\n",
    "            obj = json.loads(candidate)\n",
    "            return {\n",
    "                \"triples\": extract_triples_from_obj(obj),\n",
    "                \"mentions\": obj.get(\"mentions\"),\n",
    "                \"raw_json_obj\": obj,\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3. Fallback: regex mine triples from messy text\n",
    "    triples = extract_triples_via_regex(raw_response)\n",
    "\n",
    "    return {\n",
    "        \"triples\": triples,\n",
    "        \"mentions\": None,\n",
    "        \"raw_json_obj\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_triples_from_obj(obj: Any) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Safely pull triples out of a parsed JSON object,\n",
    "    accounting for [\"h\",\"r\",\"t\"] or {\"triple\":[...]} formats.\n",
    "    \"\"\"\n",
    "    results: List[List[str]] = []\n",
    "\n",
    "    # if it's dict-like and has \"triples\"\n",
    "    if isinstance(obj, dict) and \"triples\" in obj:\n",
    "        for item in obj[\"triples\"]:\n",
    "            # item could be [\"h\",\"r\",\"t\"]\n",
    "            if isinstance(item, list) and len(item) >= 3:\n",
    "                results.append(item[:3])\n",
    "            # item could be { \"triple\": [\"h\",\"r\",\"t\"], ... }\n",
    "            elif isinstance(item, dict) and \"triple\" in item:\n",
    "                tval = item[\"triple\"]\n",
    "                if isinstance(tval, list) and len(tval) >= 3:\n",
    "                    results.append(tval[:3])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_triples_via_regex(raw_text: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Ultra-forgiving fallback.\n",
    "    Finds patterns like:\n",
    "      \"triple\": [\"Head\", \"Rel\", \"Tail\"]\n",
    "    even if the outer JSON is broken.\n",
    "    \"\"\"\n",
    "    triples: List[List[str]] = []\n",
    "\n",
    "    # This regex:\n",
    "    # - looks for \"triple\": [ \"....\", \"....\", \"....\" ]\n",
    "    # - captures the 3 strings inside\n",
    "    triple_pattern = re.compile(\n",
    "        r'\"triple\"\\s*:\\s*\\[\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\]'\n",
    "    )\n",
    "\n",
    "    for match in triple_pattern.finditer(raw_text):\n",
    "        h, r, t = match.groups()\n",
    "        triples.append([h, r, t])\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09581aab-b4bc-423c-b914-e07e46614035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt1_pipeline(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    k: int = 6,\n",
    "    max_items: Optional[int] = None,\n",
    "    max_new_tokens: int = 900,\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Core batch function for Prompt 1.\n",
    "    Reads each row from input_jsonl_path,\n",
    "    runs Prompt 1 on it,\n",
    "    and writes a JSONL of model outputs with parsed JSON.\n",
    "    Uses robust parsing to recover triples even from malformed model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # load ontology once\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology_json = json.load(f)\n",
    "\n",
    "    sys_text = build_prompt1_system()\n",
    "    outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "    for idx, rec in enumerate(read_jsonl(input_jsonl_path, max_items=max_items)):\n",
    "        # extract text from record\n",
    "        text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "        # build user prompt\n",
    "        usr_text = build_prompt1_user(\n",
    "            TEXT=text_val,\n",
    "            ontology_json=ontology_json,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[ITEM {idx}] text_key={text_key}\")\n",
    "            print(f\"[PROMPT_USER] {usr_text[:320]} ...\")\n",
    "\n",
    "        # generate model response\n",
    "        raw_response = generate_model_response(\n",
    "            generator=generator,\n",
    "            tokenizer=tokenizer,\n",
    "            system_msg=sys_text,\n",
    "            user_msg=usr_text,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # robust parsing of model output\n",
    "        parsed_bundle = robust_parse_model_output(raw_response)\n",
    "\n",
    "        # construct output record\n",
    "        out_record = {\n",
    "            \"id\": rec.get(\"id\"),\n",
    "            \"input text\": text_val,\n",
    "            \"prompts\": {\n",
    "                \"system_prompt\": sys_text,\n",
    "                \"user_prompt\": usr_text,\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"LLM_output\": raw_response,\n",
    "                \"json\": parsed_bundle.get(\"raw_json_obj\"),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        outputs.append(out_record)\n",
    "\n",
    "    # write all outputs\n",
    "    write_jsonl(output_jsonl_path, outputs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DONE] wrote {len(outputs)} records -> {output_jsonl_path}\")\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5d8192-dc0a-4a05-8051-2580354b6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pprint\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "# INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "# OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test1.jsonl\"\n",
    "\n",
    "# MAX_ITEMS        = 1          # how many examples to actually run through model\n",
    "# MAX_NEW_TOKENS   = 900\n",
    "# TEMPERATURE      = 0.25\n",
    "# VERBOSE          = True\n",
    "# K_CANDIDATES     = 6\n",
    "\n",
    "# # --- 1. Quick peek at input file ---\n",
    "# peek_items = list(read_jsonl(INPUT_JSONL, max_items=3))\n",
    "\n",
    "# if not peek_items:\n",
    "#     print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "# else:\n",
    "#     print(f\"[DEBUG] Loaded {len(peek_items)} sample record(s) from {INPUT_JSONL}\")\n",
    "#     for i, rec in enumerate(peek_items):\n",
    "#         text_val, text_key = extract_text_field(rec)\n",
    "#         print(f\"\\n--- SAMPLE {i} ---\")\n",
    "#         print(\"[keys]:\", list(rec.keys()))\n",
    "#         print(\" id:\", rec.get(\"id\"))\n",
    "#         print(f\" chosen_text_key: {text_key}\")\n",
    "#         preview = text_val[:200] + (\"...\" if len(text_val) > 200 else \"\")\n",
    "#         print(\" text preview:\", preview)\n",
    "\n",
    "# # --- 2. Load ontology once for prompt inspection ---\n",
    "# with open(ONTOLOGY_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "#     ontology_data = json.load(f)\n",
    "\n",
    "# concept_block_dbg  = render_concept_list(ontology_data)\n",
    "# relation_block_dbg = render_relation_list(ontology_data)\n",
    "\n",
    "# print(\"\\n[DEBUG] ONTOLOGY CONCEPT LIST (truncated):\")\n",
    "# print(concept_block_dbg[:500] + (\"...\" if len(concept_block_dbg) > 500 else \"\"))\n",
    "\n",
    "# print(\"\\n[DEBUG] ONTOLOGY RELATION LIST (truncated):\")\n",
    "# print(relation_block_dbg[:500] + (\"...\" if len(relation_block_dbg) > 500 else \"\"))\n",
    "\n",
    "# # --- 3. Show the exact SYSTEM and USER prompt that will go to the model for the FIRST sample ---\n",
    "# if peek_items:\n",
    "#     sample_text, _ = extract_text_field(peek_items[0])\n",
    "\n",
    "#     system_prompt_dbg = build_prompt1_system()\n",
    "#     user_prompt_dbg   = build_prompt1_user(\n",
    "#         TEXT=sample_text,\n",
    "#         ontology_json=ontology_data,\n",
    "#         k=K_CANDIDATES,\n",
    "#     )\n",
    "\n",
    "#     print(\"\\n================ [SYSTEM PROMPT] ================\")\n",
    "#     print(system_prompt_dbg)\n",
    "\n",
    "#     print(\"\\n================ [USER PROMPT - FIRST SAMPLE] ================\")\n",
    "#     # we don't print the entire ontology if it's massive, but we already showed truncated above\n",
    "#     # still, show first ~1200 chars so you can visually inspect formatting\n",
    "#     up_prev = user_prompt_dbg[:12000]\n",
    "#     print(up_prev)\n",
    "#     if len(user_prompt_dbg) > 12000:\n",
    "#         print(\"... [USER PROMPT TRUNCATED FOR DISPLAY] ...\")\n",
    "\n",
    "# # --- 4. Spin up the model ---\n",
    "# generator, tokenizer = setup_model()\n",
    "\n",
    "# # --- 5. Dry-run: generate ONLY for the first record (no batch write yet),\n",
    "# #         so we can inspect the raw model output and parsed JSON.\n",
    "# if peek_items:\n",
    "#     one_text, _ = extract_text_field(peek_items[0])\n",
    "#     raw_single = generate_model_response(\n",
    "#         generator=generator,\n",
    "#         tokenizer=tokenizer,\n",
    "#         system_msg=build_prompt1_system(),\n",
    "#         user_msg=build_prompt1_user(\n",
    "#             TEXT=one_text,\n",
    "#             ontology_json=ontology_data,\n",
    "#             k=K_CANDIDATES,\n",
    "#         ),\n",
    "#         max_new_tokens=MAX_NEW_TOKENS,\n",
    "#         temperature=TEMPERATURE,\n",
    "#     )\n",
    "\n",
    "#     print(\"\\n================ [RAW MODEL OUTPUT - FIRST SAMPLE] ================\")\n",
    "#     print(raw_single[:150000] + (\"...\" if len(raw_single) > 150000 else \"\"))\n",
    "\n",
    "#     parsed_single = None\n",
    "#     try:\n",
    "#         parsed_single = json.loads(raw_single)\n",
    "#     except Exception:\n",
    "#         try:\n",
    "#             start_i = raw_single.find(\"{\")\n",
    "#             end_i   = raw_single.rfind(\"}\")\n",
    "#             if start_i != -1 and end_i != -1 and end_i > start_i:\n",
    "#                 candidate = raw_single[start_i:end_i+1]\n",
    "#                 parsed_single = json.loads(candidate)\n",
    "#         except Exception:\n",
    "#             parsed_single = None\n",
    "\n",
    "#     print(\"\\n================ [PARSED MODEL JSON - FIRST SAMPLE] ================\")\n",
    "#     pprint.pprint(parsed_single, width=120)\n",
    "\n",
    "#     # Sanity check for evaluator compatibility:\n",
    "#     if parsed_single and \"triples\" in parsed_single:\n",
    "#         print(\"\\n[CHECK] triples[0] example for evaluator compatibility:\")\n",
    "#         if parsed_single[\"triples\"]:\n",
    "#             pprint.pprint(parsed_single[\"triples\"][0], width=100)\n",
    "#         else:\n",
    "#             print(\"No triples returned.\")\n",
    "#     else:\n",
    "#         print(\"[WARN] Model output did not parse into expected {'mentions':..., 'triples':...} shape.\")\n",
    "\n",
    "# # --- 6. Full mini-pipeline run (writes OUTPUT_JSONL) on first MAX_ITEMS records ---\n",
    "# print(\"\\n================ [BATCH PIPELINE RUN] ================\")\n",
    "# batch_outputs = run_prompt1_pipeline(\n",
    "#     generator=generator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     ontology_json_path=ONTOLOGY_JSON,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     max_new_tokens=MAX_NEW_TOKENS,\n",
    "#     temperature=TEMPERATURE,\n",
    "#     verbose=VERBOSE,\n",
    "#     k=K_CANDIDATES,\n",
    "# )\n",
    "\n",
    "# print(f\"\\n[SUCCESS] Wrote {len(batch_outputs)} items to {OUTPUT_JSONL}\")\n",
    "\n",
    "# # --- 7. Peek at what was written (first 1-2 records) ---\n",
    "# print(\"\\n================ [WRITTEN OUTPUT PREVIEW] ================\")\n",
    "# for i, rec in enumerate(batch_outputs[:2]):\n",
    "#     print(f\"\\n--- OUTPUT ITEM {i} ---\")\n",
    "#     # show keys and parsed json triples\n",
    "#     print(\"[output keys]:\", list(rec.keys()))\n",
    "#     resp = rec.get(\"response\", {})\n",
    "#     parsed_json = resp.get(\"json\")\n",
    "#     if parsed_json:\n",
    "#         print(\"[triples exists? ]\", \"triples\" in parsed_json)\n",
    "#         if \"triples\" in parsed_json and parsed_json[\"triples\"]:\n",
    "#             print(\"[first triple]:\")\n",
    "#             pprint.pprint(parsed_json[\"triples\"][0], width=100)\n",
    "#     else:\n",
    "#         print(\"[WARN] No parsed JSON for this item.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e60e0-55cb-4f41-8268-e29c2c875647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974ba25-6848-4a35-9a93-3cc1a409dee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c499b0-51c8-4c0a-9b7d-d60a11b84d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f0a737-d4ea-4d61-b775-c34b39cf0e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fd75c-3edc-4b62-a4ae-5e3c556ee89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054edb2-84ff-4c63-bf5f-23fb4dcc1787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f89ebd-35d4-4a3f-b40f-ad700acc088c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9141775-d659-40ff-9083-c6397cb11c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# WIKIDATA BATCH RUN\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ont_{index}_{category}_test.jsonl  ->\n",
    "# {index}_{category}_ontology.json,\n",
    "# ont_{index}_{category}_few_shot.jsonl,\n",
    "# ont_{index}_{category}_output.jsonl\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str, BASE_INPUT: str, BASE_ONTO: str, BASE_OUT: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "\n",
    "    return input_jsonl, ontology_json, output_jsonl, tag\n",
    "\n",
    "\n",
    "def run_wikidata_batch():\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        \"ont_1_movie_test.jsonl\",\n",
    "        \"ont_2_music_test.jsonl\",\n",
    "        \"ont_3_sport_test.jsonl\",\n",
    "        \"ont_4_book_test.jsonl\",\n",
    "        \"ont_5_military_test.jsonl\",\n",
    "        \"ont_6_building_test.jsonl\",\n",
    "        \"ont_7_tv_test.jsonl\",\n",
    "        \"ont_8_politician_test.jsonl\",\n",
    "        \"ont_9_organization_test.jsonl\",\n",
    "        \"ont_10_airport_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    generator, tokenizer = setup_model()\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # derive ontology index/category from filename\n",
    "            # e.g. \"ont_1_movie_test.jsonl\" -> idx=\"1\", cat=\"movie\"\n",
    "            m = re.match(r\"ont_(\\d+)_(.+?)_test\\.jsonl$\", fname)\n",
    "            if not m:\n",
    "                print(f\"[SKIP] can't parse filename pattern: {fname}\")\n",
    "                continue\n",
    "            idx, cat = m.group(1), m.group(2)\n",
    "\n",
    "            input_jsonl  = os.path.join(BASE_INPUT, fname)\n",
    "            # ontology_json = os.path.join(BASE_ONTO, f\"ont_{idx}_{cat}.json\")\n",
    "            ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "            output_jsonl = os.path.join(\n",
    "                BASE_OUT,\n",
    "                fname.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN] wikidata ont_{idx}_{cat}\")\n",
    "\n",
    "            run_prompt1_pipeline(\n",
    "                generator=generator,\n",
    "                tokenizer=tokenizer,\n",
    "                input_jsonl_path=input_jsonl,\n",
    "                ontology_json_path=ontology_json,\n",
    "                output_jsonl_path=output_jsonl,\n",
    "                max_items=None,\n",
    "                max_new_tokens=900,\n",
    "                temperature=0.25,\n",
    "                verbose=False,\n",
    "                k=6,\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE] wikidata ont_{idx}_{cat}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] wikidata {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d918558-8c3c-4fa0-ab8d-1bc84ce9265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_wikidata_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ad589-131c-4a0b-bec6-5ac1b7ec93f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724fbd0-c9e1-4c48-af0d-ea1b903b7ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5096f150-390f-4c9d-899b-4acb9f9209a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b7d39c-d30e-4a03-a809-081a904a9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# DBPEDIA BATCH RUN\n",
    "########################################\n",
    "\n",
    "\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str, BASE_INPUT: str, BASE_ONTO: str, BASE_OUT: str):\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    # ont_{idx}_{cat}_test.jsonl -> ont_{idx}_{cat}_output.jsonl\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl = os.path.join(BASE_OUT, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "\n",
    "    return input_jsonl, ontology_json, output_jsonl, tag\n",
    "\n",
    "\n",
    "\n",
    "def run_dbpedia_batch():\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/\"\n",
    "    \n",
    "    \n",
    "    FILENAMES = [\n",
    "        # \"ont_12_monument_test.jsonl\",\n",
    "        # \"ont_1_university_test.jsonl\",\n",
    "        # \"ont_2_musicalwork_test.jsonl\",\n",
    "        # \"ont_3_airport_test.jsonl\",\n",
    "        \"ont_4_building_test.jsonl\",\n",
    "        \"ont_5_athlete_test.jsonl\",\n",
    "        \"ont_6_politician_test.jsonl\",\n",
    "        \"ont_7_company_test.jsonl\",\n",
    "        \"ont_8_celestialbody_test.jsonl\",\n",
    "        \"ont_9_astronaut_test.jsonl\",\n",
    "        \"ont_10_comicscharacter_test.jsonl\",\n",
    "        \"ont_11_meanoftransportation_test.jsonl\",\n",
    "        \"ont_13_food_test.jsonl\",\n",
    "        \"ont_14_writtenwork_test.jsonl\",\n",
    "        \"ont_15_sportsteam_test.jsonl\",\n",
    "        \"ont_16_city_test.jsonl\",\n",
    "        \"ont_17_artist_test.jsonl\",\n",
    "        \"ont_18_scientist_test.jsonl\",\n",
    "        \"ont_19_film_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    generator, tokenizer = setup_model()\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # derive ontology index/category from filename\n",
    "            # e.g. \"ont_1_movie_test.jsonl\" -> idx=\"1\", cat=\"movie\"\n",
    "            m = re.match(r\"ont_(\\d+)_(.+?)_test\\.jsonl$\", fname)\n",
    "            if not m:\n",
    "                print(f\"[SKIP] can't parse filename pattern: {fname}\")\n",
    "                continue\n",
    "            idx, cat = m.group(1), m.group(2)\n",
    "\n",
    "            input_jsonl  = os.path.join(BASE_INPUT, fname)\n",
    "            # ontology_json = os.path.join(BASE_ONTO, f\"ont_{idx}_{cat}.json\")\n",
    "            ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "            output_jsonl = os.path.join(\n",
    "                BASE_OUT,\n",
    "                fname.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN] dbpedia ont_{idx}_{cat}\")\n",
    "\n",
    "            run_prompt1_pipeline(\n",
    "                generator=generator,\n",
    "                tokenizer=tokenizer,\n",
    "                input_jsonl_path=input_jsonl,\n",
    "                ontology_json_path=ontology_json,\n",
    "                output_jsonl_path=output_jsonl,\n",
    "                max_items=None,\n",
    "                max_new_tokens=900,\n",
    "                temperature=0.25,\n",
    "                verbose=False,\n",
    "                k=6,\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE] wikidata ont_{idx}_{cat}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] wikidata {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50c556-6446-472a-a3d7-6835160a20d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0c72dc579c44a3b77ebebb4d5332cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_4_building\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] wikidata ont_4_building\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_5_athlete\n",
      "[DONE] wikidata ont_5_athlete\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_6_politician\n",
      "[DONE] wikidata ont_6_politician\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_7_company\n",
      "[DONE] wikidata ont_7_company\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_8_celestialbody\n",
      "[DONE] wikidata ont_8_celestialbody\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_9_astronaut\n",
      "[DONE] wikidata ont_9_astronaut\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_10_comicscharacter\n",
      "[DONE] wikidata ont_10_comicscharacter\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_11_meanoftransportation\n",
      "[DONE] wikidata ont_11_meanoftransportation\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_13_food\n",
      "[DONE] wikidata ont_13_food\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_14_writtenwork\n",
      "[DONE] wikidata ont_14_writtenwork\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_15_sportsteam\n",
      "[DONE] wikidata ont_15_sportsteam\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_16_city\n"
     ]
    }
   ],
   "source": [
    "run_dbpedia_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607dd9f-a9f2-49c7-9632-15c51468c55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
