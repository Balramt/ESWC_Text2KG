{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a562f097-b19d-4c65-be00-bd9a53ec5ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 29 20:04:20 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 87%   74C    P0            308W /  370W |   14658MiB /  24576MiB |     53%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    977424      C   ....conda/envs/kg_pipeline/bin/python3      14648MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe4feed-deef-46e9-8a29-5fa4a1f70e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (1023795) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 1023795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba3e522e-b082-418d-85f8-dc6e51b04a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limbo\n",
      "/opt/miniforge3/envs/jupyterhub/bin/python\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!which python\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28bc8447-99bb-4400-a759-80c6f9cfb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 1: IMPORTS / GLOBAL CONFIG / MODEL SETUP\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "# -------- Runtime config (edit these before running Block 13 manual test) --------\n",
    "ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "\n",
    "MODEL_ID   = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "K_TRIPLES  = 1        # \"extract up to {k} triples\"\n",
    "MAX_ITEMS  = 2     # None = use all rows from INPUT_JSONL\n",
    "VERBOSE    = True     # default True for manual testing first\n",
    "\n",
    "\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load the chat model + tokenizer and return a text-generation pipeline.\n",
    "    Uses half precision + device_map='auto' for efficiency.\n",
    "    \"\"\"\n",
    "    print(f\"[LOAD] model={model_id}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d85a8a0-3c6a-4586-b569-565d19580104",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 2: DATA I/O HELPERS\n",
    "########################################\n",
    "\n",
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Stream records from a .jsonl file.\n",
    "    Stops early if max_items is provided.\n",
    "    Yields dicts.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dicts as JSON lines.\n",
    "    Creates parent directory if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def extract_text_field(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the text field from an input record.\n",
    "    Returns (text_value, key_used).\n",
    "    Falls back to the longest string field if none of the preferred keys exist.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "\n",
    "    # fallback: choose longest string in record\n",
    "    best_key, best_val = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_val):\n",
    "            best_key, best_val = k, v\n",
    "    return best_val.strip(), best_key\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes so we can safely embed text\n",
    "    inside quoted blocks in the USER prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87a61603-701b-417a-993e-aa98038fe4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 3: ONTOLOGY HELPERS + PROMPT 2 MESSAGE BUILDERS\n",
    "########################################\n",
    "\n",
    "def load_ontology_json(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load ontology JSON file.\n",
    "    Expected structure:\n",
    "      {\n",
    "        \"concepts\": [\n",
    "          {\"id\": \"...\", \"qid\": \"...\", \"label\": \"SomeClass\"},\n",
    "          ...\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"id\": \"...\",\n",
    "            \"label\": \"location\",\n",
    "            \"domain\": \"SomeConceptID\",\n",
    "            \"range\": \"SomeConceptID\"\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map any known identifier (qid/id/label) -> canonical label string.\n",
    "    This lets us convert domain/range IDs into human-readable names.\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for concept in ontology_json.get(\"concepts\", []):\n",
    "        label = str(concept.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            raw_val = concept.get(keyname)\n",
    "            if raw_val is None:\n",
    "                continue\n",
    "\n",
    "            sval = str(raw_val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _label_for(raw_val: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Convert domain/range IDs to readable labels.\n",
    "    Fallback to string form of raw_val.\n",
    "    \"\"\"\n",
    "    if raw_val is None:\n",
    "        return \"\"\n",
    "    rval = str(raw_val).strip()\n",
    "    return cindex.get(rval, rval)\n",
    "\n",
    "\n",
    "def format_ontology_concepts(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of ontology concepts by label.\n",
    "    We'll present these to the model.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if label:\n",
    "            lines.append(f\"- {label}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def format_ontology_relations(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of relations with (domain, range) in human-readable form.\n",
    "    Format: - relationLabel(domainLabel,rangeLabel)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def build_p2_system() -> str:\n",
    "    \"\"\"\n",
    "    System message for Prompt 2.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are a KG triple extractor. \"\n",
    "        \"Match relation cues in the text and return only triples that satisfy the \"\n",
    "        \"ontology’s domain→range. Cite exact evidence. Output JSON only.\"\n",
    "    )\n",
    "\n",
    "def build_p2_user(text: str, ontology_json: Dict[str, Any], k: int) -> str:\n",
    "    \"\"\"\n",
    "    Prompt 2 user message:\n",
    "    - No few-shot examples\n",
    "    - No per-domain lists\n",
    "    - Includes a universal explanation of how to recognize and record lexical cues\n",
    "    \"\"\"\n",
    "\n",
    "    ontology_concepts_block = format_ontology_concepts(ontology_json)\n",
    "    ontology_relations_block = format_ontology_relations(ontology_json)\n",
    "\n",
    "    return dedent(f\"\"\"\\\n",
    "    Task: Using explicit lexical cues found in the text, extract up to {k} triples [subject, relation, object].\n",
    "    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n",
    "\n",
    "    CUE GENERATION GUIDANCE:\n",
    "    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n",
    "      and signals the relation between them.\n",
    "    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n",
    "        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n",
    "      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n",
    "    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n",
    "    - The cue expresses the natural-language realization of the ontology relation.\n",
    "    - If no such linking phrase appears in the text for a given relation, skip that relation.\n",
    "    - Do NOT invent cues or use world knowledge; work only from the surface text.\n",
    "\n",
    "    PROCEDURE:\n",
    "    1. Identify candidate subjects and objects that match ontology domain and range types.\n",
    "    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n",
    "    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n",
    "    4. If none is found, do not emit a triple for that relation.\n",
    "    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n",
    "\n",
    "    Text:\n",
    "    \"{_escape_multiline(text)}\"\n",
    "\n",
    "    Ontology concepts:\n",
    "    {ontology_concepts_block}\n",
    "\n",
    "    Ontology relations (domain → range):\n",
    "    {ontology_relations_block}\n",
    "\n",
    "    Output JSON only in this exact schema:\n",
    "    {{\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,  // confidence 0–1\n",
    "          \"cue\": \"matched cue phrase from text\",\n",
    "          \"support\": \"exact quoted span(s)\",\n",
    "          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    - Work only from visible text evidence.\n",
    "    - If no relation cue appears, return an empty list of triples.\n",
    "    - Do NOT invent or assume facts not in text.\n",
    "    - Always quote exact spans for support and cue.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e294b47f-a711-4e30-b2f3-e2ca6ba00975",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 4: GENERATION + PARSER HELPERS\n",
    "########################################\n",
    "\n",
    "def generate_raw_json(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build chat-style prompt for Prompt 2 and get model output.\n",
    "    We expect JSON-only, but we'll still post-parse later.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    out = generator(\n",
    "        prompt_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # HF pipeline returns list[{\"generated_text\": \"...\"}]\n",
    "    if isinstance(out[0], dict) and \"generated_text\" in out[0]:\n",
    "        return out[0][\"generated_text\"].strip()\n",
    "    else:\n",
    "        # fallback\n",
    "        return str(out[0]).strip()\n",
    "\n",
    "\n",
    "def try_parse_json(raw: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Best-effort parse of model output (should be JSON).\n",
    "    1. direct json.loads\n",
    "    2. fallback: grab first {...} block\n",
    "    \"\"\"\n",
    "    raw_strip = raw.strip()\n",
    "\n",
    "    # direct attempt\n",
    "    try:\n",
    "        return json.loads(raw_strip)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: regex for first {...}\n",
    "    m = re.search(r\"\\{.*\\}\", raw_strip, flags=re.DOTALL)\n",
    "    if m:\n",
    "        block = m.group(0)\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "632b2df6-b110-45c6-abc2-ebf4133c37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 5: SINGLE-FILE PIPELINE (Prompt 2)\n",
    "########################################\n",
    "\n",
    "\n",
    "def run_pipeline_prompt2(\n",
    "    ontology_path: str,\n",
    "    input_jsonl_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    k_triples: int = 5,\n",
    "    max_items: Optional[int] = None,\n",
    "    verbose: bool = True,\n",
    "    model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    generator=None,\n",
    "    tokenizer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Prompt 2 over a single dataset file:\n",
    "      - load ontology + rows\n",
    "      - build prompts\n",
    "      - generate model output\n",
    "      - parse JSON\n",
    "      - write trace jsonl\n",
    "\n",
    "    Behavior:\n",
    "      • If `generator` and `tokenizer` are provided, reuse them (no new model load).\n",
    "      • Otherwise, load the model from `model_id` internally (backward compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. load ontology\n",
    "    ontology_json = load_ontology_json(ontology_path)\n",
    "\n",
    "    # 2. init / reuse model\n",
    "    local_model_loaded = False\n",
    "    if generator is None or tokenizer is None:\n",
    "        generator, tokenizer = setup_model(model_id=model_id)\n",
    "        local_model_loaded = True  # so we know we \"own\" it, if you ever want cleanup\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 3. iterate input rows\n",
    "    for idx, rec in enumerate(read_jsonl(input_jsonl_path, max_items=max_items)):\n",
    "        rec_id = str(rec.get(\"id\") or f\"item_{idx}\")\n",
    "        text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "        sys_prompt = build_p2_system()\n",
    "        usr_prompt = build_p2_user(text_val, ontology_json, k_triples)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"======================================\")\n",
    "            print(f\"[ID] {rec_id}\")\n",
    "            print(f\"[TEXT_KEY] {text_key}\")\n",
    "            print(\"[SYSTEM PROMPT]\\n\", sys_prompt)\n",
    "            print(\"[USER PROMPT]\\n\", usr_prompt)\n",
    "            print(\"[SOURCE TEXT]\\n\", text_val)\n",
    "\n",
    "        raw_response = generate_raw_json(\n",
    "            generator=generator,\n",
    "            tokenizer=tokenizer,\n",
    "            system_prompt=sys_prompt,\n",
    "            user_prompt=usr_prompt,\n",
    "            max_new_tokens=768,\n",
    "            temperature=0.25,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        parsed_json = try_parse_json(raw_response)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"[RAW RESPONSE]\\n\", raw_response)\n",
    "            print(\"[PARSED JSON]\\n\", parsed_json)\n",
    "\n",
    "        out_record = {\n",
    "            \"id\": rec_id,\n",
    "            \"input text\": text_val,\n",
    "            \"prompts\": {\n",
    "                \"system_prompt\": sys_prompt,\n",
    "                \"user_prompt\": usr_prompt,\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"LLM_output\": raw_response,\n",
    "                \"json\": parsed_json,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        results.append(out_record)\n",
    "\n",
    "    # 4. write collected results\n",
    "    write_jsonl(output_jsonl_path, results)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n[P2 WRITE] {len(results)} rows -> {output_jsonl_path}\")\n",
    "\n",
    "    # optional: if we loaded the model here locally we *could* free VRAM,\n",
    "    # but usually you keep it for interactive use.\n",
    "    # if local_model_loaded:\n",
    "    #     torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d6081-fb76-4586-98c2-d063a0a88571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed1a7d6-2745-478f-9b6d-680b2be6e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########################################\n",
    "# # BLOCK 9: MANUAL TEST RUN (single file Prompt 2)\n",
    "# ########################################\n",
    "\n",
    "# run_pipeline_prompt2(\n",
    "#     ontology_path=ONTOLOGY_JSON,\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     k_triples=K_TRIPLES,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     verbose=VERBOSE,   #True for first check\n",
    "#     model_id=MODEL_ID,\n",
    "#     generator=None,\n",
    "#     tokenizer=None,\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee295023-5e21-4372-b31f-6442bbd86140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23898c-aef2-4536-af9c-409d998f5a74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d47ae3f-d3ee-487d-b006-b81cb50165ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Given something like 'ont_8_politician_test.jsonl', build:\n",
    "      - input_jsonl_path      -> <base_input>/ont_8_politician_test.jsonl\n",
    "      - ontology_json_path    -> <base_onto>/8_politician_ontology.json\n",
    "      - output_jsonl_path     -> <base_out>/ont_8_politician_output.jsonl\n",
    "      - tag                   -> 'ont_8_politician'\n",
    "    \"\"\"\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_wikidata_batch_p2(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        \"ont_1_movie_test.jsonl\",\n",
    "        \"ont_2_music_test.jsonl\",\n",
    "        \"ont_3_sport_test.jsonl\",\n",
    "        \"ont_4_book_test.jsonl\",\n",
    "        \"ont_5_military_test.jsonl\",\n",
    "        \"ont_6_building_test.jsonl\",\n",
    "        \"ont_7_tv_test.jsonl\",\n",
    "        \"ont_8_politician_test.jsonl\",\n",
    "        \"ont_9_organization_test.jsonl\",\n",
    "        \"ont_10_airport_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # 1. load the model ONCE and reuse it for all files\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    # 2. loop over all files\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # build all required paths for this file using the shared logic\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_paths(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P2] wikidata {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            # 3. run the single-file pipeline, reusing the loaded model\n",
    "            run_pipeline_prompt2(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES,\n",
    "                max_items=None,\n",
    "                verbose=False,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse\n",
    "                tokenizer=tokenizer,   # reuse\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P2] wikidata {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P2] wikidata {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0032ed-4311-4abf-aa49-ee532b2f43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c2a70ab30346e392866083a7d3e94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_1_movie\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_1_movie_output.jsonl\n",
      "======================================\n",
      "[ID] ont_1_movie_test_1\n",
      "[TEXT_KEY] sent\n",
      "[SYSTEM PROMPT]\n",
      " You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.\n",
      "[USER PROMPT]\n",
      "     Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n",
      "    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n",
      "\n",
      "    CUE GENERATION GUIDANCE:\n",
      "    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n",
      "      and signals the relation between them.\n",
      "    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n",
      "        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n",
      "      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n",
      "    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n",
      "    - The cue expresses the natural-language realization of the ontology relation.\n",
      "    - If no such linking phrase appears in the text for a given relation, skip that relation.\n",
      "    - Do NOT invent cues or use world knowledge; work only from the surface text.\n",
      "\n",
      "    PROCEDURE:\n",
      "    1. Identify candidate subjects and objects that match ontology domain and range types.\n",
      "    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n",
      "    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n",
      "    4. If none is found, do not emit a triple for that relation.\n",
      "    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n",
      "\n",
      "    Text:\n",
      "    \"Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\"\n",
      "\n",
      "    Ontology concepts:\n",
      "    - human\n",
      "- city\n",
      "- country\n",
      "- film\n",
      "- film genre\n",
      "- genre\n",
      "- film production company\n",
      "- film award\n",
      "- award\n",
      "- written work\n",
      "- film character\n",
      "- film organization\n",
      "\n",
      "    Ontology relations (domain → range):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    Output JSON only in this exact schema:\n",
      "    {\n",
      "      \"triples\": [\n",
      "        {\n",
      "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
      "          \"confidence\": 0.0,  // confidence 0–1\n",
      "          \"cue\": \"matched cue phrase from text\",\n",
      "          \"support\": \"exact quoted span(s)\",\n",
      "          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "\n",
      "    Rules:\n",
      "    - Work only from visible text evidence.\n",
      "    - If no relation cue appears, return an empty list of triples.\n",
      "    - Do NOT invent or assume facts not in text.\n",
      "    - Always quote exact spans for support and cue.\n",
      "\n",
      "[SOURCE TEXT]\n",
      " Bleach: Hell Verse (Japanese: BLEACH , Hepburn: BurÄ«chi Jigoku-Hen) is a 2010 Japanese animated film directed by Noriyuki Abe.\n",
      "[RAW RESPONSE]\n",
      " {\n",
      "  \"triples\": [\n",
      "    {\n",
      "      \"triple\": [\"Bleach: Hell Verse\", \"directed by\", \"Noriyuki Abe\"],\n",
      "      \"confidence\": 1.0,\n",
      "      \"cue\": \"directed by\",\n",
      "      \"support\": \"Noriyuki Abe\",\n",
      "      \"notes\": \"domain/range check: film (subject) - human (range)\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[PARSED JSON]\n",
      " {'triples': [{'triple': ['Bleach: Hell Verse', 'directed by', 'Noriyuki Abe'], 'confidence': 1.0, 'cue': 'directed by', 'support': 'Noriyuki Abe', 'notes': 'domain/range check: film (subject) - human (range)'}]}\n",
      "======================================\n",
      "[ID] ont_1_movie_test_2\n",
      "[TEXT_KEY] sent\n",
      "[SYSTEM PROMPT]\n",
      " You are a KG triple extractor. Match relation cues in the text and return only triples that satisfy the ontology’s domain→range. Cite exact evidence. Output JSON only.\n",
      "[USER PROMPT]\n",
      "     Task: Using explicit lexical cues found in the text, extract up to 1 triples [subject, relation, object].\n",
      "    Enforce ontology domain→range strictly; if a triple is invalid, omit it.\n",
      "\n",
      "    CUE GENERATION GUIDANCE:\n",
      "    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n",
      "      and signals the relation between them.\n",
      "    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n",
      "        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n",
      "      These are only illustrative examples — you must infer the appropriate cue for each relation label and sentence.\n",
      "    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n",
      "    - The cue expresses the natural-language realization of the ontology relation.\n",
      "    - If no such linking phrase appears in the text for a given relation, skip that relation.\n",
      "    - Do NOT invent cues or use world knowledge; work only from the surface text.\n",
      "\n",
      "    PROCEDURE:\n",
      "    1. Identify candidate subjects and objects that match ontology domain and range types.\n",
      "    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n",
      "    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n",
      "    4. If none is found, do not emit a triple for that relation.\n",
      "    5. Resolve simple pronouns only if doing so maintains correct domain→range typing.\n",
      "\n",
      "    Text:\n",
      "    \"Keyboard Cat's original form was a video originally made in 1984 by Charlie Schmidt of his cat Fatso seemingly playing a piano (though manipulated by Schmidt off-camera) to a cheery tune.\"\n",
      "\n",
      "    Ontology concepts:\n",
      "    - human\n",
      "- city\n",
      "- country\n",
      "- film\n",
      "- film genre\n",
      "- genre\n",
      "- film production company\n",
      "- film award\n",
      "- award\n",
      "- written work\n",
      "- film character\n",
      "- film organization\n",
      "\n",
      "    Ontology relations (domain → range):\n",
      "    - director(film,human)\n",
      "- screenwriter(film,human)\n",
      "- genre(film,genre)\n",
      "- based on(film,written work)\n",
      "- cast member(film,human)\n",
      "- award received(film,award)\n",
      "- production company(film,film production company)\n",
      "- country of origin(film,country)\n",
      "- publication date(film,)\n",
      "- characters(film,film character)\n",
      "- narrative location(film,city)\n",
      "- filming location(film,city)\n",
      "- main subject(film,)\n",
      "- nominated for(film,award)\n",
      "- cost(film,)\n",
      "\n",
      "    Output JSON only in this exact schema:\n",
      "    {\n",
      "      \"triples\": [\n",
      "        {\n",
      "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
      "          \"confidence\": 0.0,  // confidence 0–1\n",
      "          \"cue\": \"matched cue phrase from text\",\n",
      "          \"support\": \"exact quoted span(s)\",\n",
      "          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "\n",
      "    Rules:\n",
      "    - Work only from visible text evidence.\n",
      "    - If no relation cue appears, return an empty list of triples.\n",
      "    - Do NOT invent or assume facts not in text.\n",
      "    - Always quote exact spans for support and cue.\n",
      "\n",
      "[SOURCE TEXT]\n",
      " Keyboard Cat's original form was a video originally made in 1984 by Charlie Schmidt of his cat Fatso seemingly playing a piano (though manipulated by Schmidt off-camera) to a cheery tune.\n"
     ]
    }
   ],
   "source": [
    "run_wikidata_batch_p2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ea626-c6fb-410b-be81-e956ed6bee0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44837244-337c-4b84-847a-3189883ba13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c30802-738f-4025-bb55-1ebb39d8f424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ae95b-398d-40b0-b7a8-5d4ecf67f700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e6404b-0132-4ce3-b3c2-9ed81ea1738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "DBPEDIA_PATTERN = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_dbpedia_paths(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Given 'ont_14_writtenwork_test.jsonl', return:\n",
    "      - input_jsonl_path      -> <base_input>/ont_14_writtenwork_test.jsonl\n",
    "      - ontology_json_path    -> <base_onto>/14_writtenwork_ontology.json\n",
    "      - output_jsonl_path     -> <base_out>/ont_14_writtenwork_output.jsonl\n",
    "      - tag                   -> 'ont_14_writtenwork'\n",
    "    \"\"\"\n",
    "    m = DBPEDIA_PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_dbpedia_batch_p2(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        \"ont_12_monument_test.jsonl\",\n",
    "        \"ont_1_university_test.jsonl\",\n",
    "        \"ont_10_comicscharacter_test.jsonl\",\n",
    "        \"ont_11_meanoftransportation_test.jsonl\",\n",
    "        \"ont_13_food_test.jsonl\",\n",
    "        \"ont_14_writtenwork_test.jsonl\",\n",
    "        \"ont_15_software_test.jsonl\",\n",
    "        \"ont_16_person_test.jsonl\",\n",
    "        \"ont_17_athlete_test.jsonl\",\n",
    "        \"ont_18_organization_test.jsonl\",\n",
    "        \"ont_19_film_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # 1. load the model ONCE and reuse it for all dbpedia files\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    # 2. iterate over each file\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # construct all paths for this file\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_dbpedia_paths(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P2] dbpedia {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            # 3. call the single-file pipeline, reusing the SAME model\n",
    "            run_pipeline_prompt2(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES,\n",
    "                max_items=None,\n",
    "                verbose=False,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse the already-loaded model\n",
    "                tokenizer=tokenizer,   # reuse tokenizer\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P2] dbpedia {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P2] dbpedia {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c52a65-8050-4eb5-90f4-0d14fb9df21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3348d4e7ff2b47429635e74ea3761738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_12_monument\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_12_monument_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/12_monument_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_12_monument_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# FULL DATASET BATCH RUNS\n",
    "########################################\n",
    "run_dbpedia_batch_p2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c632b8-f94f-490a-ab06-ef57d26fdb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
