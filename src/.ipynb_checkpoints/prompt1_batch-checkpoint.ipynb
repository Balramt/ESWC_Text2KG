{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a1566a-f6d9-4629-99a1-e13a36397dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 30 03:12:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 74%   54C    P5            157W /  370W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab20767-b691-442a-8f09-57414ef1c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 1050157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28cd502-6b2c-4153-a10e-6e63d1229bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limbo\n",
      "/opt/miniforge3/envs/jupyterhub/bin/python\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "!which python\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9363af-cef7-474f-98b2-0fb380b60743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Required imports ===\n",
    "import os\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# --- FIXED setup_model with pad_token patch ---\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load the chat model + tokenizer and return a text-generation pipeline.\n",
    "    Uses half precision + device_map='auto' for efficiency.\n",
    "    \"\"\"\n",
    "    print(f\"[LOAD] model={model_id}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # ✅ FIX: set pad_token = eos_token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "        batch_size=16,       # ✅ enables batching\n",
    "        truncation=True,     # ✅ prevents runaway context\n",
    "    )\n",
    "\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ff8832-fc9b-47a6-8536-752069a95182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "# BLOCK 2\n",
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Stream records from a .jsonl file.\n",
    "    Stops early if max_items is provided.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dicts as JSON lines.\n",
    "    Creates parent directory if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def extract_text_field(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the text field from an input record.\n",
    "    Returns (text_value, key_used).\n",
    "    Falls back to the longest string field if none of the preferred keys exist.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "\n",
    "    # fallback: choose longest string in record\n",
    "    best_key, best_val = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_val):\n",
    "            best_key, best_val = k, v\n",
    "    return best_val.strip(), best_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67f6a35-f5ac-4f72-9f7b-53130a3f04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 3\n",
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map any known identifier (qid/id/label) -> canonical label string.\n",
    "    This lets us convert domain/range IDs into human-readable names.\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for concept in ontology_json.get(\"concepts\", []):\n",
    "        label = str(concept.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            raw_val = concept.get(keyname)\n",
    "            if raw_val is None:\n",
    "                continue\n",
    "\n",
    "            sval = str(raw_val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _label_for(raw_val: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Convert domain/range IDs to readable labels.\n",
    "    Fallback to string form of raw_val.\n",
    "    \"\"\"\n",
    "    if raw_val is None:\n",
    "        return \"\"\n",
    "    rval = str(raw_val).strip()\n",
    "    return cindex.get(rval, rval)\n",
    "\n",
    "\n",
    "def render_concept_list(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of ontology concepts by label.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if label:\n",
    "            lines.append(f\"- {label}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def render_relation_list(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of relations with (domain, range) in human-readable form.\n",
    "    Format: - relationLabel(domainLabel,rangeLabel)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes so we can safely embed text\n",
    "    inside quoted blocks in the USER prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7d6097-d4de-4925-80ac-f8f067e52d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 4\n",
    "def build_prompt1_system() -> str:\n",
    "    \"\"\"\n",
    "    SYSTEM message for Prompt 1.\n",
    "    Allows both ontology-aligned and non-ontology triples,\n",
    "    still returns strict JSON only.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are a KG triple proposer in a Tree-of-Thoughts loop. \"\n",
    "        \"First detect entity mentions and assign tentative ontology types. \"\n",
    "        \"Then propose candidate triples [subject, relation, object] that express factual statements in the text. \"\n",
    "        \"You MUST include:\\n\"\n",
    "        \"1) triples whose relation/domain/range matches the ontology, AND\\n\"\n",
    "        \"2) any other clearly stated factual triples in the text even if the relation or types are not present in the ontology.\\n\"\n",
    "        \"Return only JSON. Do not include any natural language outside JSON.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_prompt1_user(\n",
    "    TEXT: str,\n",
    "    ontology_json: Dict[str, Any],\n",
    "    k: int,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    USER message for Prompt 1.\n",
    "    Updated so the model will not suppress non-ontology triples.\n",
    "    We keep the same JSON schema (mentions[], triples[]),\n",
    "    but adjust the task + constraints language.\n",
    "    \"\"\"\n",
    "    concept_block = render_concept_list(ontology_json)\n",
    "    relation_block = render_relation_list(ontology_json)\n",
    "\n",
    "    return dedent(f\"\"\"\n",
    "    Task:\n",
    "    1) From the text, list detected entity mentions with tentative ontology types.\n",
    "    2) Propose up to k={k} candidate triples [subject, relation, object].\n",
    "\n",
    "    VERY IMPORTANT:\n",
    "    - You MUST include all explicit factual triples stated in the text, even if the relation,\n",
    "      subject type, or object type is not listed in the ontology.\n",
    "    - ALSO include ontology-valid triples whose domain/range matches the ontology relations.\n",
    "\n",
    "    For each triple, include confidence ∈ [0,1] and cite the exact supporting span(s).\n",
    "\n",
    "    Text\n",
    "    \"{_escape_multiline(TEXT)}\"\n",
    "\n",
    "    Ontology concepts\n",
    "    {concept_block}\n",
    "\n",
    "    Ontology relations (domain → range)\n",
    "    {relation_block}\n",
    "\n",
    "    Output format (JSON only)\n",
    "    {{\n",
    "      \"mentions\": [\n",
    "        {{\"surface\": \"...\", \"type_candidates\": [\"ConceptA\",\"ConceptB\"], \"span\": [start,end]}}\n",
    "      ],\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,\n",
    "          \"support\": \"exact quote from text\",\n",
    "          \"notes\": \"why this triple is supported; if ontology applies, explain domain/range fit. If not in ontology, say 'not in ontology but supported by text'.\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Constraints\n",
    "    - Extract ALL clearly stated factual triples in the text.\n",
    "    - If a triple matches an ontology relation, enforce domain→range consistency and mention that in notes.\n",
    "    - If a triple does NOT match any ontology relation, you MUST STILL include it (do not discard it).\n",
    "    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n",
    "    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n",
    "    - Do not invent entities that are not mentioned in the text.\n",
    "    - Output MUST be valid JSON and nothing else.\n",
    "    \"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64771423-6baf-4d97-b777-883fec60bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 6\n",
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "def robust_parse_model_output(raw_response: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try very hard to get structured data out of the model response.\n",
    "    Returns a dict with at least:\n",
    "      {\n",
    "        \"triples\": [ [head, rel, tail], ... ],\n",
    "        \"mentions\": [...] or None,\n",
    "        \"raw_json_obj\": ... or None\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Try direct parse\n",
    "    try:\n",
    "        obj = json.loads(raw_response)\n",
    "        return {\n",
    "            \"triples\": extract_triples_from_obj(obj),\n",
    "            \"mentions\": obj.get(\"mentions\"),\n",
    "            \"raw_json_obj\": obj,\n",
    "        }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2. Try slice from first { to last }\n",
    "    try:\n",
    "        start_i = raw_response.find(\"{\")\n",
    "        end_i = raw_response.rfind(\"}\")\n",
    "        if start_i != -1 and end_i != -1 and end_i > start_i:\n",
    "            candidate = raw_response[start_i:end_i+1]\n",
    "            obj = json.loads(candidate)\n",
    "            return {\n",
    "                \"triples\": extract_triples_from_obj(obj),\n",
    "                \"mentions\": obj.get(\"mentions\"),\n",
    "                \"raw_json_obj\": obj,\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3. Fallback: regex mine triples from messy text\n",
    "    triples = extract_triples_via_regex(raw_response)\n",
    "\n",
    "    return {\n",
    "        \"triples\": triples,\n",
    "        \"mentions\": None,\n",
    "        \"raw_json_obj\": None,\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_triples_from_obj(obj: Any) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Safely pull triples out of a parsed JSON object,\n",
    "    accounting for [\"h\",\"r\",\"t\"] or {\"triple\":[...]} formats.\n",
    "    \"\"\"\n",
    "    results: List[List[str]] = []\n",
    "\n",
    "    # if it's dict-like and has \"triples\"\n",
    "    if isinstance(obj, dict) and \"triples\" in obj:\n",
    "        for item in obj[\"triples\"]:\n",
    "            # item could be [\"h\",\"r\",\"t\"]\n",
    "            if isinstance(item, list) and len(item) >= 3:\n",
    "                results.append(item[:3])\n",
    "            # item could be { \"triple\": [\"h\",\"r\",\"t\"], ... }\n",
    "            elif isinstance(item, dict) and \"triple\" in item:\n",
    "                tval = item[\"triple\"]\n",
    "                if isinstance(tval, list) and len(tval) >= 3:\n",
    "                    results.append(tval[:3])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_triples_via_regex(raw_text: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Ultra-forgiving fallback.\n",
    "    Finds patterns like:\n",
    "      \"triple\": [\"Head\", \"Rel\", \"Tail\"]\n",
    "    even if the outer JSON is broken.\n",
    "    \"\"\"\n",
    "    triples: List[List[str]] = []\n",
    "\n",
    "    triple_pattern = re.compile(\n",
    "        r'\"triple\"\\s*:\\s*\\[\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*,\\s*\"([^\"]+)\"\\s*\\]'\n",
    "    )\n",
    "\n",
    "    for match in triple_pattern.finditer(raw_text):\n",
    "        h, r, t = match.groups()\n",
    "        triples.append([h, r, t])\n",
    "\n",
    "    return triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b0fac0-608b-4a32-a10c-0cb5edb2bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 5 — Safe + self-contained (adaptive batching)\n",
    "\n",
    "# === Global-safe defaults (in case Block 1 not yet run) ===\n",
    "BATCH_SIZE   = globals().get(\"BATCH_SIZE\", 16)   # same as Mistral batch setup\n",
    "ADAPT_FACTOR = globals().get(\"ADAPT_FACTOR\", 2)  # dyn_max ≈ 2× input tokens\n",
    "ADAPT_CAP    = globals().get(\"ADAPT_CAP\", 3000)   # hard cap to prevent slow runs\n",
    "\n",
    "\n",
    "def _build_prompt_text(tokenizer, system_msg: str, user_msg: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a chat-style prompt text using tokenizer’s chat template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def _batched(iterable, n):\n",
    "    \"\"\"\n",
    "    Yield chunks of size n from iterable.\n",
    "    \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, n))\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "\n",
    "def _compute_dyn_max_new_tokens(\n",
    "    prompts: List[str],\n",
    "    tokenizer,\n",
    "    cap: int = ADAPT_CAP,\n",
    "    factor: int = ADAPT_FACTOR\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Adaptive per-batch cap:\n",
    "      dyn_max = max( min(factor * input_len_tokens, cap) for p in prompts )\n",
    "    Ensures output length scales with input but never exceeds cap.\n",
    "    \"\"\"\n",
    "    lens = [tokenizer(p, return_tensors=\"pt\")[\"input_ids\"].shape[1] for p in prompts]\n",
    "    return max(min(factor * L, cap) for L in lens) if lens else cap\n",
    "\n",
    "\n",
    "def generate_model_responses_batched(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    prompt_texts: List[str],\n",
    "    temperature: float = 0.25,\n",
    "    cap_max_new_tokens: int = ADAPT_CAP,  # interpret caller’s max as a cap\n",
    "    verbose: bool = False,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate model outputs in true batches with:\n",
    "      (A) truncation=True and adaptive max_new_tokens\n",
    "      (B) batching via HF pipeline\n",
    "    Returns a list of raw generated strings, one per prompt.\n",
    "    \"\"\"\n",
    "    outputs: List[str] = []\n",
    "\n",
    "    for batch_prompts in _batched(prompt_texts, BATCH_SIZE):\n",
    "        # adaptive max per batch (respect caller’s cap if smaller)\n",
    "        dyn_max = _compute_dyn_max_new_tokens(batch_prompts, tokenizer, cap=cap_max_new_tokens)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[BATCH] size={len(batch_prompts)}  dyn_max={dyn_max}\")\n",
    "\n",
    "        out_batch = generator(\n",
    "            batch_prompts,\n",
    "            max_new_tokens=dyn_max,   # (A) adaptive cap\n",
    "            truncation=True,          # (A) enable truncation\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            return_full_text=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            batch_size=BATCH_SIZE,    # (B) true batching\n",
    "        )\n",
    "\n",
    "        # HF pipeline may return nested lists; flatten if needed\n",
    "        if out_batch and isinstance(out_batch[0], list):\n",
    "            flat = [o for sub in out_batch for o in sub]\n",
    "        else:\n",
    "            flat = out_batch\n",
    "\n",
    "        for o in flat:\n",
    "            outputs.append(o[\"generated_text\"] if isinstance(o, dict) else o)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c02eea4-e728-49a8-9c82-954ef56892d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 7\n",
    "def run_prompt1_pipeline(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    input_jsonl_path: str,\n",
    "    ontology_json_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    k: int = 6,\n",
    "    max_items: Optional[int] = None,\n",
    "    max_new_tokens: int = 900,     # we now treat this as a CAP (ADAPT_CAP by default)\n",
    "    temperature: float = 0.25,\n",
    "    verbose: bool = False,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Core batch function for Prompt 1.\n",
    "    Reads each row from input_jsonl_path,\n",
    "    runs Prompt 1 on it (BATCHED + adaptive length),\n",
    "    and writes a JSONL of model outputs with parsed JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # load ontology once\n",
    "    with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        ontology_json = json.load(f)\n",
    "\n",
    "    sys_text = build_prompt1_system()\n",
    "    outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 1) Build prompts for all selected records\n",
    "    recs = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "    prompt_texts: List[str] = []\n",
    "    meta: List[Tuple[int, Dict[str, Any], str]] = []  # (idx, rec, text_val)\n",
    "\n",
    "    for idx, rec in enumerate(recs):\n",
    "        # extract text from record\n",
    "        text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "        # build user prompt\n",
    "        usr_text = build_prompt1_user(\n",
    "            TEXT=text_val,\n",
    "            ontology_json=ontology_json,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[ITEM {idx}] text_key={text_key}\")\n",
    "            print(f\"[PROMPT_USER] {usr_text[:320]} ...\")\n",
    "\n",
    "        # build chat prompt text for the model\n",
    "        prompt_text = _build_prompt_text(tokenizer, sys_text, usr_text)\n",
    "        prompt_texts.append(prompt_text)\n",
    "        meta.append((idx, rec, text_val))\n",
    "\n",
    "    # 2) Generate in batches (A: truncation+adaptive, B: batching)\n",
    "    cap = min(max_new_tokens, ADAPT_CAP) if max_new_tokens else ADAPT_CAP\n",
    "    raw_outputs = generate_model_responses_batched(\n",
    "        generator=generator,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt_texts=prompt_texts,\n",
    "        temperature=temperature,\n",
    "        cap_max_new_tokens=cap,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    # 3) Parse + package\n",
    "    for (idx, rec, text_val), raw_response in zip(meta, raw_outputs):\n",
    "        parsed_bundle = robust_parse_model_output(raw_response)\n",
    "\n",
    "        out_record = {\n",
    "            \"id\": rec.get(\"id\"),\n",
    "            \"input text\": text_val,\n",
    "            \"prompts\": {\n",
    "                \"system_prompt\": sys_text,\n",
    "                \"user_prompt\": None,  # omit full user prompt to reduce file size; add if you want\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"LLM_output\": raw_response,\n",
    "                \"json\": parsed_bundle.get(\"raw_json_obj\"),\n",
    "            },\n",
    "        }\n",
    "        outputs.append(out_record)\n",
    "\n",
    "    # 4) write all outputs\n",
    "    write_jsonl(output_jsonl_path, outputs)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DONE] wrote {len(outputs)} records -> {output_jsonl_path}\")\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3089d2ca-2fab-4cf6-b773-69b21e39d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================ DEBUG BLOCK (BATCHED) ============================\n",
    "\n",
    "# import pprint\n",
    "\n",
    "# # --- CONFIG ---\n",
    "# ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "# INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "# OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output_test8.jsonl\"\n",
    "\n",
    "# MAX_ITEMS      = 1           # how many examples to actually test\n",
    "# TEMPERATURE    = 0.25\n",
    "# VERBOSE        = True\n",
    "# K_CANDIDATES   = 6\n",
    "\n",
    "# # batching + adaptive length knobs\n",
    "# BATCH_SIZE     = 16          # match your Mistral batch setup\n",
    "# ADAPT_FACTOR   = 2           # dyn_max ≈ 2x input length\n",
    "# ADAPT_CAP      = 3000         # hard upper cap for speed\n",
    "\n",
    "# def compute_dyn_max_new_tokens(prompts, tokenizer, cap=ADAPT_CAP, factor=ADAPT_FACTOR):\n",
    "#     lens = [tokenizer(p, return_tensors=\"pt\")[\"input_ids\"].shape[1] for p in prompts]\n",
    "#     return max(min(factor * L, cap) for L in lens) if lens else cap\n",
    "\n",
    "# def generate_model_responses_batched(\n",
    "#     generator,\n",
    "#     tokenizer,\n",
    "#     system_msg: str,\n",
    "#     user_msgs: list[str],\n",
    "#     temperature: float = 0.25,\n",
    "#     batch_size: int = BATCH_SIZE,\n",
    "# ) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Generate for a list of USER messages in one or more batches using the HF pipeline.\n",
    "#     Implements:\n",
    "#       (A) truncation=True + adaptive max_new_tokens per batch\n",
    "#       (B) true batching with batch_size\n",
    "#     \"\"\"\n",
    "#     # Build chat-formatted prompts (system is shared)\n",
    "#     prompts = []\n",
    "#     for um in user_msgs:\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": system_msg},\n",
    "#             {\"role\": \"user\",   \"content\": um},\n",
    "#         ]\n",
    "#         prompt_text = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "#         prompts.append(prompt_text)\n",
    "\n",
    "#     outputs: list[str] = []\n",
    "\n",
    "#     # Process in batches\n",
    "#     for i in range(0, len(prompts), batch_size):\n",
    "#         batch_prompts = prompts[i:i+batch_size]\n",
    "#         dyn_max = compute_dyn_max_new_tokens(batch_prompts, tokenizer)\n",
    "\n",
    "#         if VERBOSE:\n",
    "#             print(f\"\\n[BATCH GEN] items={len(batch_prompts)} dyn_max={dyn_max}\")\n",
    "\n",
    "#         outs = generator(\n",
    "#             batch_prompts,\n",
    "#             max_new_tokens=dyn_max,       # (A) adaptive length\n",
    "#             truncation=True,               # (A) truncation ON\n",
    "#             do_sample=True,                # keep your sampling as-is\n",
    "#             temperature=temperature,\n",
    "#             top_p=0.9,\n",
    "#             return_full_text=False,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#             batch_size=min(batch_size, len(batch_prompts)),  # (B) true batching\n",
    "#         )\n",
    "\n",
    "#         # HF pipeline can return nested lists; flatten if needed\n",
    "#         if outs and isinstance(outs[0], list):\n",
    "#             outs = [o for sub in outs for o in sub]\n",
    "\n",
    "#         for o in outs:\n",
    "#             outputs.append(o[\"generated_text\"] if isinstance(o, dict) else o)\n",
    "\n",
    "#     return outputs\n",
    "\n",
    "\n",
    "# # --- 1) Quick peek at input file ---\n",
    "# peek_items = list(read_jsonl(INPUT_JSONL, max_items=3))\n",
    "\n",
    "# if not peek_items:\n",
    "#     print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "# else:\n",
    "#     print(f\"[DEBUG] Loaded {len(peek_items)} sample record(s) from {INPUT_JSONL}\")\n",
    "#     for i, rec in enumerate(peek_items[:MAX_ITEMS or 1]):\n",
    "#         text_val, text_key = extract_text_field(rec)\n",
    "#         print(f\"\\n--- SAMPLE {i} ---\")\n",
    "#         print(\"[keys]:\", list(rec.keys()))\n",
    "#         print(\" id:\", rec.get(\"id\"))\n",
    "#         print(f\" chosen_text_key: {text_key}\")\n",
    "#         preview = text_val[:200] + (\"...\" if len(text_val) > 200 else \"\")\n",
    "#         print(\" text preview:\", preview)\n",
    "\n",
    "# # --- 2) Load ontology for prompt inspection ---\n",
    "# with open(ONTOLOGY_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "#     ontology_data = json.load(f)\n",
    "\n",
    "# concept_block_dbg  = render_concept_list(ontology_data)\n",
    "# relation_block_dbg = render_relation_list(ontology_data)\n",
    "\n",
    "# print(\"\\n[DEBUG] ONTOLOGY CONCEPT LIST (truncated):\")\n",
    "# print(concept_block_dbg[:500] + (\"...\" if len(concept_block_dbg) > 500 else \"\"))\n",
    "\n",
    "# print(\"\\n[DEBUG] ONTOLOGY RELATION LIST (truncated):\")\n",
    "# print(relation_block_dbg[:500] + (\"...\" if len(relation_block_dbg) > 500 else \"\"))\n",
    "\n",
    "# # --- 3) Show SYSTEM and USER prompt for first sample ---\n",
    "# if peek_items:\n",
    "#     sample_text, _ = extract_text_field(peek_items[0])\n",
    "\n",
    "#     system_prompt_dbg = build_prompt1_system()\n",
    "#     user_prompt_dbg   = build_prompt1_user(\n",
    "#         TEXT=sample_text,\n",
    "#         ontology_json=ontology_data,\n",
    "#         k=K_CANDIDATES,\n",
    "#     )\n",
    "\n",
    "#     print(\"\\n================ [SYSTEM PROMPT] ================\")\n",
    "#     print(system_prompt_dbg)\n",
    "\n",
    "#     print(\"\\n================ [USER PROMPT - FIRST SAMPLE] ================\")\n",
    "#     up_prev = user_prompt_dbg[:12000]\n",
    "#     print(up_prev)\n",
    "#     if len(user_prompt_dbg) > 12000:\n",
    "#         print(\"... [USER PROMPT TRUNCATED FOR DISPLAY] ...\")\n",
    "\n",
    "# # --- 4) Spin up the model ---\n",
    "# generator, tokenizer = setup_model()\n",
    "\n",
    "# # --- 5) Dry-run: batched generation for the first MAX_ITEMS records ---\n",
    "# if peek_items:\n",
    "#     # Prepare USER prompts for the first MAX_ITEMS items\n",
    "#     sel_items = peek_items[:MAX_ITEMS or 1]\n",
    "#     user_msgs = []\n",
    "#     for rec in sel_items:\n",
    "#         text_val, _ = extract_text_field(rec)\n",
    "#         user_msgs.append(\n",
    "#             build_prompt1_user(\n",
    "#                 TEXT=text_val,\n",
    "#                 ontology_json=ontology_data,\n",
    "#                 k=K_CANDIDATES,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     raw_list = generate_model_responses_batched(\n",
    "#         generator=generator,\n",
    "#         tokenizer=tokenizer,\n",
    "#         system_msg=build_prompt1_system(),\n",
    "#         user_msgs=user_msgs,\n",
    "#         temperature=TEMPERATURE,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#     )\n",
    "\n",
    "#     # Show the first raw output and a parsed preview\n",
    "#     first_raw = raw_list[0]\n",
    "#     print(\"\\n================ [RAW MODEL OUTPUT - FIRST SAMPLE] ================\")\n",
    "#     print(first_raw[:150000] + (\"...\" if len(first_raw) > 150000 else \"\"))\n",
    "\n",
    "#     parsed_single = None\n",
    "#     try:\n",
    "#         parsed_single = json.loads(first_raw)\n",
    "#     except Exception:\n",
    "#         try:\n",
    "#             start_i = first_raw.find(\"{\")\n",
    "#             end_i   = first_raw.rfind(\"}\")\n",
    "#             if start_i != -1 and end_i != -1 and end_i > start_i:\n",
    "#                 candidate = first_raw[start_i:end_i+1]\n",
    "#                 parsed_single = json.loads(candidate)\n",
    "#         except Exception:\n",
    "#             parsed_single = None\n",
    "\n",
    "#     print(\"\\n================ [PARSED MODEL JSON - FIRST SAMPLE] ================\")\n",
    "#     pprint.pprint(parsed_single, width=120)\n",
    "\n",
    "#     # Sanity check for evaluator compatibility:\n",
    "#     if parsed_single and \"triples\" in parsed_single:\n",
    "#         print(\"\\n[CHECK] triples[0] example for evaluator compatibility:\")\n",
    "#         if parsed_single[\"triples\"]:\n",
    "#             pprint.pprint(parsed_single[\"triples\"][0], width=100)\n",
    "#         else:\n",
    "#             print(\"No triples returned.\")\n",
    "#     else:\n",
    "#         print(\"[WARN] Model output did not parse into expected {'mentions':..., 'triples':...} shape.\")\n",
    "\n",
    "# # --- 6) Full mini-pipeline run (writes OUTPUT_JSONL) on first MAX_ITEMS records ---\n",
    "# print(\"\\n================ [BATCH PIPELINE RUN] ================\")\n",
    "# batch_outputs = run_prompt1_pipeline(\n",
    "#     generator=generator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     ontology_json_path=ONTOLOGY_JSON,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     max_new_tokens=900,      # this is ignored in the batched path if you've applied A/B there too\n",
    "#     temperature=TEMPERATURE,\n",
    "#     verbose=VERBOSE,\n",
    "#     k=K_CANDIDATES,\n",
    "# )\n",
    "\n",
    "# print(f\"\\n[SUCCESS] Wrote {len(batch_outputs)} items to {OUTPUT_JSONL}\")\n",
    "\n",
    "# # --- 7) Peek at what was written (first 1-2 records) ---\n",
    "# print(\"\\n================ [WRITTEN OUTPUT PREVIEW] ================\")\n",
    "# for i, rec in enumerate(batch_outputs[:2]):\n",
    "#     print(f\"\\n--- OUTPUT ITEM {i} ---\")\n",
    "#     print(\"[output keys]:\", list(rec.keys()))\n",
    "#     resp = rec.get(\"response\", {})\n",
    "#     parsed_json = resp.get(\"json\")\n",
    "#     if parsed_json:\n",
    "#         print(\"[triples exists? ]\", \"triples\" in parsed_json)\n",
    "#         if \"triples\" in parsed_json and parsed_json[\"triples\"]:\n",
    "#             print(\"[first triple]:\")\n",
    "#             pprint.pprint(parsed_json[\"triples\"][0], width=100)\n",
    "#     else:\n",
    "#         print(\"[WARN] No parsed JSON for this item.\")\n",
    "# # ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf52b54a-538e-4894-9a92-18740113d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# WIKIDATA BATCH RUN (same style as DBpedia)\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "WIKI_PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_wikidata_paths(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Parse filenames like ont_8_politician_test.jsonl\n",
    "    and return input_jsonl, ontology_json, output_jsonl, tag\n",
    "    \"\"\"\n",
    "    m = WIKI_PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl  = os.path.join(base_input, filename)\n",
    "    ontology_json = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "    output_jsonl  = os.path.join(base_out, filename.replace(\"_test.jsonl\", \"_output.jsonl\"))\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl, ontology_json, output_jsonl, tag\n",
    "\n",
    "\n",
    "def run_wikidata_batch():\n",
    "    \"\"\"\n",
    "    Run Prompt 1 pipeline in batched mode for Wikidata datasets.\n",
    "    Mirrors the DBpedia version: explicit read → build prompts → batched generate → parse → write.\n",
    "    \"\"\"\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        \"ont_10_culture_test.jsonl\",\n",
    "        \"ont_1_movie_test.jsonl\",\n",
    "        \"ont_2_music_test.jsonl\",\n",
    "        \"ont_3_sport_test.jsonl\",\n",
    "        \"ont_4_book_test.jsonl\",\n",
    "        \"ont_5_military_test.jsonl\",\n",
    "        \"ont_6_computer_test.jsonl\",\n",
    "        \"ont_7_space_test.jsonl\",\n",
    "        \"ont_8_politics_test.jsonl\",\n",
    "        \"ont_9_nature_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    # 1) load model ONCE (same as dbpedia code)\n",
    "    generator, tokenizer = setup_model()\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN] wikidata {fname}\")\n",
    "\n",
    "            # 2) build all paths\n",
    "            input_jsonl, ontology_json, output_jsonl, tag = make_wikidata_paths(\n",
    "                fname,\n",
    "                BASE_INPUT,\n",
    "                BASE_ONTO,\n",
    "                BASE_OUT,\n",
    "            )\n",
    "\n",
    "            # 3) load ontology\n",
    "            with open(ontology_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                ontology_data = json.load(f)\n",
    "\n",
    "            # 4) load input records\n",
    "            recs = list(read_jsonl(input_jsonl))\n",
    "            if not recs:\n",
    "                print(f\"[SKIP] No records found in {input_jsonl}\")\n",
    "                continue\n",
    "\n",
    "            # 5) build system + all chat-formatted prompts (same as dbpedia)\n",
    "            system_prompt = build_prompt1_system()\n",
    "            prompt_texts: List[str] = []\n",
    "            for rec in recs:\n",
    "                text_val, _ = extract_text_field(rec)\n",
    "                user_prompt = build_prompt1_user(\n",
    "                    TEXT=text_val,\n",
    "                    ontology_json=ontology_data,\n",
    "                    k=6,   # same as your original wikidata call\n",
    "                )\n",
    "                prompt_text = _build_prompt_text(tokenizer, system_prompt, user_prompt)\n",
    "                prompt_texts.append(prompt_text)\n",
    "\n",
    "            # 6) generate (BATCHED) exactly like dbpedia\n",
    "            raw_outputs = generate_model_responses_batched(\n",
    "                generator=generator,\n",
    "                tokenizer=tokenizer,\n",
    "                prompt_texts=prompt_texts,\n",
    "                temperature=0.25,\n",
    "                cap_max_new_tokens=3000,   # treated as CAP; actual dyn max inside helper\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # 7) parse + package + write\n",
    "            outputs: List[dict] = []\n",
    "            for rec, raw_text in zip(recs, raw_outputs):\n",
    "                parsed = robust_parse_model_output(raw_text)\n",
    "                outputs.append({\n",
    "                    \"id\": rec.get(\"id\"),\n",
    "                    \"input text\": rec,   # keep same style as dbpedia code 2\n",
    "                    \"response\": {\n",
    "                        \"LLM_output\": raw_text,\n",
    "                        \"json\": parsed.get(\"raw_json_obj\"),\n",
    "                    },\n",
    "                })\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)\n",
    "            write_jsonl(output_jsonl, outputs)\n",
    "            print(f\"[DONE] Wrote {len(outputs)} records → {output_jsonl}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] wikidata {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2875e945-2b83-4fe5-90c3-8ee86e05c969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3e15934dcd44edbba9c997f3b3bec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_10_culture_test.jsonl\n",
      "[DONE] Wrote 159 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_10_culture_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_1_movie_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Wrote 840 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_1_movie_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_2_music_test.jsonl\n",
      "[DONE] Wrote 675 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_2_music_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_3_sport_test.jsonl\n",
      "[DONE] Wrote 487 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_3_sport_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_4_book_test.jsonl\n",
      "[DONE] Wrote 550 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_4_book_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_5_military_test.jsonl\n",
      "[DONE] Wrote 230 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_5_military_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_6_computer_test.jsonl\n",
      "[DONE] Wrote 230 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_6_computer_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_7_space_test.jsonl\n",
      "[DONE] Wrote 203 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_7_space_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_8_politics_test.jsonl\n",
      "[DONE] Wrote 214 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_8_politics_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] wikidata ont_9_nature_test.jsonl\n",
      "[DONE] Wrote 474 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/wikidata/ont_9_nature_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "run_wikidata_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a342cd1-18a9-4ae6-9ec9-358cd2a4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 10 — DBPEDIA BATCH RUN (compatible with adaptive batching)\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-z]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str, BASE_INPUT: str, BASE_ONTO: str, BASE_OUT: str):\n",
    "    \"\"\"\n",
    "    Parse filenames like ont_18_scientist_test.jsonl\n",
    "    and return input, ontology, and output paths.\n",
    "    \"\"\"\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "    input_jsonl  = os.path.join(BASE_INPUT, filename)\n",
    "    ontology_json = os.path.join(BASE_ONTO, f\"{idx}_{cat}_ontology.json\")\n",
    "    output_jsonl  = os.path.join(BASE_OUT, filename.replace(\"_test.jsonl\", \"_output.jsonl\"))\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl, ontology_json, output_jsonl, tag\n",
    "\n",
    "\n",
    "def run_dbpedia_batch():\n",
    "    \"\"\"\n",
    "    Run Prompt 1 pipeline in batched mode for DBpedia datasets.\n",
    "    Uses adaptive truncation and dynamic max token length.\n",
    "    \"\"\"\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        # \"ont_12_monument_test.jsonl\",\n",
    "        # \"ont_1_university_test.jsonl\",\n",
    "        # \"ont_2_musicalwork_test.jsonl\",\n",
    "        # \"ont_3_airport_test.jsonl\",\n",
    "        # \"ont_4_building_test.jsonl\",\n",
    "        # \"ont_5_athlete_test.jsonl\",\n",
    "        # \"ont_6_politician_test.jsonl\",\n",
    "        # \"ont_7_company_test.jsonl\",\n",
    "        # \"ont_8_celestialbody_test.jsonl\",\n",
    "        # \"ont_9_astronaut_test.jsonl\",\n",
    "        # \"ont_10_comicscharacter_test.jsonl\",\n",
    "        # \"ont_11_meanoftransportation_test.jsonl\",\n",
    "        # \"ont_13_food_test.jsonl\",\n",
    "        # \"ont_14_writtenwork_test.jsonl\",\n",
    "        # \"ont_15_sportsteam_test.jsonl\",\n",
    "        # \"ont_16_city_test.jsonl\",\n",
    "        # \"ont_17_artist_test.jsonl\",\n",
    "        \"ont_18_scientist_test.jsonl\",\n",
    "        \"ont_19_film_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    # === Load model once (for efficiency) ===\n",
    "    generator, tokenizer = setup_model()\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN] dbpedia {fname}\")\n",
    "\n",
    "            input_jsonl, ontology_json, output_jsonl, tag = make_paths(\n",
    "                fname, BASE_INPUT, BASE_ONTO, BASE_OUT\n",
    "            )\n",
    "\n",
    "            # --- Load ontology ---\n",
    "            with open(ontology_json, \"r\", encoding=\"utf-8\") as f:\n",
    "                ontology_data = json.load(f)\n",
    "\n",
    "            # --- Load records ---\n",
    "            recs = list(read_jsonl(input_jsonl))\n",
    "            if not recs:\n",
    "                print(f\"[SKIP] No records found in {input_jsonl}\")\n",
    "                continue\n",
    "\n",
    "            # --- Build full chat-formatted prompts ---\n",
    "            system_prompt = build_prompt1_system()\n",
    "            prompt_texts: List[str] = []\n",
    "            for rec in recs:\n",
    "                text_val, _ = extract_text_field(rec)\n",
    "                user_prompt = build_prompt1_user(\n",
    "                    TEXT=text_val,\n",
    "                    ontology_json=ontology_data,\n",
    "                    k=6,\n",
    "                )\n",
    "                prompt_text = _build_prompt_text(tokenizer, system_prompt, user_prompt)\n",
    "                prompt_texts.append(prompt_text)\n",
    "\n",
    "            # --- Generate model responses (batched) ---\n",
    "            raw_outputs = generate_model_responses_batched(\n",
    "                generator=generator,\n",
    "                tokenizer=tokenizer,\n",
    "                prompt_texts=prompt_texts,\n",
    "                temperature=0.25,\n",
    "                cap_max_new_tokens=3000,   # treated as CAP; per-batch dyn_max ≤ 512 by default\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            # --- Parse and save outputs ---\n",
    "            outputs: List[dict] = []\n",
    "            for rec, raw_text in zip(recs, raw_outputs):\n",
    "                parsed = robust_parse_model_output(raw_text)\n",
    "                outputs.append({\n",
    "                    \"id\": rec.get(\"id\"),\n",
    "                    \"input text\": rec,\n",
    "                    \"response\": {\n",
    "                        \"LLM_output\": raw_text,\n",
    "                        \"json\": parsed.get(\"raw_json_obj\"),\n",
    "                    },\n",
    "                })\n",
    "\n",
    "            os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)\n",
    "            write_jsonl(output_jsonl, outputs)\n",
    "            print(f\"[DONE] Wrote {len(outputs)} records → {output_jsonl}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR] dbpedia {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92127a4d-3ee5-4bfa-b3a3-3be25542cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66bb4c3a807a42428c453ce6850a658d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_18_scientist_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Wrote 149 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_18_scientist_output.jsonl\n",
      "\n",
      "================================================================================\n",
      "[RUN] dbpedia ont_19_film_test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Wrote 127 records → /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt1/dbpedia/ont_19_film_output.jsonl\n"
     ]
    }
   ],
   "source": [
    "run_dbpedia_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd1643-564d-45ea-9adb-e19455fe1899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
