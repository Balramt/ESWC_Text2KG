{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eedaa00-7386-4279-b7fa-3f1befa24172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 31 09:08:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8             31W /  370W |   14742MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1150845      C   ....conda/envs/kg_pipeline/bin/python3      14732MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1b179-07e6-46a9-b835-326a3368053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 1150845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c6a2b-084b-434e-b5af-60bb571cbb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12531f71-e818-4311-847c-1afda9ef4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 1: IMPORTS / GLOBAL CONFIG / MODEL SETUP\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "# -------- Runtime config (edit these before running Block 13 manual test) --------\n",
    "ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "\n",
    "MODEL_ID   = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "K_TRIPLES  = 1        # \"extract up to {k} triples\"\n",
    "MAX_ITEMS  = 16     # None = use all rows from INPUT_JSONL\n",
    "VERBOSE    = True     # default True for manual testing first\n",
    "\n",
    "# batching + adaptive generation config (added)\n",
    "BATCH_SIZE   = 16        # how many prompt-2 items per batch\n",
    "ADAPT_FACTOR = 2         # dyn_max = min(ADAPT_FACTOR * input_len, ADAPT_CAP)\n",
    "ADAPT_CAP    = 2000      # << as you asked\n",
    "\n",
    "\n",
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load the chat model + tokenizer and return a text-generation pipeline.\n",
    "    Uses half precision + device_map='auto' for efficiency.\n",
    "    \"\"\"\n",
    "    print(f\"[LOAD] model={model_id}\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return generator, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaeb313-6b65-4bd2-9f61-a44a7bf4d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 2: DATA I/O HELPERS\n",
    "########################################\n",
    "\n",
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Stream records from a .jsonl file.\n",
    "    Stops early if max_items is provided.\n",
    "    Yields dicts.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dicts as JSON lines.\n",
    "    Creates parent directory if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def extract_text_field(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the text field from an input record.\n",
    "    Returns (text_value, key_used).\n",
    "    Falls back to the longest string field if none of the preferred keys exist.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "\n",
    "    # fallback: choose longest string in record\n",
    "    best_key, best_val = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_val):\n",
    "            best_key, best_val = k, v\n",
    "    return best_val.strip(), best_key\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes so we can safely embed text\n",
    "    inside quoted blocks in the USER prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f93e57-1964-442a-a31f-eedefb7daf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 3: ONTOLOGY HELPERS + PROMPT 2 MESSAGE BUILDERS\n",
    "########################################\n",
    "\n",
    "def load_ontology_json(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load ontology JSON file.\n",
    "    Expected structure:\n",
    "      {\n",
    "        \"concepts\": [\n",
    "          {\"id\": \"...\", \"qid\": \"...\", \"label\": \"SomeClass\"},\n",
    "          ...\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"id\": \"...\",\n",
    "            \"label\": \"location\",\n",
    "            \"domain\": \"SomeConceptID\",\n",
    "            \"range\": \"SomeConceptID\"\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map any known identifier (qid/id/label) -> canonical label string.\n",
    "    This lets us convert domain/range IDs into human-readable names.\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for concept in ontology_json.get(\"concepts\", []):\n",
    "        label = str(concept.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            raw_val = concept.get(keyname)\n",
    "            if raw_val is None:\n",
    "                continue\n",
    "\n",
    "            sval = str(raw_val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _label_for(raw_val: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Convert domain/range IDs to readable labels.\n",
    "    Fallback to string form of raw_val.\n",
    "    \"\"\"\n",
    "    if raw_val is None:\n",
    "        return \"\"\n",
    "    rval = str(raw_val).strip()\n",
    "    return cindex.get(rval, rval)\n",
    "\n",
    "\n",
    "def format_ontology_concepts(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of ontology concepts by label.\n",
    "    We'll present these to the model.\n",
    "    \"\"\"\n",
    "    lines: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if label:\n",
    "            lines.append(f\"- {label}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def format_ontology_relations(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Return a bullet list of relations with (domain, range) in human-readable form.\n",
    "    Format: - relationLabel(domainLabel,rangeLabel)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_p2_system() -> str:\n",
    "    \"\"\"\n",
    "    System message for Prompt 2.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are a KG triple extractor. \"\n",
    "        \"Match relation cues in the text and return only triples that satisfy the \"\n",
    "        \"ontologyâ€™s domainâ†’range. Cite exact evidence. Output JSON only.\"\n",
    "    )\n",
    "\n",
    "def build_p2_user(text: str, ontology_json: Dict[str, Any], k: int) -> str:\n",
    "    \"\"\"\n",
    "    Prompt 2 user message:\n",
    "    - No few-shot examples\n",
    "    - No per-domain lists\n",
    "    - Includes a universal explanation of how to recognize and record lexical cues\n",
    "    \"\"\"\n",
    "\n",
    "    ontology_concepts_block = format_ontology_concepts(ontology_json)\n",
    "    ontology_relations_block = format_ontology_relations(ontology_json)\n",
    "\n",
    "    return dedent(f\"\"\"\\\n",
    "    Task: Using explicit lexical cues found in the text, extract up to {k} triples [subject, relation, object].\n",
    "    Enforce ontology domainâ†’range strictly; if a triple is invalid, omit it.\n",
    "\n",
    "    CUE GENERATION GUIDANCE:\n",
    "    - A lexical cue is a word or short phrase in the text that directly connects the subject and object\n",
    "      and signals the relation between them.\n",
    "    - It can be a **verb**, **verb phrase**, or **prepositional phrase** such as:\n",
    "        \"directed by\", \"founded in\", \"built by\", \"located in\", \"written by\", \"headed by\".\n",
    "      These are only illustrative examples â€” you must infer the appropriate cue for each relation label and sentence.\n",
    "    - The cue MUST appear literally in the text and should sit between, or very near, the subject and object mentions.\n",
    "    - The cue expresses the natural-language realization of the ontology relation.\n",
    "    - If no such linking phrase appears in the text for a given relation, skip that relation.\n",
    "    - Do NOT invent cues or use world knowledge; work only from the surface text.\n",
    "\n",
    "    PROCEDURE:\n",
    "    1. Identify candidate subjects and objects that match ontology domain and range types.\n",
    "    2. Locate a surface phrase that connects them and expresses the ontology relation label.\n",
    "    3. Record that phrase as the \"cue\" and quote the full supporting span that contains it.\n",
    "    4. If none is found, do not emit a triple for that relation.\n",
    "    5. Resolve simple pronouns only if doing so maintains correct domainâ†’range typing.\n",
    "\n",
    "    Text:\n",
    "    \"{_escape_multiline(text)}\"\n",
    "\n",
    "    Ontology concepts:\n",
    "    {ontology_concepts_block}\n",
    "\n",
    "    Ontology relations (domain â†’ range):\n",
    "    {ontology_relations_block}\n",
    "\n",
    "    Output JSON only in this exact schema:\n",
    "    {{\n",
    "      \"triples\": [\n",
    "        {{\n",
    "          \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "          \"confidence\": 0.0,  // confidence 0â€“1\n",
    "          \"cue\": \"matched cue phrase from text\",\n",
    "          \"support\": \"exact quoted span(s)\",\n",
    "          \"notes\": \"domain/range check; or pronoun resolution note if applied\"\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    - Extract ALL clearly stated factual triples in the text.\n",
    "    - If no relation cue appears, return an empty list of triples.\n",
    "    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n",
    "    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n",
    "    - Do not invent entities that are not mentioned in the text.\n",
    "    - Output MUST be valid JSON and nothing else.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf0c878-0a56-4347-947e-afef9a40d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 4: GENERATION + PARSER HELPERS\n",
    "########################################\n",
    "\n",
    "def generate_raw_json(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    max_new_tokens: int = 768,\n",
    "    temperature: float = 0.25,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build chat-style prompt for Prompt 2 and get model output.\n",
    "    We expect JSON-only, but we'll still post-parse later.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    out = generator(\n",
    "        prompt_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # HF pipeline returns list[{\"generated_text\": \"...\"}]\n",
    "    if isinstance(out[0], dict) and \"generated_text\" in out[0]:\n",
    "        return out[0][\"generated_text\"].strip()\n",
    "    else:\n",
    "        # fallback\n",
    "        return str(out[0]).strip()\n",
    "\n",
    "\n",
    "def try_parse_json(raw: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Best-effort parse of model output (should be JSON).\n",
    "    1. direct json.loads\n",
    "    2. fallback: grab first {...} block\n",
    "    \"\"\"\n",
    "    raw_strip = raw.strip()\n",
    "\n",
    "    # direct attempt\n",
    "    try:\n",
    "        return json.loads(raw_strip)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # fallback: regex for first {...}\n",
    "    m = re.search(r\"\\{.*\\}\", raw_strip, flags=re.DOTALL)\n",
    "    if m:\n",
    "        block = m.group(0)\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05442f85-6730-4669-93e0-6766e46c4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # BLOCK 5: SINGLE-FILE PIPELINE (Prompt 2)\n",
    "# ########################################\n",
    "\n",
    "\n",
    "# def run_pipeline_prompt2(\n",
    "#     ontology_path: str,\n",
    "#     input_jsonl_path: str,\n",
    "#     output_jsonl_path: str,\n",
    "#     k_triples: int = 5,\n",
    "#     max_items: Optional[int] = None,\n",
    "#     verbose: bool = True,\n",
    "#     model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#     generator=None,\n",
    "#     tokenizer=None,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Run Prompt 2 over a single dataset file:\n",
    "#       - load ontology + rows\n",
    "#       - build prompts\n",
    "#       - generate model output\n",
    "#       - parse JSON\n",
    "#       - write trace jsonl\n",
    "\n",
    "#     Behavior:\n",
    "#       â€¢ If `generator` and `tokenizer` are provided, reuse them (no new model load).\n",
    "#       â€¢ Otherwise, load the model from `model_id` internally (backward compatible).\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 1. load ontology\n",
    "#     ontology_json = load_ontology_json(ontology_path)\n",
    "\n",
    "#     # 2. init / reuse model\n",
    "#     local_model_loaded = False\n",
    "#     if generator is None or tokenizer is None:\n",
    "#         generator, tokenizer = setup_model(model_id=model_id)\n",
    "#         local_model_loaded = True  # so we know we \"own\" it, if you ever want cleanup\n",
    "\n",
    "#     results: List[Dict[str, Any]] = []\n",
    "\n",
    "#     # 3. iterate input rows\n",
    "#     #    (collect first, then batch-generate)\n",
    "#     recs: List[Dict[str, Any]] = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "\n",
    "#     # build all prompts first\n",
    "#     prompt_texts: List[str] = []\n",
    "#     meta: List[Tuple[str, str, str, str, str]] = []  # (rec_id, text_val, sys_prompt, usr_prompt, text_key)\n",
    "\n",
    "#     for idx, rec in enumerate(recs):\n",
    "#         rec_id = str(rec.get(\"id\") or f\"item_{idx}\")\n",
    "#         text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "#         sys_prompt = build_p2_system()\n",
    "#         usr_prompt = build_p2_user(text_val, ontology_json, k_triples)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"======================================\")\n",
    "#             print(f\"[ID] {rec_id}\")\n",
    "#             print(f\"[TEXT_KEY] {text_key}\")\n",
    "#             print(\"[SYSTEM PROMPT]\\n\", sys_prompt)\n",
    "#             print(\"[USER PROMPT]\\n\", usr_prompt)\n",
    "#             print(\"[SOURCE TEXT]\\n\", text_val)\n",
    "\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": sys_prompt},\n",
    "#             {\"role\": \"user\",   \"content\": usr_prompt},\n",
    "#         ]\n",
    "#         prompt_text = tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "\n",
    "#         prompt_texts.append(prompt_text)\n",
    "#         meta.append((rec_id, text_val, sys_prompt, usr_prompt, text_key))\n",
    "\n",
    "#     # now batch over prompt_texts\n",
    "#     total = len(prompt_texts)\n",
    "#     for start in range(0, total, BATCH_SIZE):\n",
    "#         end = min(start + BATCH_SIZE, total)\n",
    "#         batch_prompts = prompt_texts[start:end]\n",
    "\n",
    "#         # adaptive per-batch max_new_tokens\n",
    "#         lens = [tokenizer(p, return_tensors=\"pt\")[\"input_ids\"].shape[1] for p in batch_prompts]\n",
    "#         dyn_max = max(min(ADAPT_FACTOR * L, ADAPT_CAP) for L in lens) if lens else ADAPT_CAP\n",
    "\n",
    "#         t0 = time.time()\n",
    "#         out_batch = generator(\n",
    "#             batch_prompts,\n",
    "#             max_new_tokens=dyn_max,\n",
    "#             temperature=0.25,\n",
    "#             top_p=0.9,\n",
    "#             do_sample=True,\n",
    "#             return_full_text=False,\n",
    "#             truncation=False,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             pad_token_id=tokenizer.eos_token_id,\n",
    "#         )\n",
    "#         t1 = time.time()\n",
    "#         print(f\"[BATCH P2] items {start}-{end-1} / {total} | dyn_max={dyn_max} | time={t1 - t0:.2f}s\")\n",
    "\n",
    "#         # HF may return list-of-dicts directly for batched calls\n",
    "#         for i, out in enumerate(out_batch):\n",
    "#             # normalize HF output shape\n",
    "#             if isinstance(out, list) and out and isinstance(out[0], dict) and \"generated_text\" in out[0]:\n",
    "#                 raw_response = out[0][\"generated_text\"].strip()\n",
    "#             elif isinstance(out, dict) and \"generated_text\" in out:\n",
    "#                 raw_response = out[\"generated_text\"].strip()\n",
    "#             else:\n",
    "#                     raw_response = str(out).strip()\n",
    "\n",
    "\n",
    "#             parsed_json = try_parse_json(raw_response)\n",
    "\n",
    "#             if verbose:\n",
    "#                 print(\"[RAW RESPONSE]\\n\", raw_response)\n",
    "#                 print(\"[PARSED JSON]\\n\", parsed_json)\n",
    "\n",
    "#             out_record = {\n",
    "#                 \"id\": rec_id,\n",
    "#                 \"input text\": text_val,\n",
    "#                 \"prompts\": {\n",
    "#                     \"system_prompt\": sys_prompt,\n",
    "#                     \"user_prompt\": usr_prompt,\n",
    "#                 },\n",
    "#                 \"response\": {\n",
    "#                     \"LLM_output\": raw_response,\n",
    "#                     \"json\": parsed_json,\n",
    "#                 },\n",
    "#             }\n",
    "\n",
    "#             results.append(out_record)\n",
    "\n",
    "#     # 4. write collected results\n",
    "#     write_jsonl(output_jsonl_path, results)\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"\\n[P2 WRITE] {len(results)} rows -> {output_jsonl_path}\")\n",
    "\n",
    "#     # optional: if we loaded the model here locally we *could* free VRAM,\n",
    "#     # but usually you keep it for interactive use.\n",
    "#     # if local_model_loaded:\n",
    "#     #     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94903c98-0d60-4682-a2a9-67e4db64bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 5: SINGLE-FILE PIPELINE (Prompt 2)\n",
    "########################################\n",
    "\n",
    "\n",
    "def run_pipeline_prompt2(\n",
    "    ontology_path: str,\n",
    "    input_jsonl_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    k_triples: int = 5,\n",
    "    max_items: Optional[int] = None,\n",
    "    verbose: bool = True,\n",
    "    model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    generator=None,\n",
    "    tokenizer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Prompt 2 over a single dataset file:\n",
    "      - load ontology + rows\n",
    "      - build prompts\n",
    "      - generate model output\n",
    "      - parse JSON\n",
    "      - write trace jsonl\n",
    "\n",
    "    Behavior:\n",
    "      â€¢ If `generator` and `tokenizer` are provided, reuse them (no new model load).\n",
    "      â€¢ Otherwise, load the model from `model_id` internally (backward compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. load ontology\n",
    "    ontology_json = load_ontology_json(ontology_path)\n",
    "\n",
    "    # 2. init / reuse model\n",
    "    local_model_loaded = False\n",
    "    if generator is None or tokenizer is None:\n",
    "        generator, tokenizer = setup_model(model_id=model_id)\n",
    "        local_model_loaded = True  # so we know we \"own\" it, if you ever want cleanup\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 3. iterate input rows\n",
    "    #    (collect first, then batch-generate)\n",
    "    recs: List[Dict[str, Any]] = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "\n",
    "    # build all prompts first\n",
    "    prompt_texts: List[str] = []\n",
    "    meta: List[Tuple[str, str, str, str, str]] = []  # (rec_id, text_val, sys_prompt, usr_prompt, text_key)\n",
    "\n",
    "    for idx, rec in enumerate(recs):\n",
    "        rec_id = str(rec.get(\"id\") or f\"item_{idx}\")\n",
    "        text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "        sys_prompt = build_p2_system()\n",
    "        usr_prompt = build_p2_user(text_val, ontology_json, k_triples)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"======================================\")\n",
    "            print(f\"[ID] {rec_id}\")\n",
    "            print(f\"[TEXT_KEY] {text_key}\")\n",
    "            print(\"[SYSTEM PROMPT]\\n\", sys_prompt)\n",
    "            print(\"[USER PROMPT]\\n\", usr_prompt)\n",
    "            print(\"[SOURCE TEXT]\\n\", text_val)\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\",   \"content\": usr_prompt},\n",
    "        ]\n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        prompt_texts.append(prompt_text)\n",
    "        meta.append((rec_id, text_val, sys_prompt, usr_prompt, text_key))\n",
    "\n",
    "    # now batch over prompt_texts\n",
    "    total = len(prompt_texts)\n",
    "    for start in range(0, total, BATCH_SIZE):\n",
    "        end = min(start + BATCH_SIZE, total)\n",
    "        batch_prompts = prompt_texts[start:end]\n",
    "\n",
    "        # adaptive per-batch max_new_tokens\n",
    "        lens = [tokenizer(p, return_tensors=\"pt\")[\"input_ids\"].shape[1] for p in batch_prompts]\n",
    "        dyn_max = max(min(ADAPT_FACTOR * L, ADAPT_CAP) for L in lens) if lens else ADAPT_CAP\n",
    "\n",
    "        t0 = time.time()\n",
    "        out_batch = generator(\n",
    "            batch_prompts,\n",
    "            max_new_tokens=dyn_max,\n",
    "            temperature=0.25,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            return_full_text=False,\n",
    "            truncation=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(f\"[BATCH P2] items {start}-{end-1} / {total} | dyn_max={dyn_max} | time={t1 - t0:.2f}s\")\n",
    "\n",
    "        # HF may return list-of-dicts directly for batched calls\n",
    "        for i, out in enumerate(out_batch):\n",
    "            # ðŸ‘‡ this is the key missing piece\n",
    "            meta_idx = start + i\n",
    "            rec_id, text_val, sys_prompt, usr_prompt, text_key = meta[meta_idx]\n",
    "\n",
    "            # normalize HF output shape\n",
    "            if isinstance(out, list) and out and isinstance(out[0], dict) and \"generated_text\" in out[0]:\n",
    "                raw_response = out[0][\"generated_text\"].strip()\n",
    "            elif isinstance(out, dict) and \"generated_text\" in out:\n",
    "                raw_response = out[\"generated_text\"].strip()\n",
    "            else:\n",
    "                raw_response = str(out).strip()\n",
    "\n",
    "            parsed_json = try_parse_json(raw_response)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"[RAW RESPONSE]\\n\", raw_response)\n",
    "                print(\"[PARSED JSON]\\n\", parsed_json)\n",
    "\n",
    "            out_record = {\n",
    "                \"id\": rec_id,\n",
    "                \"input text\": text_val,\n",
    "                \"prompts\": {\n",
    "                    \"system_prompt\": sys_prompt,\n",
    "                    \"user_prompt\": usr_prompt,\n",
    "                },\n",
    "                \"response\": {\n",
    "                    \"LLM_output\": raw_response,\n",
    "                    \"json\": parsed_json,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            results.append(out_record)\n",
    "\n",
    "    # 4. write collected results\n",
    "    write_jsonl(output_jsonl_path, results)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n[P2 WRITE] {len(results)} rows -> {output_jsonl_path}\")\n",
    "\n",
    "    # optional: if we loaded the model here locally we *could* free VRAM,\n",
    "    # but usually you keep it for interactive use.\n",
    "    # if local_model_loaded:\n",
    "    #     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2961a7fe-2bb7-41ed-a091-8e6f318a1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################\n",
    "# # BLOCK 9: MANUAL TEST RUN (single file Prompt 2)\n",
    "# ########################################\n",
    "\n",
    "# run_pipeline_prompt2(\n",
    "#     ontology_path=ONTOLOGY_JSON,\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     k_triples=K_TRIPLES,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     verbose=VERBOSE,   #True for first check\n",
    "#     model_id=MODEL_ID,\n",
    "#     generator=None,\n",
    "#     tokenizer=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d81c4c-c013-4592-94c6-c3e765382075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "379d7655-8dd5-4e7a-a26d-f916913fc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "PATTERN = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_paths(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Given something like 'ont_8_politician_test.jsonl', build:\n",
    "      - input_jsonl_path      -> <base_input>/ont_8_politician_test.jsonl\n",
    "      - ontology_json_path    -> <base_onto>/8_politician_ontology.json\n",
    "      - output_jsonl_path     -> <base_out>/ont_8_politician_output.jsonl\n",
    "      - tag                   -> 'ont_8_politician'\n",
    "    \"\"\"\n",
    "    m = PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_wikidata_batch_p2(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        #\"ont_1_movie_test.jsonl\",\n",
    "        #\"ont_2_music_test.jsonl\",\n",
    "        #\"ont_3_sport_test.jsonl\",\n",
    "        #\"ont_4_book_test.jsonl\",\n",
    "        #\"ont_5_military_test.jsonl\",\n",
    "        \"ont_6_computer_test.jsonl\",\n",
    "        \"ont_7_space_test.jsonl\",\n",
    "        \"ont_8_politics_test.jsonl\",\n",
    "        \"ont_9_nature_test.jsonl\",\n",
    "        \"ont_10_culture_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # 1. load the model ONCE and reuse it for all files\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    # 2. loop over all files\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # build all required paths for this file using the shared logic\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_paths(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P2] wikidata {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            # 3. run the single-file pipeline, reusing the loaded model\n",
    "            run_pipeline_prompt2(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES,\n",
    "                max_items=None,\n",
    "                verbose=False,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse\n",
    "                tokenizer=tokenizer,   # reuse\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P2] wikidata {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P2] wikidata {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7f92e4-189d-4f95-b35b-ebd3f2a823f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be45a36a52549f6b53342f50b87efb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_6_computer\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_6_computer_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/6_computer_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_6_computer_output.jsonl\n",
      "[BATCH P2] items 0-15 / 230 | dyn_max=1648 | time=197.45s\n",
      "[BATCH P2] items 16-31 / 230 | dyn_max=1746 | time=213.45s\n",
      "[BATCH P2] items 32-47 / 230 | dyn_max=1692 | time=243.06s\n",
      "[BATCH P2] items 48-63 / 230 | dyn_max=1686 | time=205.86s\n",
      "[BATCH P2] items 64-79 / 230 | dyn_max=1742 | time=166.60s\n",
      "[BATCH P2] items 80-95 / 230 | dyn_max=1686 | time=163.50s\n",
      "[BATCH P2] items 96-111 / 230 | dyn_max=1660 | time=145.94s\n",
      "[BATCH P2] items 112-127 / 230 | dyn_max=1766 | time=302.62s\n",
      "[BATCH P2] items 128-143 / 230 | dyn_max=1710 | time=263.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH P2] items 144-159 / 230 | dyn_max=1686 | time=194.39s\n",
      "[BATCH P2] items 160-175 / 230 | dyn_max=1716 | time=254.82s\n",
      "[BATCH P2] items 176-191 / 230 | dyn_max=1672 | time=157.17s\n",
      "[BATCH P2] items 192-207 / 230 | dyn_max=1716 | time=159.43s\n",
      "[BATCH P2] items 208-223 / 230 | dyn_max=1686 | time=161.94s\n",
      "[BATCH P2] items 224-229 / 230 | dyn_max=1688 | time=69.15s\n",
      "[DONE P2] wikidata ont_6_computer\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_7_space\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_7_space_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/7_space_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_7_space_output.jsonl\n",
      "[BATCH P2] items 0-15 / 203 | dyn_max=1808 | time=150.84s\n",
      "[BATCH P2] items 16-31 / 203 | dyn_max=1818 | time=132.50s\n",
      "[BATCH P2] items 32-47 / 203 | dyn_max=1810 | time=164.84s\n",
      "[BATCH P2] items 48-63 / 203 | dyn_max=1860 | time=125.63s\n",
      "[BATCH P2] items 64-79 / 203 | dyn_max=1834 | time=118.38s\n",
      "[BATCH P2] items 80-95 / 203 | dyn_max=1764 | time=94.56s\n",
      "[BATCH P2] items 96-111 / 203 | dyn_max=1798 | time=95.35s\n",
      "[BATCH P2] items 112-127 / 203 | dyn_max=1796 | time=98.55s\n",
      "[BATCH P2] items 128-143 / 203 | dyn_max=1928 | time=216.29s\n",
      "[BATCH P2] items 144-159 / 203 | dyn_max=1928 | time=185.42s\n",
      "[BATCH P2] items 160-175 / 203 | dyn_max=1836 | time=174.63s\n",
      "[BATCH P2] items 176-191 / 203 | dyn_max=1832 | time=165.17s\n",
      "[BATCH P2] items 192-202 / 203 | dyn_max=1800 | time=114.33s\n",
      "[DONE P2] wikidata ont_7_space\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_8_politics\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_8_politics_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/8_politics_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_8_politics_output.jsonl\n",
      "[BATCH P2] items 0-15 / 214 | dyn_max=1820 | time=148.18s\n",
      "[BATCH P2] items 16-31 / 214 | dyn_max=1826 | time=166.88s\n",
      "[BATCH P2] items 32-47 / 214 | dyn_max=1818 | time=144.55s\n",
      "[BATCH P2] items 48-63 / 214 | dyn_max=1808 | time=179.51s\n",
      "[BATCH P2] items 64-79 / 214 | dyn_max=1812 | time=172.50s\n",
      "[BATCH P2] items 80-95 / 214 | dyn_max=1832 | time=113.26s\n",
      "[BATCH P2] items 96-111 / 214 | dyn_max=1818 | time=145.81s\n",
      "[BATCH P2] items 112-127 / 214 | dyn_max=1816 | time=168.88s\n",
      "[BATCH P2] items 128-143 / 214 | dyn_max=1784 | time=147.94s\n",
      "[BATCH P2] items 144-159 / 214 | dyn_max=1758 | time=134.97s\n",
      "[BATCH P2] items 160-175 / 214 | dyn_max=1802 | time=191.96s\n",
      "[BATCH P2] items 176-191 / 214 | dyn_max=1802 | time=200.29s\n",
      "[BATCH P2] items 192-207 / 214 | dyn_max=1808 | time=199.91s\n",
      "[BATCH P2] items 208-213 / 214 | dyn_max=1778 | time=49.59s\n",
      "[DONE P2] wikidata ont_8_politics\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_9_nature\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_9_nature_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/9_nature_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_9_nature_output.jsonl\n",
      "[BATCH P2] items 0-15 / 474 | dyn_max=1960 | time=236.45s\n",
      "[BATCH P2] items 16-31 / 474 | dyn_max=1940 | time=184.40s\n",
      "[BATCH P2] items 32-47 / 474 | dyn_max=1942 | time=164.01s\n",
      "[BATCH P2] items 48-63 / 474 | dyn_max=1940 | time=145.62s\n",
      "[BATCH P2] items 64-79 / 474 | dyn_max=2000 | time=151.85s\n",
      "[BATCH P2] items 80-95 / 474 | dyn_max=1982 | time=159.27s\n",
      "[BATCH P2] items 96-111 / 474 | dyn_max=2000 | time=226.13s\n",
      "[BATCH P2] items 112-127 / 474 | dyn_max=2000 | time=122.35s\n",
      "[BATCH P2] items 128-143 / 474 | dyn_max=1934 | time=151.94s\n",
      "[BATCH P2] items 144-159 / 474 | dyn_max=1944 | time=101.28s\n",
      "[BATCH P2] items 160-175 / 474 | dyn_max=1992 | time=150.14s\n",
      "[BATCH P2] items 176-191 / 474 | dyn_max=2000 | time=195.91s\n",
      "[BATCH P2] items 192-207 / 474 | dyn_max=1968 | time=262.59s\n",
      "[BATCH P2] items 208-223 / 474 | dyn_max=2000 | time=156.24s\n",
      "[BATCH P2] items 224-239 / 474 | dyn_max=1988 | time=149.60s\n",
      "[BATCH P2] items 240-255 / 474 | dyn_max=1974 | time=147.01s\n",
      "[BATCH P2] items 256-271 / 474 | dyn_max=2000 | time=147.25s\n",
      "[BATCH P2] items 272-287 / 474 | dyn_max=1918 | time=133.24s\n",
      "[BATCH P2] items 288-303 / 474 | dyn_max=1984 | time=167.03s\n",
      "[BATCH P2] items 304-319 / 474 | dyn_max=1976 | time=174.02s\n",
      "[BATCH P2] items 320-335 / 474 | dyn_max=1928 | time=162.41s\n",
      "[BATCH P2] items 336-351 / 474 | dyn_max=1968 | time=172.06s\n",
      "[BATCH P2] items 352-367 / 474 | dyn_max=2000 | time=137.93s\n",
      "[BATCH P2] items 368-383 / 474 | dyn_max=1984 | time=125.77s\n",
      "[BATCH P2] items 384-399 / 474 | dyn_max=1940 | time=166.67s\n",
      "[BATCH P2] items 400-415 / 474 | dyn_max=1926 | time=154.78s\n",
      "[BATCH P2] items 416-431 / 474 | dyn_max=1950 | time=169.54s\n",
      "[BATCH P2] items 432-447 / 474 | dyn_max=1964 | time=165.63s\n",
      "[BATCH P2] items 448-463 / 474 | dyn_max=2000 | time=197.10s\n",
      "[BATCH P2] items 464-473 / 474 | dyn_max=1982 | time=92.82s\n",
      "[DONE P2] wikidata ont_9_nature\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] wikidata ont_10_culture\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_10_culture_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/10_culture_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/wikidata/ont_10_culture_output.jsonl\n",
      "[BATCH P2] items 0-15 / 159 | dyn_max=1766 | time=141.20s\n",
      "[BATCH P2] items 16-31 / 159 | dyn_max=1770 | time=146.15s\n",
      "[BATCH P2] items 32-47 / 159 | dyn_max=2000 | time=209.69s\n",
      "[BATCH P2] items 48-63 / 159 | dyn_max=1824 | time=139.61s\n",
      "[BATCH P2] items 64-79 / 159 | dyn_max=1778 | time=164.00s\n",
      "[BATCH P2] items 80-95 / 159 | dyn_max=1788 | time=142.56s\n",
      "[BATCH P2] items 96-111 / 159 | dyn_max=1708 | time=145.97s\n",
      "[BATCH P2] items 112-127 / 159 | dyn_max=1722 | time=149.71s\n",
      "[BATCH P2] items 128-143 / 159 | dyn_max=1692 | time=178.20s\n",
      "[BATCH P2] items 144-158 / 159 | dyn_max=1826 | time=148.13s\n",
      "[DONE P2] wikidata ont_10_culture\n"
     ]
    }
   ],
   "source": [
    "run_wikidata_batch_p2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bce12c5-9f12-4ec2-8838-04b69184a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "DBPEDIA_PATTERN = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "\n",
    "def make_dbpedia_paths(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Given 'ont_14_writtenwork_test.jsonl', return:\n",
    "      - input_jsonl_path      -> <base_input>/ont_14_writtenwork_test.jsonl\n",
    "      - ontology_json_path    -> <base_onto>/14_writtenwork_ontology.json\n",
    "      - output_jsonl_path     -> <base_out>/ont_14_writtenwork_output.jsonl\n",
    "      - tag                   -> 'ont_14_writtenwork'\n",
    "    \"\"\"\n",
    "    m = DBPEDIA_PATTERN.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_dbpedia_batch_p2(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        #\"ont_12_monument_test.jsonl\",\n",
    "        #\"ont_1_university_test.jsonl\",\n",
    "        #\"ont_2_musicalwork_test.jsonl\",\n",
    "        #\"ont_3_airport_test.jsonl\",\n",
    "        #\"ont_4_building_test.jsonl\",\n",
    "        #\"ont_5_athlete_test.jsonl\",\n",
    "        \"ont_6_politician_test.jsonl\",\n",
    "        \"ont_7_company_test.jsonl\",\n",
    "        \"ont_8_celestialbody_test.jsonl\",\n",
    "        \"ont_9_astronaut_test.jsonl\",\n",
    "        \"ont_10_comicscharacter_test.jsonl\",\n",
    "        \"ont_11_meanoftransportation_test.jsonl\",\n",
    "        \"ont_13_food_test.jsonl\",\n",
    "        \"ont_14_writtenwork_test.jsonl\",\n",
    "        \"ont_15_sportsteam_test.jsonl\",\n",
    "        \"ont_16_city_test.jsonl\",\n",
    "        \"ont_17_artist_test.jsonl\",\n",
    "        \"ont_18_scientist_test.jsonl\",\n",
    "        \"ont_19_film_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # 1. load the model ONCE and reuse it for all dbpedia files\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    # 2. iterate over each file\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            # construct all paths for this file\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_dbpedia_paths(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P2] dbpedia {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            # 3. call the single-file pipeline, reusing the SAME model\n",
    "            run_pipeline_prompt2(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES,\n",
    "                max_items=None,\n",
    "                verbose=False,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse the already-loaded model\n",
    "                tokenizer=tokenizer,   # reuse tokenizer\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P2] dbpedia {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P2] dbpedia {fname}: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311649f2-f8fe-4143-b9f8-455e7cb7fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] model=mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27804b72e209404ba9208aca6149dee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_6_politician\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_6_politician_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/6_politician_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_6_politician_output.jsonl\n",
      "[BATCH P2] items 0-15 / 135 | dyn_max=2000 | time=180.81s\n",
      "[BATCH P2] items 16-31 / 135 | dyn_max=2000 | time=220.78s\n",
      "[BATCH P2] items 32-47 / 135 | dyn_max=2000 | time=266.82s\n",
      "[BATCH P2] items 48-63 / 135 | dyn_max=2000 | time=227.56s\n",
      "[BATCH P2] items 64-79 / 135 | dyn_max=2000 | time=187.01s\n",
      "[BATCH P2] items 80-95 / 135 | dyn_max=2000 | time=166.87s\n",
      "[BATCH P2] items 96-111 / 135 | dyn_max=2000 | time=283.14s\n",
      "[BATCH P2] items 112-127 / 135 | dyn_max=2000 | time=221.04s\n",
      "[BATCH P2] items 128-134 / 135 | dyn_max=2000 | time=80.90s\n",
      "[DONE P2] dbpedia ont_6_politician\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_7_company\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_7_company_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/7_company_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_7_company_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH P2] items 0-15 / 56 | dyn_max=2000 | time=212.66s\n",
      "[BATCH P2] items 16-31 / 56 | dyn_max=2000 | time=176.53s\n",
      "[BATCH P2] items 32-47 / 56 | dyn_max=2000 | time=162.94s\n",
      "[BATCH P2] items 48-55 / 56 | dyn_max=2000 | time=103.25s\n",
      "[DONE P2] dbpedia ont_7_company\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_8_celestialbody\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_8_celestialbody_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/8_celestialbody_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_8_celestialbody_output.jsonl\n",
      "[BATCH P2] items 0-15 / 72 | dyn_max=2000 | time=207.74s\n",
      "[BATCH P2] items 16-31 / 72 | dyn_max=2000 | time=232.82s\n",
      "[BATCH P2] items 32-47 / 72 | dyn_max=2000 | time=238.29s\n",
      "[BATCH P2] items 48-63 / 72 | dyn_max=2000 | time=257.62s\n",
      "[BATCH P2] items 64-71 / 72 | dyn_max=2000 | time=124.55s\n",
      "[DONE P2] dbpedia ont_8_celestialbody\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_9_astronaut\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_9_astronaut_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/9_astronaut_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_9_astronaut_output.jsonl\n",
      "[BATCH P2] items 0-15 / 68 | dyn_max=2000 | time=142.34s\n",
      "[BATCH P2] items 16-31 / 68 | dyn_max=2000 | time=174.75s\n",
      "[BATCH P2] items 32-47 / 68 | dyn_max=2000 | time=264.10s\n",
      "[BATCH P2] items 48-63 / 68 | dyn_max=2000 | time=242.66s\n",
      "[BATCH P2] items 64-67 / 68 | dyn_max=2000 | time=35.20s\n",
      "[DONE P2] dbpedia ont_9_astronaut\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_10_comicscharacter\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_10_comicscharacter_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/10_comicscharacter_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_10_comicscharacter_output.jsonl\n",
      "[BATCH P2] items 0-15 / 36 | dyn_max=1888 | time=135.13s\n",
      "[BATCH P2] items 16-31 / 36 | dyn_max=1878 | time=150.30s\n",
      "[BATCH P2] items 32-35 / 36 | dyn_max=1868 | time=45.42s\n",
      "[DONE P2] dbpedia ont_10_comicscharacter\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_11_meanoftransportation\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_11_meanoftransportation_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/11_meanoftransportation_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_11_meanoftransportation_output.jsonl\n",
      "[BATCH P2] items 0-15 / 92 | dyn_max=2000 | time=200.90s\n",
      "[BATCH P2] items 16-31 / 92 | dyn_max=2000 | time=218.09s\n",
      "[BATCH P2] items 32-47 / 92 | dyn_max=2000 | time=154.58s\n",
      "[BATCH P2] items 48-63 / 92 | dyn_max=2000 | time=180.75s\n",
      "[BATCH P2] items 64-79 / 92 | dyn_max=2000 | time=164.83s\n",
      "[BATCH P2] items 80-91 / 92 | dyn_max=2000 | time=125.47s\n",
      "[DONE P2] dbpedia ont_11_meanoftransportation\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_13_food\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_13_food_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/13_food_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_13_food_output.jsonl\n",
      "[BATCH P2] items 0-15 / 153 | dyn_max=2000 | time=205.41s\n",
      "[BATCH P2] items 16-31 / 153 | dyn_max=1996 | time=164.26s\n",
      "[BATCH P2] items 32-47 / 153 | dyn_max=2000 | time=222.90s\n",
      "[BATCH P2] items 48-63 / 153 | dyn_max=2000 | time=178.37s\n",
      "[BATCH P2] items 64-79 / 153 | dyn_max=1994 | time=212.76s\n",
      "[BATCH P2] items 80-95 / 153 | dyn_max=2000 | time=208.33s\n",
      "[BATCH P2] items 96-111 / 153 | dyn_max=2000 | time=274.77s\n",
      "[BATCH P2] items 112-127 / 153 | dyn_max=2000 | time=230.06s\n",
      "[BATCH P2] items 128-143 / 153 | dyn_max=2000 | time=166.98s\n",
      "[BATCH P2] items 144-152 / 153 | dyn_max=2000 | time=84.70s\n",
      "[DONE P2] dbpedia ont_13_food\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_14_writtenwork\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_14_writtenwork_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/14_writtenwork_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_14_writtenwork_output.jsonl\n",
      "[BATCH P2] items 0-15 / 127 | dyn_max=2000 | time=182.71s\n",
      "[BATCH P2] items 16-31 / 127 | dyn_max=2000 | time=186.14s\n",
      "[BATCH P2] items 32-47 / 127 | dyn_max=2000 | time=148.14s\n",
      "[BATCH P2] items 48-63 / 127 | dyn_max=2000 | time=188.90s\n",
      "[BATCH P2] items 64-79 / 127 | dyn_max=2000 | time=257.36s\n",
      "[BATCH P2] items 80-95 / 127 | dyn_max=2000 | time=165.14s\n",
      "[BATCH P2] items 96-111 / 127 | dyn_max=2000 | time=177.85s\n",
      "[BATCH P2] items 112-126 / 127 | dyn_max=2000 | time=171.54s\n",
      "[DONE P2] dbpedia ont_14_writtenwork\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_15_sportsteam\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_15_sportsteam_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/15_sportsteam_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_15_sportsteam_output.jsonl\n",
      "[BATCH P2] items 0-15 / 110 | dyn_max=2000 | time=224.13s\n",
      "[BATCH P2] items 16-31 / 110 | dyn_max=2000 | time=203.45s\n",
      "[BATCH P2] items 32-47 / 110 | dyn_max=2000 | time=231.16s\n",
      "[BATCH P2] items 48-63 / 110 | dyn_max=2000 | time=252.22s\n",
      "[BATCH P2] items 64-79 / 110 | dyn_max=2000 | time=279.89s\n",
      "[BATCH P2] items 80-95 / 110 | dyn_max=2000 | time=361.39s\n",
      "[BATCH P2] items 96-109 / 110 | dyn_max=2000 | time=223.72s\n",
      "[DONE P2] dbpedia ont_15_sportsteam\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_16_city\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_16_city_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/16_city_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_16_city_output.jsonl\n",
      "[BATCH P2] items 0-15 / 217 | dyn_max=1974 | time=201.86s\n",
      "[BATCH P2] items 16-31 / 217 | dyn_max=1942 | time=212.98s\n",
      "[BATCH P2] items 32-47 / 217 | dyn_max=1956 | time=202.14s\n",
      "[BATCH P2] items 48-63 / 217 | dyn_max=1948 | time=190.03s\n",
      "[BATCH P2] items 64-79 / 217 | dyn_max=1970 | time=233.22s\n",
      "[BATCH P2] items 80-95 / 217 | dyn_max=1962 | time=216.39s\n",
      "[BATCH P2] items 96-111 / 217 | dyn_max=1980 | time=213.50s\n",
      "[BATCH P2] items 112-127 / 217 | dyn_max=1936 | time=186.74s\n",
      "[BATCH P2] items 128-143 / 217 | dyn_max=1944 | time=210.82s\n",
      "[BATCH P2] items 144-159 / 217 | dyn_max=1952 | time=233.91s\n",
      "[BATCH P2] items 160-175 / 217 | dyn_max=1952 | time=201.30s\n",
      "[BATCH P2] items 176-191 / 217 | dyn_max=1950 | time=223.81s\n",
      "[BATCH P2] items 192-207 / 217 | dyn_max=1972 | time=191.88s\n",
      "[BATCH P2] items 208-216 / 217 | dyn_max=1942 | time=146.54s\n",
      "[DONE P2] dbpedia ont_16_city\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_17_artist\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_17_artist_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/17_artist_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_17_artist_output.jsonl\n",
      "[BATCH P2] items 0-15 / 84 | dyn_max=2000 | time=165.14s\n",
      "[BATCH P2] items 16-31 / 84 | dyn_max=2000 | time=146.92s\n",
      "[BATCH P2] items 32-47 / 84 | dyn_max=2000 | time=178.34s\n",
      "[BATCH P2] items 48-63 / 84 | dyn_max=2000 | time=157.82s\n",
      "[BATCH P2] items 64-79 / 84 | dyn_max=2000 | time=166.85s\n",
      "[BATCH P2] items 80-83 / 84 | dyn_max=2000 | time=44.86s\n",
      "[DONE P2] dbpedia ont_17_artist\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_18_scientist\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_18_scientist_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/18_scientist_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_18_scientist_output.jsonl\n",
      "[BATCH P2] items 0-15 / 149 | dyn_max=2000 | time=157.07s\n",
      "[BATCH P2] items 16-31 / 149 | dyn_max=2000 | time=144.99s\n",
      "[BATCH P2] items 32-47 / 149 | dyn_max=2000 | time=182.53s\n",
      "[BATCH P2] items 48-63 / 149 | dyn_max=2000 | time=174.04s\n",
      "[BATCH P2] items 64-79 / 149 | dyn_max=2000 | time=211.95s\n",
      "[BATCH P2] items 80-95 / 149 | dyn_max=2000 | time=123.71s\n",
      "[BATCH P2] items 96-111 / 149 | dyn_max=2000 | time=178.61s\n",
      "[BATCH P2] items 112-127 / 149 | dyn_max=2000 | time=194.03s\n",
      "[BATCH P2] items 128-143 / 149 | dyn_max=2000 | time=174.77s\n",
      "[BATCH P2] items 144-148 / 149 | dyn_max=2000 | time=53.08s\n",
      "[DONE P2] dbpedia ont_18_scientist\n",
      "\n",
      "================================================================================\n",
      "[RUN P2] dbpedia ont_19_film\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/ont_19_film_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/19_film_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt2/dbpedia/ont_19_film_output.jsonl\n",
      "[BATCH P2] items 0-15 / 127 | dyn_max=2000 | time=179.23s\n",
      "[BATCH P2] items 16-31 / 127 | dyn_max=2000 | time=235.23s\n",
      "[BATCH P2] items 32-47 / 127 | dyn_max=2000 | time=190.72s\n",
      "[BATCH P2] items 48-63 / 127 | dyn_max=2000 | time=200.35s\n",
      "[BATCH P2] items 64-79 / 127 | dyn_max=2000 | time=228.90s\n",
      "[BATCH P2] items 80-95 / 127 | dyn_max=2000 | time=132.41s\n",
      "[BATCH P2] items 96-111 / 127 | dyn_max=2000 | time=152.57s\n",
      "[BATCH P2] items 112-126 / 127 | dyn_max=2000 | time=173.34s\n",
      "[DONE P2] dbpedia ont_19_film\n"
     ]
    }
   ],
   "source": [
    "run_dbpedia_batch_p2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c4606-f215-4f9c-8735-f531883271e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
