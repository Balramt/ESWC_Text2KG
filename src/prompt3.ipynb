{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac9943d-1fad-496f-843b-38c66ae3f38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  2 12:54:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:81:00.0 Off |                  N/A |\n",
      "| 84%   62C    P3            123W /  370W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b78e488-a6f4-49d1-90df-cd5001a87ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: kill: (1377479) - No such process\n"
     ]
    }
   ],
   "source": [
    "!kill 1377479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ffa3a6-e4a7-4a91-badb-8568d62576d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prompt 3 Notebook (Open-IE under ontology) ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from textwrap import dedent\n",
    "from typing import Dict, Any, List, Tuple, Optional, Iterable\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764bfe56-87de-4b1a-bdb8-a102f9b6b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\"):\n",
    "    \"\"\"\n",
    "    Load tokenizer + model + text-generation pipeline (GPU if available).\n",
    "    Mirrors Prompt 1/2: device_map='auto', fp16, cache enabled.\n",
    "    Returns (generator_pipeline, tokenizer).\n",
    "    \"\"\"\n",
    "    print(\"⏳ Loading model:\", model_id)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",            # use GPU(s) if available\n",
    "        torch_dtype=torch.float16,    # fp16 on CUDA\n",
    "    )\n",
    "    model.config.use_cache = True\n",
    "\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device_map=\"auto\",            # keep consistent with model\n",
    "    )\n",
    "    return generator, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1410a1-19bc-493a-b259-a8a4d49c41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str, max_items: Optional[int] = None) -> Iterable[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generator yielding JSON objects from a JSONL file.\n",
    "    Honors max_items if provided (int).\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            n += 1\n",
    "            if isinstance(max_items, int) and n >= max_items:\n",
    "                break\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dict records to a JSONL file.\n",
    "    Creates parent directories if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded9e97d-d9e9-4ff8-b3b2-511c08b360d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "def _extract_text(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Return (text, key_used) from an input record.\n",
    "    Mirrors Prompt 1/2 behavior: prefer common keys; else longest string field.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "    # Fallback: choose the longest string-valued field\n",
    "    best_k, best_v = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_v):\n",
    "            best_k, best_v = k, v\n",
    "    return best_v, best_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a80c77ba-506f-4a34-beb1-57affa94ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 2: DATA I/O HELPERS\n",
    "########################################\n",
    "\n",
    "_TEXT_KEYS_PRIORITY = (\"sent\", \"text\", \"Text\", \"sentence\", \"Sentence\")\n",
    "\n",
    "\n",
    "def read_jsonl(path: str, max_items: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Stream records from a .jsonl file.\n",
    "    Stops early if max_items is provided.\n",
    "    Yields dicts.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "            count += 1\n",
    "            if max_items is not None and count >= max_items:\n",
    "                break\n",
    "\n",
    "\n",
    "def write_jsonl(path: str, records: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Write a list of dicts as JSON lines.\n",
    "    Creates parent directory if needed.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec, ensure_ascii=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "def extract_text_field(rec: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Heuristic to pick the text field from an input record.\n",
    "    Returns (text_value, key_used).\n",
    "    Falls back to the longest string field if none of the preferred keys exist.\n",
    "    \"\"\"\n",
    "    for k in _TEXT_KEYS_PRIORITY:\n",
    "        v = rec.get(k)\n",
    "        if isinstance(v, str) and v.strip():\n",
    "            return v.strip(), k\n",
    "\n",
    "    # fallback: choose longest string in record\n",
    "    best_key, best_val = \"\", \"\"\n",
    "    for k, v in rec.items():\n",
    "        if isinstance(v, str) and len(v) > len(best_val):\n",
    "            best_key, best_val = k, v\n",
    "    return best_val.strip(), best_key\n",
    "\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes so we can safely embed text\n",
    "    inside quoted blocks in the USER prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a576e97d-0656-418d-bb71-276e258dfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# BLOCK 3: ONTOLOGY HELPERS + PROMPT 2 MESSAGE BUILDERS\n",
    "########################################\n",
    "\n",
    "def load_ontology_json(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load ontology JSON file.\n",
    "    Expected structure:\n",
    "      {\n",
    "        \"concepts\": [\n",
    "          {\"id\": \"...\", \"qid\": \"...\", \"label\": \"SomeClass\"},\n",
    "          ...\n",
    "        ],\n",
    "        \"relations\": [\n",
    "          {\n",
    "            \"id\": \"...\",\n",
    "            \"label\": \"location\",\n",
    "            \"domain\": \"SomeConceptID\",\n",
    "            \"range\": \"SomeConceptID\"\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def _build_concept_index(ontology_json: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Map any known identifier to its human-readable label.\n",
    "    Keys include 'qid', 'id', 'label' (all as strings).\n",
    "    \"\"\"\n",
    "    idx: Dict[str, str] = {}\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        label = str(c.get(\"label\", \"\")).strip()\n",
    "        if not label:\n",
    "            continue\n",
    "        for keyname in (\"qid\", \"id\", \"label\"):\n",
    "            val = c.get(keyname)\n",
    "            if val is None:\n",
    "                continue\n",
    "            sval = str(val).strip()\n",
    "            if sval:\n",
    "                idx[sval] = label\n",
    "    return idx\n",
    "\n",
    "def _label_for(value: Any, cindex: Dict[str, str]) -> str:\n",
    "    \"\"\"Return the human-readable label for a given value, if known.\"\"\"\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    sval = str(value).strip()\n",
    "    return cindex.get(sval, sval)\n",
    "\n",
    "def format_ontology_concepts(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"Render concepts as a comma-separated list of labels (like Prompt 1/2).\"\"\"\n",
    "    labels: List[str] = []\n",
    "    for c in ontology_json.get(\"concepts\", []):\n",
    "        lab = str(c.get(\"label\", \"\")).strip()\n",
    "        if lab:\n",
    "            labels.append(lab)\n",
    "    return \", \".join(labels)\n",
    "\n",
    "def format_ontology_relations(ontology_json: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render relations one per line, showing domain → range with labels.\n",
    "    Example:\n",
    "      - director(Film, Human)\n",
    "      - country of origin(Film, Country)\n",
    "    \"\"\"\n",
    "    cindex = _build_concept_index(ontology_json)\n",
    "    lines: List[str] = []\n",
    "    for r in ontology_json.get(\"relations\", []):\n",
    "        rel_label = str(r.get(\"label\", \"\")).strip()\n",
    "        dom_label = _label_for(r.get(\"domain\"), cindex)\n",
    "        rng_label = _label_for(r.get(\"range\"), cindex)\n",
    "        if rel_label:\n",
    "            lines.append(f\"- {rel_label}({dom_label},{rng_label})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def _escape_multiline(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Escape backslashes and quotes for safe embedding in the user prompt.\n",
    "    \"\"\"\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebba79e4-a96c-4cf8-881f-b6c22ebdf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p3_system() -> str:\n",
    "    return (\n",
    "        \"You are an open IE extractor operating under a fixed ontology. From the text, propose \"\n",
    "        \"triples [subject, relation, object] that satisfy the ontology’s domain→range. For every \"\n",
    "        \"triple, cite exact supporting span(s) and give a 0–1 confidence. Output JSON only.\"\n",
    "    )\n",
    "\n",
    "def build_p3_user(TEXT: str, ONTO: Dict[str, Any], k: int) -> str:\n",
    "    \"\"\"\n",
    "    Build the user message for Prompt 3 (open IE under ontology),\n",
    "    embedding the text and ontology, with list-style support spans.\n",
    "    \"\"\"\n",
    "    return dedent(f\"\"\"\\\n",
    "    Task: Extract up to k triples that are directly supported by the text. You may paraphrase, but you must quote the evidence substrings. Enforce domain→range strictly; if a triple is invalid, omit it.\n",
    "\n",
    "    Requirements\n",
    "    - Only produce triples whose subject type matches the relation’s domain and whose object type matches the relation’s range.\n",
    "    - Return JSON only, with this schema\n",
    "        {{\n",
    "          \"triples\": [\n",
    "            {{\n",
    "              \"triple\": [\"subject\",\"relation\",\"object\"],\n",
    "              \"subject_type\": \"Concept\",\n",
    "              \"object_type\": \"Concept\",\n",
    "              \"confidence\": 0.0,\n",
    "              \"support\": [\n",
    "                {{\"quote\": \"exact substring\", \"char_span\": [start,end]}}\n",
    "              ],\n",
    "              \"notes\": \"brief justification, including any pronoun/coref used\"\n",
    "            }}\n",
    "          ]\n",
    "        }}\n",
    "\n",
    "    Text\n",
    "    \"{_escape_multiline(TEXT)}\"\n",
    "\n",
    "    Ontology concepts\n",
    "    {format_ontology_concepts(ONTO)}\n",
    "\n",
    "    Ontology relations (domain → range)\n",
    "    {format_ontology_relations(ONTO)}\n",
    "\n",
    "    Constraints\n",
    "    - Extract ALL clearly stated factual triples in the text.\n",
    "    - If a triple matches an ontology relation, enforce domain→range consistency.\n",
    "    - If a triple does NOT match any ontology relation, you MUST STILL include it (do not discard it).\n",
    "    - Always extract any explicit date, time, or year mentioned in the text as part of a factual triple.\n",
    "    - Resolve pronouns to the nearest valid antecedent and describe that in notes.\n",
    "    - Do not invent entities that are not mentioned in the text.\n",
    "    - Output MUST be valid JSON and nothing else.\n",
    "    \"\"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a634a876-dece-4ba4-ae85-28fe5ccced69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_json(\n",
    "    generator,\n",
    "    tokenizer,\n",
    "    system_text: str,\n",
    "    user_text: str,\n",
    "    max_new_tokens: int = 900,\n",
    "    temperature: float = 0.25,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate RAW model output (string). No parsing here.\n",
    "    Mirrors Prompt 1/2: format chat with tokenizer.apply_chat_template.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_text},\n",
    "        {\"role\": \"user\",   \"content\": user_text},\n",
    "    ]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    out = generator(\n",
    "        formatted,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        truncation=False,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return out[0][\"generated_text\"] if isinstance(out[0], dict) else out[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62382ea3-d2ea-4fae-aa20-edaafb1b8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_pipeline_prompt3(\n",
    "#     input_jsonl_path: str,\n",
    "#     ontology_json_path: str,\n",
    "#     output_jsonl_path: str,\n",
    "#     max_items: Optional[int] = None,\n",
    "#     max_new_tokens: int = 900,\n",
    "#     temperature: float = 0.25,\n",
    "#     verbose: bool = False,  # True to inspect prompts & raw output\n",
    "#     k: int = 6,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     RAW-only pipeline for Prompt 3 (no filtering / no validation).\n",
    "#     Writes a JSONL where each line contains:\n",
    "#       - id\n",
    "#       - input text\n",
    "#       - prompts (system/user)\n",
    "#       - response: raw + parsed json\n",
    "#     \"\"\"\n",
    "#     # --- load ontology ---\n",
    "#     with open(ontology_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         ontology_json = json.load(f)\n",
    "\n",
    "#     # --- load inputs (with max_items) ---\n",
    "#     items = list(read_jsonl(input_jsonl_path, max_items=max_items))\n",
    "#     if verbose:\n",
    "#         print(f\"[RUN] Loaded {len(items)} input items from {input_jsonl_path}\")\n",
    "\n",
    "#     # --- model (same style as Prompt 1/2) ---\n",
    "#     generator, tokenizer = setup_model()\n",
    "\n",
    "#     outputs: List[Dict[str, Any]] = []\n",
    "\n",
    "#     for i, rec in enumerate(items, start=1):\n",
    "#         rid = str(rec.get(\"id\") or f\"item_{i}\")\n",
    "#         text, key_used = _extract_text(rec)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"\\n[RUN] === ID={rid} ===\")\n",
    "#             print(f\"[INFO] text key: {key_used!r}\")\n",
    "\n",
    "#         # --- build Prompt 3 (open IE under ontology) ---\n",
    "#         sys_prompt = build_p3_system()\n",
    "#         usr_prompt = build_p3_user(text, ontology_json, k)\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"\\n==== [DEBUG] SYSTEM PROMPT ====\\n\", sys_prompt)\n",
    "#             print(\"\\n==== [DEBUG] USER PROMPT ====\\n\", usr_prompt)\n",
    "\n",
    "#         # --- generate RAW output (same signature as Prompt 1/2) ---\n",
    "#         try:\n",
    "#             raw = generate_raw_json(\n",
    "#                 generator=generator,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 system_text=sys_prompt,\n",
    "#                 user_text=usr_prompt,\n",
    "#                 max_new_tokens=max_new_tokens,\n",
    "#                 temperature=temperature,\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] Generation failed for {rid}: {e}\")\n",
    "#             raw = \"\"\n",
    "\n",
    "#         if verbose:\n",
    "#             print(\"\\n==== [DEBUG] RAW MODEL OUTPUT ====\\n\", raw)\n",
    "\n",
    "#         # --- parse JSON (best-effort, same as Prompt 1/2) ---\n",
    "#         parsed = None\n",
    "#         if isinstance(raw, str) and raw.strip():\n",
    "#             try:\n",
    "#                 parsed = json.loads(raw)\n",
    "#             except Exception:\n",
    "#                 m = re.search(r\"\\{[\\s\\S]*\\}\", raw)\n",
    "#                 if m:\n",
    "#                     try:\n",
    "#                         parsed = json.loads(m.group(0))\n",
    "#                     except Exception:\n",
    "#                         parsed = None\n",
    "\n",
    "#         # --- record output (Prompt 1/2 schema) ---\n",
    "#         out_rec = {\n",
    "#             \"id\": rid,\n",
    "#             \"input text\": text,\n",
    "#             \"prompts\": {\n",
    "#                 \"system_prompt\": sys_prompt,\n",
    "#                 \"user_prompt\": usr_prompt,\n",
    "#             },\n",
    "#             \"response\": {\n",
    "#                 \"LLM_output\": raw,\n",
    "#                 \"json\": parsed,\n",
    "#             },\n",
    "#         }\n",
    "#         outputs.append(out_rec)\n",
    "\n",
    "#     # --- write JSONL (one record per line) ---\n",
    "#     write_jsonl(output_jsonl_path, outputs)\n",
    "#     if verbose:\n",
    "#         print(f\"\\n[RUN] Wrote {len(outputs)} records to {output_jsonl_path}\")\n",
    "#     return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d5ca95a-375c-4949-8167-4da49762690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Paths (adjust to your filesystem) ===\n",
    "# ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "# INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "# OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "\n",
    "# # === Run knobs (mirror Prompt 1/2 defaults) ===\n",
    "# MAX_ITEMS      = 1            # set to None to process all\n",
    "# MAX_NEW_TOKENS = 900\n",
    "# TEMPERATURE    = 0.25\n",
    "# VERBOSE        = True         # True to inspect prompts + raw outputs\n",
    "# K_CANDIDATES   = 6\n",
    "\n",
    "# # --- Quick sanity peek (first record) ---\n",
    "# first = next(read_jsonl(INPUT_JSONL, max_items=1), None)\n",
    "# if first is None:\n",
    "#     print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "# else:\n",
    "#     print(\"[DEBUG] First record keys:\", list(first.keys()))\n",
    "#     print(\" id:\", first.get(\"id\"))\n",
    "#     sent = first.get(\"sent\") or first.get(\"text\") or \"\"\n",
    "#     print(\" sent:\", sent[:160] + (\"...\" if len(sent) > 160 else \"\"))\n",
    "\n",
    "# # --- Run Prompt 3 pipeline (RAW only) ---\n",
    "# _ = run_pipeline_prompt3(\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     ontology_json_path=ONTOLOGY_JSON,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     max_new_tokens=MAX_NEW_TOKENS,\n",
    "#     temperature=TEMPERATURE,\n",
    "#     verbose=VERBOSE,\n",
    "#     k=K_CANDIDATES,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02dc090a-b044-4afd-925e-50d0694aea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_parse_json(raw: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Same style as Prompt 2: first try json.loads, then fallback to first {...}\n",
    "    \"\"\"\n",
    "    raw = raw.strip()\n",
    "    try:\n",
    "        return json.loads(raw)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    m = re.search(r\"\\{.*\\}\", raw, flags=re.DOTALL)\n",
    "    if m:\n",
    "        block = m.group(0)\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def run_pipeline_prompt3(\n",
    "    ontology_path: str,\n",
    "    input_jsonl_path: str,\n",
    "    output_jsonl_path: str,\n",
    "    k_triples: int = 5,\n",
    "    max_items: Optional[int] = None,\n",
    "    verbose: bool = True,\n",
    "    model_id: str = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    generator=None,\n",
    "    tokenizer=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Prompt 3 over a single dataset file:\n",
    "      - load ontology + rows\n",
    "      - build prompts (Prompt 3 style)\n",
    "      - generate model output\n",
    "      - parse JSON\n",
    "      - write trace jsonl\n",
    "\n",
    "    Behavior:\n",
    "      • If `generator` and `tokenizer` are provided, reuse them (no new model load).\n",
    "      • Otherwise, load the model from `model_id` internally (backward compatible).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. load ontology\n",
    "    ontology_json = load_ontology_json(ontology_path)\n",
    "\n",
    "    # 2. init / reuse model\n",
    "    local_model_loaded = False\n",
    "    if generator is None or tokenizer is None:\n",
    "        generator, tokenizer = setup_model(model_id=model_id)\n",
    "        local_model_loaded = True\n",
    "\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 3. iterate input rows\n",
    "    for idx, rec in enumerate(read_jsonl(input_jsonl_path, max_items=max_items)):\n",
    "        rec_id = str(rec.get(\"id\") or f\"item_{idx}\")\n",
    "        text_val, text_key = extract_text_field(rec)\n",
    "\n",
    "        # <-- Prompt 3 prompt builders\n",
    "        sys_prompt = build_p3_system()\n",
    "        usr_prompt = build_p3_user(text_val, ontology_json, k_triples)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"======================================\")\n",
    "            print(f\"[ID] {rec_id}\")\n",
    "            print(f\"[TEXT_KEY] {text_key}\")\n",
    "            print(\"[SYSTEM PROMPT]\\n\", sys_prompt)\n",
    "            print(\"[USER PROMPT]\\n\", usr_prompt)\n",
    "            print(\"[SOURCE TEXT]\\n\", text_val)\n",
    "\n",
    "        raw_response = generate_raw_json(\n",
    "            generator=generator,\n",
    "            tokenizer=tokenizer,\n",
    "            system_text=sys_prompt,\n",
    "            user_text=usr_prompt,\n",
    "            max_new_tokens= 1500,\n",
    "            temperature=0.25,\n",
    "        )\n",
    "        \n",
    "\n",
    "        parsed_json = try_parse_json(raw_response)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"[RAW RESPONSE]\\n\", raw_response)\n",
    "            print(\"[PARSED JSON]\\n\", parsed_json)\n",
    "\n",
    "        out_record = {\n",
    "            \"id\": rec_id,\n",
    "            \"input text\": text_val,\n",
    "            \"prompts\": {\n",
    "                \"system_prompt\": sys_prompt,\n",
    "                \"user_prompt\": usr_prompt,\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"LLM_output\": raw_response,\n",
    "                \"json\": parsed_json,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        results.append(out_record)\n",
    "\n",
    "    # 4. write collected results\n",
    "    write_jsonl(output_jsonl_path, results)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n[P3 WRITE] {len(results)} rows -> {output_jsonl_path}\")\n",
    "\n",
    "    # optional cleanup if we self-loaded the model:\n",
    "    # if local_model_loaded:\n",
    "    #     torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f11a5fed-ba07-4a2c-823f-ee689fd7fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Paths (adjust to your filesystem) ===\n",
    "# ONTOLOGY_JSON = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/1_movie_ontology.json\"\n",
    "# INPUT_JSONL   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_1_movie_test.jsonl\"\n",
    "# OUTPUT_JSONL  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_1_movie_output_test.jsonl\"\n",
    "\n",
    "# # === Debug / run knobs ===\n",
    "# MAX_ITEMS      = 1        # only process first 1 row for debugging\n",
    "# VERBOSE        = True     # print prompts, raw model output, parsed json\n",
    "# MODEL_ID       = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# K_TRIPLES_P3   = 6        # how many ontology triples we include in the user prompt\n",
    "\n",
    "# # --- Quick sanity peek (first record) ---\n",
    "# first = next(read_jsonl(INPUT_JSONL, max_items=1), None)\n",
    "# if first is None:\n",
    "#     print(f\"[ERROR] No records found in: {INPUT_JSONL}\")\n",
    "# else:\n",
    "#     print(\"[DEBUG] First record keys:\", list(first.keys()))\n",
    "#     print(\" id:\", first.get(\"id\"))\n",
    "#     sent = first.get(\"sent\") or first.get(\"text\") or \"\"\n",
    "#     preview = sent[:160] + (\"...\" if len(sent) > 160 else \"\")\n",
    "#     print(\" sent:\", preview)\n",
    "\n",
    "# # --- Run Prompt 3 pipeline on a small slice (MAX_ITEMS=1) ---\n",
    "# _ = run_pipeline_prompt3(\n",
    "#     ontology_path=ONTOLOGY_JSON,\n",
    "#     input_jsonl_path=INPUT_JSONL,\n",
    "#     output_jsonl_path=OUTPUT_JSONL,\n",
    "#     k_triples=K_TRIPLES_P3,\n",
    "#     max_items=MAX_ITEMS,\n",
    "#     verbose=VERBOSE,\n",
    "#     model_id=MODEL_ID,\n",
    "#     # For debug we let run_pipeline_prompt3 load the model internally,\n",
    "#     # so we do NOT pass generator/tokenizer here.\n",
    "#     generator=None,\n",
    "#     tokenizer=None,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccbe0d7-0868-461d-916a-02eb73874e48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808e6847-b330-4bba-8273-3e3460d7ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1c25b42-c314-487d-bb64-7f9401f62c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "WIKIDATA_PATTERN_P3 = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "\n",
    "# === Debug / run knobs ===\n",
    "MAX_ITEMS      = None        # only process first 1 row for debugging\n",
    "VERBOSE        = True     # print prompts, raw model output, parsed json\n",
    "MODEL_ID       = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "K_TRIPLES_P3   = 6        # how many ontology triples we include in the user prompt\n",
    "\n",
    "def make_wikidata_paths_p3(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Build all paths for a Wikidata file for Prompt 3.\n",
    "\n",
    "    Example:\n",
    "      filename = \"ont_8_politician_test.jsonl\"\n",
    "\n",
    "      input_jsonl_path   -> <base_input>/ont_8_politician_test.jsonl\n",
    "      ontology_json_path -> <base_onto>/8_politician_ontology.json\n",
    "      output_jsonl_path  -> <base_out>/ont_8_politician_output.jsonl\n",
    "      tag                -> \"ont_8_politician\"\n",
    "    \"\"\"\n",
    "    m = WIKIDATA_PATTERN_P3.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_wikidata_batch_p3(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        #\"ont_1_movie_test.jsonl\",\n",
    "        #\"ont_2_music_test.jsonl\",\n",
    "        #\"ont_3_sport_test.jsonl\",\n",
    "        \"ont_4_book_test.jsonl\",\n",
    "        \"ont_5_military_test.jsonl\",\n",
    "        \"ont_6_computer_test.jsonl\",\n",
    "        \"ont_7_space_test.jsonl\",\n",
    "        \"ont_8_politics_test.jsonl\",\n",
    "        \"ont_9_nature_test.jsonl\",\n",
    "        \"ont_10_culture_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    # ensure output dir exists\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # load model once, reuse for all files\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_wikidata_paths_p3(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P3] wikidata {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            run_pipeline_prompt3(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES_P3,\n",
    "                max_items=MAX_ITEMS,\n",
    "                verbose=False,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse model\n",
    "                tokenizer=tokenizer,   # reuse tokenizer\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P3] wikidata {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P3] wikidata {fname}: {exc}max token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eed2317-a3c0-4507-b937-c271e375dc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Loading model: mistralai/Mistral-7B-Instruct-v0.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c4b8ca778954b00b1ccce44b51d899e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_4_book\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_4_book_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/4_book_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_4_book_output.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE P3] wikidata ont_4_book\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_5_military\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_5_military_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/5_military_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_5_military_output.jsonl\n",
      "[DONE P3] wikidata ont_5_military\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_6_computer\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_6_computer_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/6_computer_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_6_computer_output.jsonl\n",
      "[DONE P3] wikidata ont_6_computer\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_7_space\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_7_space_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/7_space_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_7_space_output.jsonl\n",
      "[DONE P3] wikidata ont_7_space\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_8_politics\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_8_politics_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/8_politics_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_8_politics_output.jsonl\n",
      "[DONE P3] wikidata ont_8_politics\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_9_nature\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_9_nature_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/9_nature_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_9_nature_output.jsonl\n",
      "[DONE P3] wikidata ont_9_nature\n",
      "\n",
      "================================================================================\n",
      "[RUN P3] wikidata ont_10_culture\n",
      "[INPUT ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_text/ont_10_culture_test.jsonl\n",
      "[ONTO  ] /upb/users/b/balram/profiles/unix/cs/promptKG/data/input/wikidata/input_ontology/10_culture_ontology.json\n",
      "[OUTPUT] /upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/wikidata/ont_10_culture_output.jsonl\n",
      "[DONE P3] wikidata ont_10_culture\n"
     ]
    }
   ],
   "source": [
    "run_wikidata_batch_p3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415ba97-9400-4a78-9e04-73763fe18303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13852684-0a56-491a-945a-cf60dfb55339",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_PATTERN_P3 = re.compile(r\"^ont_(\\d+)_([a-zA-Z0-9]+)_test\\.jsonl$\")\n",
    "# === Debug / run knobs ===\n",
    "MAX_ITEMS      = 1        # only process first 1 row for debugging\n",
    "VERBOSE        = True     # print prompts, raw model output, parsed json\n",
    "MODEL_ID       = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "K_TRIPLES_P3   = 6        # how many ontology triples we include in the user prompt\n",
    "\n",
    "def make_dbpedia_paths_p3(filename: str, base_input: str, base_onto: str, base_out: str):\n",
    "    \"\"\"\n",
    "    Build all paths for a DBpedia file for Prompt 3.\n",
    "\n",
    "    Example:\n",
    "      filename = \"ont_14_writtenwork_test.jsonl\"\n",
    "\n",
    "      input_jsonl_path   -> <base_input>/ont_14_writtenwork_test.jsonl\n",
    "      ontology_json_path -> <base_onto>/14_writtenwork_ontology.json\n",
    "      output_jsonl_path  -> <base_out>/ont_14_writtenwork_output.jsonl\n",
    "      tag                -> \"ont_14_writtenwork\"\n",
    "    \"\"\"\n",
    "    m = DBPEDIA_PATTERN_P3.match(filename)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unexpected filename format: {filename}\")\n",
    "    idx, cat = m.groups()\n",
    "\n",
    "    input_jsonl_path   = os.path.join(base_input, filename)\n",
    "    ontology_json_path = os.path.join(base_onto, f\"{idx}_{cat}_ontology.json\")\n",
    "\n",
    "    out_name = filename.replace(\"_test.jsonl\", \"_output.jsonl\")\n",
    "    output_jsonl_path  = os.path.join(base_out, out_name)\n",
    "\n",
    "    tag = f\"ont_{idx}_{cat}\"\n",
    "    return input_jsonl_path, ontology_json_path, output_jsonl_path, tag\n",
    "\n",
    "\n",
    "def run_dbpedia_batch_p3(verbose: bool = True):\n",
    "    BASE_INPUT = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_text/\"\n",
    "    BASE_ONTO  = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/input/dbpedia/input_ontology/\"\n",
    "    BASE_OUT   = \"/upb/users/b/balram/profiles/unix/cs/promptKG/data/output/prompt3/dbpedia/\"\n",
    "\n",
    "    FILENAMES = [\n",
    "        \"ont_12_monument_test.jsonl\",\n",
    "        \"ont_1_university_test.jsonl\",\n",
    "        \"ont_2_musicalwork_test.jsonl\",\n",
    "        \"ont_3_airport_test.jsonl\",\n",
    "        \"ont_4_building_test.jsonl\",\n",
    "        \"ont_5_athlete_test.jsonl\",\n",
    "        \"ont_6_politician_test.jsonl\",\n",
    "        \"ont_7_company_test.jsonl\",\n",
    "        \"ont_8_celestialbody_test.jsonl\",\n",
    "        \"ont_9_astronaut_test.jsonl\",\n",
    "        \"ont_10_comicscharacter_test.jsonl\",\n",
    "        \"ont_11_meanoftransportation_test.jsonl\",\n",
    "        \"ont_13_food_test.jsonl\",\n",
    "        \"ont_14_writtenwork_test.jsonl\",\n",
    "        \"ont_15_sportsteam_test.jsonl\",\n",
    "        \"ont_16_city_test.jsonl\",\n",
    "        \"ont_17_artist_test.jsonl\",\n",
    "        \"ont_18_scientist_test.jsonl\",\n",
    "        \"ont_19_film_test.jsonl\",\n",
    "    ]\n",
    "\n",
    "    # ensure output dir exists\n",
    "    os.makedirs(BASE_OUT, exist_ok=True)\n",
    "\n",
    "    # load model once, reuse for every DBpedia file\n",
    "    generator, tokenizer = setup_model(model_id=MODEL_ID)\n",
    "\n",
    "    for fname in FILENAMES:\n",
    "        try:\n",
    "            input_jsonl_path, ontology_json_path, output_jsonl_path, tag = make_dbpedia_paths_p3(\n",
    "                filename=fname,\n",
    "                base_input=BASE_INPUT,\n",
    "                base_onto=BASE_ONTO,\n",
    "                base_out=BASE_OUT,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(f\"[RUN P3] dbpedia {tag}\")\n",
    "            print(f\"[INPUT ] {input_jsonl_path}\")\n",
    "            print(f\"[ONTO  ] {ontology_json_path}\")\n",
    "            print(f\"[OUTPUT] {output_jsonl_path}\")\n",
    "\n",
    "            run_pipeline_prompt3(\n",
    "                ontology_path=ontology_json_path,\n",
    "                input_jsonl_path=input_jsonl_path,\n",
    "                output_jsonl_path=output_jsonl_path,\n",
    "                k_triples=K_TRIPLES_P3,\n",
    "                max_items=None,\n",
    "                verbose=verbose,\n",
    "                model_id=MODEL_ID,\n",
    "                generator=generator,   # reuse\n",
    "                tokenizer=tokenizer,   # reuse\n",
    "            )\n",
    "\n",
    "            print(f\"[DONE P3] dbpedia {tag}\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[ERROR P3] dbpedia {fname}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "269bc3fc-a2e5-4a26-9df3-aa87c748e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dbpedia_batch_p3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb93f7db-abf2-4823-8221-c7bdebaf5592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kg_pipeline",
   "language": "python",
   "name": "kg_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
